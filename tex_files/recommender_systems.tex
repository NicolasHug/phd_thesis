\chapter{Background on recommender systems}
\label{CHAP:background_reco_systems}
\localtableofcontents*
\vspace*{\baselineskip}

\initial{I}n a world of information overload, automatic filtering tools are
essential to extract relevant information from basic noise
\cite{RecoSystemHandbook,AdoTuzIEEE2005}. In the field of e-commerce,
recommender systems play the role of search engines when surfing the entire
web: they filter available items to provide relevant suggestions to customers.
They help to answer questions as diverse as ``what movie to rent?", ``what car
to buy?" or ``which restaurant to try?". They do so by providing the user with
a list of recommendations.

We have seen in the previous chapter that analogical learners exhibit promising
results when compared to the $k$-NN algorithms in a Boolean setting. In chapter
\ref{CHAP:analogical_recommendation}, we will try to apply analogical learning
to a recommendation task to see if analogical learners can prove useful in
these more challenging contexts.

In this chapter, we will provide some necessary background on recommender
systems, before diving into Chapter \ref{CHAP:analogical_recommendation} where
we will describe our contributions and experiments. We first provide a taxonomy
of current research in the recommender systems area in Section
\ref{SEC:taxonomy_rec_sys}, and set ourselves withing the collaborative
filtering framework. In Section \ref{SEC:recommendation_rating_prediction}, we
formally define the recommendation problem that we will address in this
document, and explain how to assess the quality of a recommender system. In
Section \ref{SEC:neighborhood_and_matrix_facto_review}, we extensively describe
two collaborative filtering methods: neighborhood-based algorithms, and matrix
factorization-based techniques. These two families of algorithm will serve as
baselines to which we will compare our own prediction algorithms in Chapter
\ref{CHAP:analogical_recommendation}.

\section{Taxonomy of recommender systems}
\label{SEC:taxonomy_rec_sys}

Undoubtedly, there are many different tasks that range among the general problem
of recommendation. If they all share the ultimate goal of providing users with
relevant recommendations, they greatly differ in nature by the items that are
recommended, and by the technical means that are involved.

To provide a clear view of the vast recommendation landscape, various taxonomies
for recommender systems have been proposed: see for example \cite{Bur02,
AdoTuzIEEE2005, Bur07}, and more recently \cite{BurRam11}. Using that of
\cite{BurRam11}, we will successively describe \textbf{content-based},
\textbf{collaborative} and \textbf{knowledge-based} approaches in the next
three sections, and show that they generally apply to different problems.

Rest assured that in real systems the frontiers between these three families
are not that sharp, and practical recommendation solutions usually fall into
more than one single category. We refer to such systems as \textbf{hybrid}
recommenders.

\subsection{Content-based techniques}

The general idea of content-based algorithms is as follows: they try to recommend to a user some items that are
\textbf{similar} to those that the user has liked in the past. For example, if
the renting history of a user mainly contains science fiction movies, the aim
of the system is to learn these preferences and to recommend some other science
fiction movies to the user. We recommend \cite{LopGemSem11} for a recent
overview of content-based methods.

To find out similar items to those that a user has liked, these systems usually
rely on a similarity metric whose nature strongly depends on the representation
of the items. In large-scale online shopping systems, were items are extremely
diverse and abundant, a similarity measure between two items would for
example be the number of web sessions on which the two item pages have jointly
been visited (yes, this is the reason why some online systems will try to sell
you a fridge even though you just bought a brand new one). In systems were
items are more homogeneous, i.e. in a movie store system, more  sophisticated
approaches can be built relying on some metadata about the items (hence the
name \textit{content}-based). In the case of movies, the metadata could be for
example for example the genre, main actors, film director, etc\dots

As content-based techniques usually do not rely on a set of ratings to compute item
similarities, a nice resulting feature is that new items that have not yet been
rated by anybody can still be recommended provided that the items are properly
described (which is not necessarily easy, as we will see). This is not the case for a new user
though: as we have no information about the items this user may like, a
content-based recommender would not be able to output any prediction. These
techniques also stand out by their explanatory potential: the motto
``\textit{here are some items that are akin to those you liked}'' is perfectly
understandable and seems sound.  Recommender systems usually strive for
explanatory power, because it is recognized that when confronted with a given
recommendation, users are more likely to accept the recommendation if they
understand and acknowledge the underlying process.

However, content-based systems are prone to various behaviours that tend to make
them less competitive with collaborative filtering methods. The first obvious drawback, known as
the problem of \textit{limited content analysis}, is precisely the need to
describe items with metadata (remember our movie example with
genre, actors etc.). Such descriptions can be extremely costly and can only
capture a very limited subset of item features, which are not necessarily the
most important ones when it comes to the users personal tastes. Going back to
our science fiction fan, it could be plausible that the user has a strong
preference for the steampunk genre, and yet is perfectly indifferent to the
superhero fiction movies, and both can still be considered as sub-genres of
science fiction. A system that could not distinguish these two kinds of movies
would fail to provide our steampunk fan with relevant recommendations.

Another well-known drawback of content-based recommenders is their tendency to
overfit and recommend only items that users may already know or do not need
anymore (such as a fridge!). The recommendations tend to lack in novelty,
surprise and diversity. Also, content-based systems tend to output much less
accurate prediction than the collaborative filtering methods (the way accuracy
is computed will be described in Section
\ref{SEC:Recommender_system_evaluation}). This is why collaborative filtering
methods have gained in popularity.

\subsection{Collaborative filtering}

The main idea behind collaborative filtering algorithms is to recommend items
to a users that other users with similar tastes have liked in the past. In its
most general form, collaborative filtering methods (also called social
filtering) try to model the social environment of users and to output
recommendation based on the behaviour of their peers.

Probably the most common way of modeling the social environment of a user is to
use historical transaction stored in the system, for example using user-item
ratings: two users are peers if their ratings are similar. If Bob and Alice
usually agree on their movie ratings, i.e. they like the same movies and
dislike the same movies, and if Alice has not seen a movie that Bob has liked,
then it will be recommended to Alice.

A main advantage of collaborative filtering methods over content-based ones is
that they do not require any item or user knowledge: only user-item feedback is
needed to output a prediction. Thanks to this, they also allow to build systems
that can deal with items of different natures. It is indeed plausible to
imagine a collaborative filtering system recommending both books and movies, on
the basis that users that love the same books usually like the same movies. As
books and movies cannot be described using the same set of features (even
though they are close), the use of content-based techniques would make the task
much more difficult, or would again require largely hand-crafted work.

The main advantage of collaborative filtering methods is also their main
weakness. Before they can become effective, these systems require a serious
amount of data (i.e. user-item interactions) to train a model or to use
heuristics efficiently. A side-effect is known as the cold-start problem: there
is no way to yield recommendations for new users that have no known ratings,
and the same goes for new items that nobody has rated.

All the recommendation algorithms that we will propose in this document are of
a collaborative nature. Section \ref{SEC:neighborhood_and_matrix_facto_review}
will be devoted to an overview of two main collaborative techniques: the
neighborhood approach and matrix factorization-based methods. They are
typical examples of the two main approaches to collaborative filtering:
heuristic-based systems, and model-based systems.

\subsection{Knowledge-based systems}

The two families we have described so far (content-based and collaborative
methods) are only suitable for certain kinds of items. While movies, books,
music or news can all fit quite well within these two frameworks, this is not
the case for some other items such as cars, bank loans, or real estates. Take
for example the problem of car recommendation. A content-based approach would
need to know what kind of cars the user have owned in the past, and would
recommend some new cars that are similar to those past vehicles. This method is
obviously bound to fail, not only because in practice such in formation about
past vehicles is never available, but also because the recommendation would be
based on some years-old preferences. As for collaborative methods, they would
require the system to dispose of a massive amount of past transactions, and
people do not buy as many cars as they watch movies: only very few information
would be available, leading to disastrous results.

In broad terms, we refer to knowledge-based systems as any method that is
neither content-based or collaborative. In general, these systems are used in
settings where the domain and contextual knowledge (i.e. the knowledge about the
items that are available in the system) prevails over any other source of
information, such as social information for example.

A practical instance of  knowledge-based recommendation takes the form of a
recommendation session, where the user indicates her needs in an interactive
manner, and the systems tries to match these needs as best as possible to
provide useful recommendations. According to \cite{FelBur08}, there exist two
kinds of knowledge-based systems, that differ in the way recommendations are
computed. The first category is made up of the \textbf{case-based} systems.
Case-based recommenders obviously draw from research in case-base reasoning
described in Section \ref{TODO}, and computations are based on similarities:
the system will try to recommend items that are similar to an ideal item
expressed by the user's needs. In
this respect, case-based recommenders are close to the aforementioned
content-based systems. The second  kind of knowledge-based recommender are the
so-called \textbf{constraint-based} systems (see \cite{FelFriJanZan11} for a complete
review). In such systems, user and items are described
by a set of properties, along with a set of constraints. In the car
recommendation example, an item constraint could for example express that a
convertible car should not possess a sunroof, and user constraint could state
that the car color should not be green. The problem of recommending an item to
a user then takes the form of a classical constraint satisfaction
problem.

We are well aware that these two definitions of case-based and constraint-based
systems are quite general and vague, but the fact remains that practical
implementation of such systems are so domain-dependent that is is difficult to
exhibit a clear unifying framework that would still be accurate.

Now that we have a clear overview of the various tasks that can be addressed in
recommendation, we will now detail the recommendation problem that will be
addressed in the next chapter.

\section{Recommendation by rating prediction}.
\label{SEC:recommendation_rating_prediction}

In this section, we formally define the recommendation problem that we plan to
address later in Chapter \ref{CHAP:analogical_recommendation}. We also describe
how to assess the quality of a recommender system.

\subsection{Problem formalization and notation}
Let $U$ be a set of users and $I$ a set of items. For some pairs $(u,i) \in U
\times I$, a rating  $\rui$ is supposed to have been given by $u$ to express
its preferences towards the item $i$. The way the preferences are expressed
depend on the rating scale that is used, which can be of different nature.
According to \cite{SchFraHerSen07}, we can distinguish four different kinds of
rating scales:

\begin{itemize}
  \item It is quite common that  $\rui$ belongs to a \textbf{numerical} rating
    scale. A typical numerical scale is $[1, 5]$, where  5 means a strong
    preference for item $i$, 1 means a strong rejection, and 3 means
    indifference, or just an average.
  \item A sub-case of the previous scale is the \textbf{binary} rating scale,
    where ratings belong to $[0, 1]$. In this case, we associate the value $0$
    with the meaning \textit{dislike} and the value $1$ with the meaning
    \textit{like}.
  \item An extreme case is the \textbf{unary} rating scale, where a value of
    $1$ translates as a preference, but rejections (or dislikes) are not
    addressed.
  \item Sometimes, the rating scale can be considered as \textbf{ordinal},
    where the elements of the scale have no absolute meaning and are only
    meaningful when compared to others, e.g. $[$\textit{rejection, indifference,
    agreement}$]$.
\end{itemize}
In practice, these semantic distinctions are purely contextual: a rating
scale $[1, 5]$ rating scale could perfectly be interpreted as purely numerical,
or conversely as purely ordinal. This observation has been addressed for
example in \cite{KorSillRECSYS11}, and will also be discussed here in the next
chapter. However, the rating scale is a decisive component of a recommender
system, and algorithms are usually designed to work with only one single rating
scale.

Ratings may be collected in different ways, and we commonly distinguish explicit and
implicit ratings. \textbf{Explicit} ratings are those for which users have
explicitly expressed their preference towards the items. \textbf{Implicit}
ratings belong to a binary or unary scale, and correspond to the result of
collected data about users behaviour, without any active involvement. For
example, if a user has listened to a given track more than 10 times, we might
consider this as an implicit rating of $1$. The ability to deal with both
explicit and implicit ratings can lead to significant improvements for a
recommender system. In \cite{KorSillRECSYS11}, a model using only explicit
numerical ratings was extended to deal with implicit ratings, which improved
the RMSE of the system (defined later) by a great deal. When no implicit
ratings are available, the same authors showed that the simple modeling of
implicit ratings as $r_{ui}' = 1 \iff r_{ui} \in R$ led to significant
improvements. Here, $r_{ui}'$ is an implicit rating stating that $u$ has rated
$i$, regardless of the rating value.

In our work, we will mostly focus on the explicit and numerical rating scheme.
Also, the algorithms we will design will be of a collaborative nature, and
therefore they will rely on a dataset containing user-item interactions (in our
case, user-item ratings).
Let us denote by $R$ the set of known ratings recorded in the system. In real
systems the size of $R$ is very small with regard to the potential number of
ratings which is $|U| \times |I|$, as a lot of ratings are missing. The set
$\Ui$ will denote the set of users that have rated item $i$, and $\Uij \eqdef
U_i \cap U_j$ is the set of users that have rated both item $i$ and item $j$.
Similarly, $\Iu$ is the set of items that user $u$ has rated, and $\Iuv \eqdef
I_u \cap I_v$ is the set of items that both users $u$ and $v$ have rated.

A very common way of providing personalized recommendations to a target user is
to estimate its taste for the items that the system provides. The taste of a
user $u$ for a given item $i$ is usually represented as the rating that $u$
would give to $i$.  Once these estimations are made, a simple option is to
recommend the items with the highest ratings among all the estimated scores for
the considered user (using the implicit assumption that a user should be
interested in the items with high scores).

More formally, a recommender system usually proceed as follows:
\begin{enumerate}
\item Using a prediction algorithm $A$, estimate the unknown ratings $\rui$
  (i.e. $\rui \notin R$). This estimation $A(u, i)$ will here be denoted
    $\predrui$.
\item Using a recommendation strategy $S$ and in the light of the previously
  estimated ratings, recommend items to users. For instance, a basic yet common
    strategy is to suggest to user $u$ the items $i \notin I_u$ with the
    highest estimation $\predrui$.
\end{enumerate}

When using a unary or binary rating scale, these two stages are usually
indistinguishable: it is natural to recommend an item to a user if the
estimated predicted rating is $1$. In practice, the recommendation strategy
depends a lot on the pragmatic constraints of the system: a store manager way
want to push forward a given item to boost its sells, regardless of the
rating predictions outputted by the algorithm. As a result, most of the
research has focused on the prediction algorithms, and so will we. Note also
that this view is only meaningful for non-ordinal rating scale. When the output
of prediction algorithm $A$ are ranks, the recommendation strategy $S$ is
obvious and the distinction need not be made.

Prediction algorithms implementations are strongly influenced by the fields of
data mining and of course machine learning. However, we want to emphasize that
the general setting of recommender systems is actually quite different than
that of classification or regression, the two main prediction tasks of machine
learning. A naive reasoning could lead us to consider that the rating
prediction problem with the rating scale $[1, 5]$ is nothing but a
classification task with five classes $1, 2, 3, 4, 5$. But this is forgetting
that the values $1, 2, 3, 4, 5$ are actually ordered, and that the difference
between \textit{class} $1$ and \textit{class} $2$ is not the same as the
difference between \textit{class} $1$ and \textit{class} $4$. But most
importantly, in classification or regression all the instances belong to the
same space (that space was $X^m$ in our previous chapters), and have the same
features. This is not the case here! In a recommendation problem, if we chose
our instances to be the users and their feature space to be the items, we would
end up with instances that are only \textbf{very partially described}, because of
course no user has rated the whole set of items. All the more, as the number of
items is usually very high, we would end up with a very high dimensional
problem, where traditional learning algorithms tend to fail due to the
so-called curse of dimensionality. Our point here is that recommender systems
problems do not really fit within the traditional machine learning setting, and
represent a new setting in their own right.

We now describe how the quality of a prediction algorithm and of a
recommendation strategy can be evaluated.

\subsection{Recommender system evaluation}
\label{SEC:Recommender_system_evaluation}
Providing an accurate measure of the overall quality of a recommender system is
not a simple task, and diverse viewpoints have to be considered.
We can distinguish three main evaluation settings. The first one is to perform
\textbf{user studies}, where actual users are asked to interact with a given system and to
provide some feedback. This evaluation process is probably the one that allows
to best capture users need, because feedback is explicitly given. Naturally,
this is also the most expensive kind of experiment to conduct, because it
requires a lot of user time. The second evaluation setting is to perform online
studies. In its most simple form, this is equivalent to A/B testing: to see
which of the two versions of our recommender performs better, we provide some
users with the first version and some other users with the second version. We
can then assess the change in performance between the two versions and decide
which one is the most suitable for our needs.

Finally, the last available option to evaluate recommender systems is to
perform off-line evaluation: in this setting, we dispose of a dataset of past
transactions (the set of all ratings $R$), and try to simulate user behaviour
to compute different measures.  In general, we will use $k$-folds
cross-validation procedures to reliably evaluate each of the performance
measures. The set of all ratings $R$ is divided into $k$ (typically $5$)
disjoint sets of equal sizes ; at each of the $k$ iterations, the test set
$\Rtest$ is set to the $k^{\text{th}}$ subset and and the training set
$\Rtrain$ is set as the union of the $k - 1$ remaining subsets: the system will
be trained on $\Rtrain$ and tested on $\Rtest$. The reported performances are
then averaged over the $k$ folds.

The first two evaluation settings (users studies and online studies) require
the use of an actual working recommender system. Thus, we will here only focus
on measures that can be calculated in an off-line evaluation process.

\paragraph{Accuracy\\}
The performance of the algorithm $A$ is usually evaluated in terms of accuracy,
which measures how close the rating predictions $\predrui$ are to the true
ratings $\rui$, for every possible prediction. The Root Mean Squared Error
(RMSE) is probably the most common indicator of how accurate an algorithm is,
and is calculated as follows:
$$\text{RMSE}(A) = \sqrt{\frac{1}{|\Rtest|} \cdot \sum_{\rui \in
\Rtest}(\predrui - \rui)^2}.$$

Another common indicator for accuracy is the Mean Absolute Error (MAE), where
important errors are not penalized more than small ones:
$$\text{MAE}(A) = \frac{1}{|\Rtest|} \cdot \sum_{\rui \in \Rtest}|\predrui -
\rui|.$$

As we said, RMSE probably is by far the most important the most popular measure for
evaluating the performance of a recommender system. The simple fact that \$1
million was awarded for a 10\% improvement of RMSE during the Netflix competition
illustrates the supremacy of RMSE. Yet this measure is also criticized, let
only because it is not very easy to interpret it in a meaningful way: if you
end-up with an RMSE of $0.5$, how do you know if it is good or not? You would
need to compare multiple algorithms before knowing what can be thought of as a
good RMSE, which by the way highly depends on  the rating scale.

Another issue with RMSE is that they do not properly reflect the way users
interact with a recommendation system.  Ultimately, the most important feature
of a recommender system probably is how well it can \textbf{order} the
different items for a given user, with regards to its preferences. Indeed, it
would be harmful to rank a disliked item higher than an item for which the user
has a strong preference, and conversely predicting a rating $\predrui$ of $2.5$
while the true rating is $1$ (i.e. with a potentially high error) is not as
serious, because the corresponding item would not have been recommended anyway.
The point here is that providing accurate predictions is crucial, but only on a
particular subset of predictions.

A related concern is that when using a suitable loss function, some algorithms
relying on an optimization process
can have a great RMSE score while still having no recommendation power.
As we will see, in Section \ref{SEC:experiments_clone}, a simple baseline
predictor  can outperform the RMSE of many other approaches, and yet this
algorithm has absolutely no recommendation power: the recommendations that it
outputs are the same of all the users.

To better reflect the user-system interaction, other precision-oriented metrics
are sometimes used in order to provide a more informed view.

\paragraph{Precision and recall\\}
Precision and recall help measuring the ability of a system to provide relevant
recommendations, and are therefore indicators of the performance of the
recommendation strategy $S$. They are defined by means of the number of true
positives, true negatives, false positives and false negatives. As such, they
are mostly used in an implicit rating scheme (or at least with binary rating
scales), but they can also be used when the rating scale is gradual. In this
latter case, the recommendation strategy has to be clearly defined.

In the following, we denote by $I^S$ the set of items that the strategy $S$
will suggest to the users using the predictions coming from $A$. For
ratings in the interval $[1, 5]$, a simple strategy could be for example to
recommend an item $i$ to user $u$ if the estimation rating $\predrui$ is
greater than $4$:
$$I^S \eqdef \Set{i \in I | \exists u \in U, ~\predrui \geq 4,~ \rui \in \Rtest}.$$

We also define $I^*$ as the set of items that are \textbf{actually} relevant to
the users, i.e.  the set of items that would have been recommended to users if
all the predictions made by $A$ were exact:
$$I^* \eqdef \Set{i \in I | \exists u \in U, ~\rui \geq 4,~ \rui \in \Rtest}.$$

\noindent
The \textbf{precision} of the
system is
defined as the fraction of recommended items that are relevant to the users,
and the \textbf{recall} is defined as the fraction of relevant recommended
items over all relevant items:
\begin{align*}
  \text{Precision} &\eqdef \frac{\mid I^S \cap I^*\mid}{\mid I^S \mid},\\
  \text{Recall} &\eqdef \frac{\mid I^S \cap I^*\mid}{\mid I^* \mid}.
\end{align*}

Precision and recall are complementary metrics: it would not make sense to
evaluate an algorithm performance by only looking at the precision, without
considering the recall. It is indeed quite easy to obtain a high precision (by
simply recommending all of the items), but that would lead to a terrible
value of recall. Precision and recall are commonly summarized into the
F-measure, which is their harmonic mean:
  $$
  \text{F} \eqdef 2 \cdot \frac{\text{Precision} \cdot
  \text{Recall}}{\text{Precision} + \text{Recall}}.
  $$

If accurate predictions are crucial, it is widely agreed that it is
insufficient for deploying an effective recommendation engine. Indeed, still
other dimensions are worth estimating in order to get a complete picture of the
performance of a system
\cite{NeeRieKonACM2006,HerKonJohTerRieACM2004,KamBriRecSys2014}.  For instance,
one may naturally expect from a recommender system not only to be accurate, but
also to be surprising, and to be able to recommend a large number of items.

\paragraph{Coverage\\}
In its simplest form, coverage is used to measure the ability of a system to
recommend a large amount of items: it is quite easy indeed to create a
recommender system that would only recommend very popular items. Such a
recommender system would drop to zero added value. We here define the
\textbf{coverage} as the proportion of recommended items out of all existing
items:
$$\text{Coverage} \eqdef \frac{\mid I^S\mid}{\mid I\mid}.$$

\paragraph{Surprise\\}
Users expect a recommender system to be surprising: recommending an extremely
popular item is not really helpful. Be warned though: surprise is one of the
most difficult notions to grasp, especially when it comes to assess it with
numerical measures. Following the works of \cite{KamBriRecSys2014}, we will
here define the surprise associated with a recommendation with the
help of the pointwise mutual information (PMI). The PMI between two items $i$
and $j$ is defined as follows:
$$\text{PMI}(i, j) \eqdef -\log_2 \frac{P(i, j)}{P(i)P(j)} / \log_2 P(i, j),$$
where $P(i)$ and $P(j)$  represent the probabilities for the items to be rated
by any user, and $P(i, j)$ is the probability for $i$ and $j$ to be rated
together. They are estimated by $P(i) \approx \frac{\mid U_i \mid}{\mid U
\mid}$ and $P(i, j) \approx \frac{\mid U_i \cap U_j \mid}{\mid U\mid}$. PMI
values fluctuate between the interval $[-1, 1]$, $-1$ meaning that $i$ and $j$
are never rated together and $1$ meaning that they are always rated together.
To estimate the surprise of recommending an item $i$ to a user $u$, the authors
propose two definitions:
\begin{itemize}
\item either to take the maximum of the PMI values for $i$ and all other items
  rated by $u$, with $\surpmax(u, i) \eqdef \max\limits_{j\in I_u}
    \text{PMI}(i, j),$
\item
 or to take the mean of these PMI values with $\surpavg(u, i) \eqdef
    \frac{\sum_{j \in I_u} \text{PMI}(i, j)}{\mid I_u\mid }$.
\end{itemize}

The overall capacity of a recommender to surprise its users can be defined as
the mean of all the surprise values for each prediction. Because they are
defined using the pointwise mutual information, the \textbf{lower} the values
of $\surpmax$ and $\surpavg$, the most \textit{surprising} the recommendations.


\paragraph{Other dimensions\\}

There are  many other measures that can assess the performances of a
recommender system (see \cite{ShaGun11} for an extensive survey). We can cite
for example \textbf{trust} which characterizes the confidence that a user would
put into a recommendation, or also \textbf{diversity} that evaluates how
various recommendation are distinct from each others.  Unfortunately these
dimensions are actually a lot harder to assess in an off-line setting, and
there are no popular quantitative measure that can evaluate them in a
satisfactory way.

We now detail two very popular collaborative filtering algorithms.

\section{Two collaborative filtering techniques: neighborhood-based, and matrix
factorization}
\label{SEC:neighborhood_and_matrix_facto_review}

We will here present two families of collaborative filtering algorithms: the
neighborhood approach based on the well known $k$-NN algorithm, and the matrix
factorization techniques whose groundings come from linear algebra and that
lead to elegant and accurate models. These two families of algorithms will
serve as baselines against which we will compare the performances of our own
algorithms in the next chapter, so it is important that we understand their
functioning in detail.

\subsection{The neighborhood approach}
\label{SEC:neighborhood_approach}

\subsubsection{Prediction framework}

The motto of collaborative filtering methods is to recommend some items that
are appreciated by other users having the same tastes. This principle is
carried out to the letter in neighborhood methods. Neighborhood approaches are instances of the general $k$-NN scheme. To estimate
the rating $\rui$ of a user $u$ for an item $i$, the most basic method consists
in computing  the set of $k$ users that are most similar to $u$ and
that have rated $i$. We will denote this set $N_i^k(u)$. The computation of
$N_i^k(u)$ depends of course on a similarity measure between users, which is
based on their respective ratings. The estimation $\predrui$ of $\rui$ is then
computed as an aggregate of the ratings $\rvi$, where $v$ is one of the
neighbors of $u$ in $N_i^k(u)$. Usually, the aggregation is simply a mean
weighted by the similarity between $u$ and $v$:

\begin{definition}[Neighborhood approach]
  The estimation of a rating $\rui$ using the neighborhood approach (also
  denoted $k$-NN here) is:
  $$\predrui = \frac{\sum\limits_{v \in N_i^k(u)} r_{vi} \cdot \text{sim}(u, v)}
  {\sum\limits_{v \in N_i^k(u)}\text{sim}(u, v)},$$
  where $N_i^k(u)$ is the set of users having rated $i$ with the highest $k$
  values of sim with $u$:
  $$N_i^k(u) \eqdef \Set{v \in U | \rvi \in R,~v\in \argmax_{u' \in
  U}^k\left[\text{sim}(u, u')\right]}$$
\end{definition}

The estimation process of the neighborhood approach is described in Algorithm
\ref{ALGO:neighborhood_prediction}.  Obviously, the training stage only needs
to be done once. Then, the
similarities can be reused for future predictions.

\begin{algorithm}[!ht]
 \caption{The neighborhood recommender.}
       \label{ALGO:neighborhood_prediction}
       \begin{algorithmic}

         \STATE {\bf Input}: A set of ratings $R$, and a couple $(u, i)$ for
         which $\rui$ is  unknown.
         \STATE {\bf Output}: $\predrui$, an estimation of $\rui$.
         \STATE \textit{Training stage:}
         \FORALL{$(u, v) \in U^2$}
         \STATE Compute $\ssim(u, v)$.
	    \ENDFOR
       \STATE \textit{Prediction stage:}
         \STATE $\text{denum} = 0, ~ \text{num} = 0$
         \STATE $U_i^s = \text{Sorted}(U_i)$  // \textit{Sort by value of
         $\ssim$ with $u$}.

         \FORALL {$v \in U_i^s[:k]$}
         \STATE \textit{I.e. for the  $k$ first users in $U_i^s$}
         \STATE \text{denum} += \ssim(u, v)
         \STATE \text{num} += $\rvi \cdot \ssim(u, v)$
         \ENDFOR

         \STATE $\predrui = \frac{\text{num}}{\text{denum}}$
\end{algorithmic}
\end{algorithm}

We need here to make an important point: instead of recommending an item to a
user, we could perfectly recommend a user to an item, which is a symmetric and
equivalent point of view. The prediction $\predrui$ would be the exact counterpart of the one we
described above, but instead of considering similarities between users, we
would compute similarities between items:
$$\predrui = \frac{\sum\limits_{j \in N_u^k(i)} r_{uj} \cdot \ssim(i, j)}
{\sum\limits_{j \in N_u^k(i)}\ssim(i, j)}.$$
The set $N_u^k(i)$ is defined as the counterpart of the set $N_i^k(u)$.
We can interpret this prediction as an edge-case of content-based
recommendation: we clearly are recommending to $u$ some items $j$ that are
close to those he has already rated in the past, which is exactly what a
content-based recommender does. In this case though, we did not use any
metadata about the items, and  items where represented as vectors of ratings.

The choice between a user-based or an item-based recommendation mainly depends on
the system at hand. When the number of users is high with regard to the number
of items, computing similarities between items can help saving memory and
computation time, but it is also important to assess the potential number of
neighbors for any given $u$ or $i$: it is always more desirable to compute
similarities over a high number of ratings. Also, item-based methods are
more easily justified: when presenting a recommendation to a user $u$, it is
easier to explain how two items relate to each other rather than involving
other users that $u$ does not even know.

\subsubsection{The similarity measures}
\label{SEC:similarity_measures}

There are many, many ways to define the similarity metric between two users (or
items).
Probably the most common one is the cosine similarity.

\begin{definition}[Cosine similarity]
  The cosine similarity between two users $u$ and $v$ is defined as:
$$
\text{cosine sim}(u, v) \eqdef \frac{ \sum\limits_{i \in \Iuv} \rui \cdot \rvi}
{\sqrt{\sum\limits_{i \in \Iuv} \rui^2} \cdot \sqrt{\sum\limits_{i \in \Iuv}
\rvi^2}}.
$$
\end{definition}

Here, users $u$ and $v$ are considered as vectors in a vector space defined by
the items they have both rated (the set of common items is $\Iuv$). Their
cosine similarity simply is the cosine of the angle between the two vectors. An
unnatural feature of this metric is that two vectors with (potentially
different) constant values will always have a similarity of $1$, because they
are collinear. For example if $u = (2, 2)$ and $v = (5, 5)$, $\text{cosine
sim}(u, v) = \frac{20}{\sqrt{8}\sqrt{50}} = 1$, while one would expect $u$ and
$v$ to be quite different w.r.t. their tastes.

Such a flaw can be overcame using the Pearson similarity, which can be viewed as
a mean-centered version of the cosine similarity.

\begin{definition}[Pearson similarity]
  The Pearson similarity between two users $u$ and $v$ is defined as:
$$
\text{Pearson sim}(u, v) \eqdef \frac{ \sum\limits_{i \in \Iuv}
(\rui -  \mu_u) \cdot (\rvi - \mu_{v})} {\sqrt{\sum\limits_{i
\in \Iuv} (\rui -  \mu_u)^2} \cdot \sqrt{\sum\limits_{i \in
\Iuv} (\rvi -  \mu_{v})^2}},
$$
where $\mu_u$ and $\mu_v$ are the average rating of users $u$ and $v$
respectively.
\end{definition}

One last similarity metric that we will use is the Mean Squared Difference
(MSD)\footnote{Strictly speaking MSD is actually a distance rather than a
similarity metric, so taking its inverse would do the job.}:
\begin{definition}[Mean squared difference]
  The mean squared difference between two users $u$ and $v$ is defined as:
$$\text{MSD}(u, v) \eqdef \frac{1}{|\Iuv|} \cdot \sum\limits_{i \in \Iuv} (\rui
- \rvi)^2$$
\end{definition}

Notice that none of these metrics take into account the support between the two
users $u$ and $v$, i.e. the number of items that they have commonly rated. It
would be foolish to have the same faith in a similarity computed over hundreds
of common items as in a similarity computed only over a few items. This is why
in practice, it is common to give a higher weight to the similarities that have
a high support. Another option is to \textit{shrink} the similarity measure of
two users toward zero if their support is low:
$$\text{shrunk\_sim}(u, v) \eqdef \frac{\mid \Iuv \mid - 1}{\mid \Iuv \mid - 1
+ \lambda} \cdot \ssim(u, v),$$
where $\lambda$ is a sort of regularization constant. Such shrinkage technique
can be motivated from a Bayesian perspective, and usually leads to significant
improvements in the system performances. It only makes sense however for
similarity measures that are centered around zero, such as the Pearson
similarity. We will not consider shrinkage in any of our experiments.

\subsubsection{Similarity computation}

We will now describe how to practically compute these similarity metrics. We
see that all of the presented metrics rely on the set of common items $\Iuv$.
In practice, the explicit computation of this set for every single pair of
users can be very expensive, and naive algorithms lead to poor performances.
Consider for example the naive similarity computation of Algorithm
\ref{ALGO:naive_sim}.
\begin{algorithm}[!ht]
 \caption{A general naive algorithm for similarity computation}
       \label{ALGO:naive_sim}
       \begin{algorithmic}

         \STATE {\bf Input}: A set of ratings $R$
         \STATE {\bf Output}: The similarity between all pairs of users.
         \FORALL{$u \in U$}
         \FORALL{$v \in U$}
         \FORALL{$i \in I_u$}
         \IF{$i \in I_v$}
         \STATE \textit{We know now that $i \in I_{uv}$, so we can use $r_{ui}$
         and $r_{vi}$ as we please, depending on the measure that is computed}.
         \ENDIF
        \ENDFOR
        \ENDFOR
        \ENDFOR
\end{algorithmic}
\end{algorithm}
In the worst case, we can consider that $\mid I_u \mid = \mid I_v \mid = \mid I
\mid$ for all $u$ and $v$, so the complexity of this naive algorithm is
$\mathcal{O}(\mid U \mid^2 \mid I \mid^2)$.

However, the complexity can be taken down by a great deal if we proceed in a
MapReduce fashion \cite{DeaGhe04}, as described in Algorithm \ref{ALGO:cosine}
which examplifies how to compute the cosine similarity.
\begin{algorithm}[!ht]
 \caption{Computation of the cosine similarity.}
       \label{ALGO:cosine}
       \begin{algorithmic}

         \STATE {\bf Input}: A set of ratings $R$.
         \STATE {\bf Output}: \text{cosine sim}(u, v) for all pairs of users.
         \STATE {\it Initialization ($n$  is defined as the number of users, i.e. $n
         = \mid U \mid$):}
         \STATE $\text{sim} = \text{null\_array}[n][n]$
         \STATE $\text{sum\_prod} = \text{null\_array}[n][n]$
         \STATE $\text{sum\_squ} = \text{null\_array}[n][n]$
         \STATE $\text{sum\_sqv} = \text{null\_array}[n][n]$
         \FORALL{$i \in I$}
           \FORALL{$u \in U_i$}
             \FORALL{$v \in U_i$}
              \STATE $\text{sum\_prod}(u, v) \pluseq \text{sum\_prod}(u, v) +
              \rui \cdot \rvi$
              \STATE $\text{sum\_squ}(u, v) \pluseq \text{sum\_squ}(u, v) +
              {\rui}^2$
              \STATE $\text{sum\_sqv}(u, v) \pluseq \text{sum\_sqv}(u, v) +
              {\rvi}^2$
             \ENDFOR
           \ENDFOR
         \ENDFOR
         \FORALL{$u \in U_i$}
           \FORALL{$v \in U_i$}
           \STATE $\text{sim}(u, v) = \frac{\text{sum\_prod}(u,
           v)}{\sqrt{\text{sum\_squ}(u, v) \cdot \text{sum\_sqv}(u, v)}}$
           \ENDFOR
         \ENDFOR
\end{algorithmic}
\end{algorithm}
Actually, any other similarity measure that rely on the set $I_{uv}$ can be
computed this way (see \cite{SchBodVolRECSYS12}). Considering again the worst
case where $\mid U_i \mid = \mid U \mid$, the complexity of Algorithm
\ref{ALGO:cosine} is  $\mathcal{O}(\mid I \mid \cdot \mid U \mid^2 + \mid
U \mid^2) = \mathcal{O}( \mid U \mid^2 \cdot [ \mid I \mid + 1])$, which is far
less than that of Algorithm \ref{ALGO:naive_sim}. Another undeniable advantage
of Algorithm  \ref{ALGO:cosine} is that it follows the MapReduce framework, and
as such it can be easily parallelized for another significant performance gain.
Indeed, the first three embedded \texttt{for} loops could be dispatched in various
clusters, each dealing with a given set of items (this is the Map stage). Each
of the clusters would have its own three sum matrices \texttt{sum\_prod},
\texttt{sum\_squ} and \texttt{sum\_sqi}, which could then all be merged to
rebuild the entire sum matrices (Reduce stage).

That's all for the neighborhood methods. In the next section, we describe
another very popular collaborative filtering technique, which models the data
in a significantly different (but meaningful!) way: matrix factorization
techniques.

\subsection{Matrix factorization techniques}
\label{SEC:matrix_facto}

About ten years ago, the Netflix company organized a competition where the goal
was to improve the RMSE of their standard prediction algorithm by $10\%$. The
challenge quickly became very popular, not only because the winning prize was
of \$1 million, but also because the dataset was orders of magnitude larger
than any other available dataset at the time: about 100 million ratings from
480.000 users and 17.700 movies. Unfortunately, the dataset is no longer
publicly available, but the challenge led to the emergence of many successful
recommendation techniques, among which matrix factorization methods clearly
stand out. Let's be honest: the author of this document has a crush on matrix
factorization techniques, and as a consequence they will be thoroughly
detailed. All the more, just like neighborhood approaches, they have become one
of the main baselines for benchmarking.

The matrix factorization approach we will describe here is heavily inspired by
the Singular Value Decomposition (SVD) of a matrix, one of the highlights of
linear algebra. Because having a basic understanding of SVD will be very
insightful for us, we will briefly review it now.

\subsubsection{Background on SVD}

Let's first dive into the
theory (but not for long):

\begin{proposition}
  Any real-valued matrix $R \in \mathcal{M}^{m \times n}$ of rank $r$ can be
  decomposed as the product of three matrices\footnote{The matrix $I$ is
  \textbf{not} the identity matrix!}:

  $$R = U\Sigma I^t,$$
  where $U\in \mathcal{M}^{m \times r}$, $\Sigma\in \mathcal{M}^{r \times r}$
  is a diagonal matrix, and $I\in \mathcal{M}^{n \times r}$. Such a
  factorization is called the Singular Value Decomposition of $R$.
\end{proposition}

As $\Sigma$ is a diagonal matrix, it only act as a scaling factor for either $U$ or
$I$. For the sake of simplicity, we will consider that the decomposition can be
written $R = UI^t$, where $\Sigma$ has been merged into either $U$ or $I$. In
practice, we know how to compute the the two matrices $U$ and $I$: their
columns are are the
eigenvectors\footnote{For this reason, SVD and Principal Component Analysis are
strongly related.} of the two matrices $R^tR$ and $RR^t$, and their associated
eigenvalues are the squared entries of $\Sigma$, called the singular values
(hence the name of the factorization).

The key point is that the columns of $U$ are actually an \textbf{orthonormal
basis} for
the column space of $R$, and the columns of $I$ are an \textbf{orthonormal
basis} for the
row space of $R$. Maybe this statement is worth some explanation. The column
space of $R$ is the vector space that is spanned by the $n$ columns or $R$,
i.e. the set of vectors that are a linear combination of the columns of $R$. As
some of the columns of $R$ may be linearly dependent, this vector space is of
dimension $r\leq n$: the rank of a matrix is defined as the number of
independent columns, and equivalently as the number of independent rows. Our
statement says that the columns of $U$ span the same space, and particularly that they form
an orthonormal basis of $r$ vectors. The same goes for the $r$ orthogonal columns
of $I$, which span the row space of $R$.

A particular case of this general statement is that \textbf{any column of $R$
can be expressed as a unique linear combination of the columns of $U$}. In
fact, we already know that for any matrix  $AB = A \times B$, the columns of $AB$ are
linear combinations of the columns of A (and the rows of $AB$ are linear
combinations of the rows of $B$). But the key point here is that the columns $U_i$
actually span the whole column space of $R$. As the
columns of $U$ are orthonormal, each column has a unique contribution that
cannot be compensated by the others. Symmetrically, \textbf{any row of $R$ is a
linear combination of the columns of $I$.} What does it have to do with our
recommendation problem? Imagine for a moment that $R$ is a dense rating matrix,
where the rows represent items and the columns represent users:
$$
R = \begin{blockarray}{cccc}
  \text{Alice} & \text{Bob} & \text{Charlie} \\
\begin{block}{(ccc) c}
  4 & 5 & 5 & \text{Titanic} \\
  2 & 1 & 1 & \text{Toy Story} \\
  1 & 2 & 2 & \text{LORT} \\
  3 & 3 & 3 & \text{Mad Max} \\
  4 & 2 & 2 & \text{E.T.} \\
\end{block}
\end{blockarray}
=
\begin{pmatrix}
  \vertbar & \vertbar & & \vertbar\\
  U_1& U_2 & \cdots & U_r\\
  \vertbar & \vertbar & & \vertbar\\
\end{pmatrix}
\cdot
\begin{pmatrix}
  \horzbar& I_1 & \horzbar\\
  \horzbar& I_2 & \horzbar\\
   & \vdots & \\
  \horzbar& I_r &\horzbar \\
\end{pmatrix}
,
$$
where $U_1, \cdots U_r$ are the columns of $U$ and $I_1, \cdots I_r$ are the
columns of $I$ (i.e. the lines of $I^t$).

When we compute the SVD of $R$, we find in the columns of $U$ some
\textit{prototype} users that have no real existence, but that can be combined
(linearly) to build up any of the users: Alice, Bob and Charlie are linear
combinations of $U_1, U_2, \dots U_r$. Also, any linear combination of Alice,
Bob and Charlie can be expressed as a linear combination of the $U_i$. Similarly, the columns of $I$ are
\textit{prototype} movies that do not properly exist but that can be combined
to build up Titanic, Toy Story, etc., and any of their linear combinations. We can consider each vector $I_i$
to be some sort of idealized movie type, for example $I_1$ is a typical action
movie, while $I_2$ would be a typical romantic movie, etc. Take the example of
Titanic, which is an \textit{action-packed romance} movie. Thanks to the SVD,
we can express Titanic  as a linear combination $\alpha_1 \cdot I_1 + \alpha_2
\cdot I_2 + \cdots + \alpha_r \cdot I_r$, and as $I_1$ and $I_2$ represent
typical action and romantic movies, the values $\alpha_1$ and $\alpha_2$
will be high. Also, if
$I_3$ is a typical science-fiction movie, $\alpha_3$ should be fairly low (or
even negative) for Titanic. The magic of SVD is that all these coefficients (or factors)
are automatically derived from the matrix $R$.

Similarly, the $U_i$'s can be viewed as some idealized critics, with specific
tastes toward some particular kind of movies. Obviously in practice, there is
no way to associate a clear semantic meaning to the different $U_i$ or $I_i$,
and the way they are defined only depends on the matrix $R$.

Moving further, let's now focus on the meaning of a single rating $r_{ui}$.
Each $\rui \in R$ is defined as a dot product $\rui =
{q_i}^t \cdot p_u$, where $q_i \in \mathbb{R}^r$ is a row in $U$ and represents
the item $i$, and $p_u \in \mathbb{R}^r$ is a column in $I^t$ and represents the
user $u$. We can now understand that \textbf{$p_u$ represents how well $u$
agrees with each of the $r$ prototype movies, and $q_i$ represents how well $i$
suits the tastes of the $r$ prototype users}.

\begin{testexample}
As an example, let us consider the rating of Charlie for Titanic. We will
assume that the rank $r$ is $3$ and that the three movies types are
\textit{Action}, \textit{Romance}, and \textit{Science fiction}. Charlie is a
guy who loves action movies and likes romantic movies, but does not like
science fiction at all. The two vectors $p_u$ and $q_i$ are described in Table
\ref{TAB:Charlie_Titanic}, and the rating $r_{ui}$ is high because Titanic
has all the attributes (or factors) that Charlie likes\footnote{These values
are completely arbitrary and obviously are not necessarily those that would
appear in the true SVD of $R$.}.
\end{testexample}

\begin{table}[h!] \centering \begin{tabular}{ l   c  c  c }
\toprule
    & Action & Romance & Science fiction\\
  \midrule
    Titanic $q_i$ & 1 & 3 & 0\\
    Charlie $p_u$ & 2 & 1 & -1\\
\bottomrule
\end{tabular}
  \caption{Charlie's rating for Titanic is ${q_i}^t \cdot p_u = 1 \cdot 2 + 3
  \cdot 1 + 0 \cdot (-1) = 5$.}
\label{TAB:Charlie_Titanic}
\end{table}

\subsubsection{Application to recommendation}

The above statement is the principal motivation for the use of SVD in recommendation
settings. It postulates the existence of $f$ factors/criteria (whose nature is
not necessarily known) that determine the value of any rating $\rui$.  A user
$u$ is modeled as a vector $p_u \in \mathbb{R}^f$, where each component of
$p_u$ models the importance of the corresponding factor for $u$.  Similarly, an
item $i$ is modeled as a vector $q_i \in \mathbb{R}^f$, where each component of
$q_i$ models how well $i$ fits the corresponding criteria.  From then, a rating
prediction $\predrui$ is calculated as the dot product of the two vectors $p_u$
and $q_i$:
$$\predrui = {q_i}^t \cdot p_u.$$

Once the number of factors $f$ is set\footnote{$f$ must be lower than or equal
to the rank $r$. When $f < r$ we end up with a \textbf{low-rank} approximation
of the matrix $R$.}, the problem is here to estimate the
vectors $p_u$ and $q_i$ for every possible user and item. We saw that when the
matrix $R$ is dense (i.e. when  there is no unknown entry), there exists an analytic
solution for computing the two matrices $U$ and $I$. An equivalent solution is
to solve the following minimization problem\footnote{From then on we will abuse
notation and use $R$ either as a matrix (sparse or dense) and as a rating
dataset as defined in Section \ref{SEC:recommendation_rating_prediction}.},
with the constraint that the vectors $q_i$ must be orthogonal, as well as the
vectors $p_u$:
$$
\sum_{\rui \in R} \left(\rui - {q_i}^t \cdot p_u \right)^2.
$$

But what happens in a recommendation setting, where the matrix $R$ is extremely
sparse? The SVD of $R$ is not even properly defined! One of the main
contribution during the Netflix competition was made by Simon Funk in a blog
post\footnote{\url{http://sifter.org/~simon/journal/20061211.html}}, who empirically
showed that \textbf{we should actually just not care} that the matrix $R$ is
sparse, and still solve the same optimization problem only using the known
ratings.  When $R$ is sparse, the problem is not convex in both $p_u$ and $q_i$
so this optimization will only lead to a local minimum, but this method turned
out very efficient in practice. Before that, techniques usually involved  a
preliminary step where the matrix $R$ was filled using some heuristic, in order
to have a fully defined problem \cite{SarKarKonRie00}. Funk's contribution led
him to the top 10 competitors of the Netflix competition at the time, and was
heavily used by other competing teams, including the BelKor team who ended up
winning the contest \cite{Kor09}. It also turns out that the orthogonality
constraint is useful for interpretation purposes, but in practice it usually
leads to a lower generalization power, so we will simply not bother and allow
the factor vectors to be non-orthogonal.

The appeal of this basic approach does not only rest upon its link to SVD, but
also because it is easily extendable. It was later improved and theoretically
studied in many ways, for example by incorporating user and items biases
\cite{KorACM2010} (which we will further detail later in Section
\ref{SEC:current_advances_neighborhood_techniques}), by adding the ability to
incorporate implicit ratings along with explicit ones \cite{Pat07, KorACM2010},
or by adding some time-dependent features, taking into account items popularity
over time \cite{Kor09}. Still in \cite{KorACM2010}, a model combining matrix
factorization techniques and neighborhood approaches was proposed. For a
complete overview mixing all previously mentioned methods, see \cite{KorBel11}
or \cite{KorBelVol09}.

In \cite{SalMni07}, the matrix factorization we just saw was studied from a Bayesian
perspective, and was named Probabilistic Matrix Factorization (PMF), leading to
the following regularized least squares problem:

$$
\sum_{\rui \in R} \left(\rui - {q_i}^t \cdot p_u \right)^2 +
\lambda\left(\norm{}{q_i}^2 + \norm{}{p_u}^2\right),
$$
where $\lambda$ is a regularization term. Not
surprisingly, stochastic gradient descent (SGD) tend to perform really well
when it comes to the solving of this problem, and is quite simple to implement.
We have already talked too much about matrix factorization, but a few lines
more won't hurt so now propose to derive the associated SGD algorithm.

\subsubsection{Optimization by Stochastic Gradient Descent}

When we need to minimize a loss function $L$ of the form $L(\theta) = \sum_k
L_k(\theta)$, where $\theta$ is a (potentially multidimensional) parameter, SGD
comes up as a very popular optimization technique. While a vanilla gradient
descent would require to compute the derivative of $L$ with respect to
$\theta$, in SGD we just assume that the derivative of each $L_k$ is enough.
SGD is an iterative optimization process where the value of $\theta$ is updated
using the following rule: $\theta = \theta - \alpha \frac{\partial
L_k}{\partial \theta}$ where $\alpha$ the learning rate (or stepsize). This
update is performed over all $k$' for a given number of times. Various
theoretical guarantees about the convergence of this algorithm have been
derived, that obviously depend on the stepsize and on some regularity
conditions on the loss function $L$: see \cite{BotCurNoc16} for a recent
overview.

As far as we are concerned, our loss function is
$$
L(p_u, q_i) \eqdef \sum_{\rui \in R} \left(\rui - {q_i}^t \cdot p_u \right)^2 +
\lambda\left(\norm{}{q_i}^2 + \norm{}{p_u}^2\right).
$$
Let $L_{ui}$ be the cost associated with a single prediction $\rui$: $$L_{ui}
\eqdef \left(\rui - {q_i}^t \cdot p_u \right)^2 + \lambda\left(\norm{}{q_i}^2 +
\norm{}{p_u}^2\right).$$

Denoting err the prediction error ($\text{err} = \rui - {q_i}^t \cdot p_u$), the
derivative of $L_{ui}$ with respect to $q_i$ is (remember that $q_i$ is a
vector):
$$\frac{\partial L_{ui}}{\partial q_i} = -\text{err} \cdot p_u + 2\lambda
q_i.$$
Similarly, the derivative of $L_{ui}$ with respect to $p_u$ is:
$$\frac{\partial L_{ui}}{\partial p_u} = -\text{err} \cdot q_i + 2\lambda
p_u.$$
With these derivatives in mind, the optimization problem can then be easily
solved by following Algorithm \ref{ALGO:matrix_facto_sgd}.

\begin{algorithm}[!ht]
 \caption{Stochastic Gradient Descent for matrix factorization.}
       \label{ALGO:matrix_facto_sgd}
       \begin{algorithmic}

         \STATE {\bf Input}: A set of ratings $R$, a learning rate $\alpha$, a
         regularization penalty $\lambda$ and a number of iterations $n$.
         \STATE {\bf Output}: $p_u$ and $q_i$, the user and item factors.
         \STATE {\it Initialization:}
         \STATE Randomly initialize $p_u$ and $q_i$
         \FOR{$\text{iteration} \in [1, n]$}
         \FORALL{$\rui \in R$}
         \STATE $\text{err} = \rui - {q_i}^t \cdot p_u$
         \STATE $q_i = q_i - \alpha (\text{err} \cdot p_u - \lambda q_i)$
         \STATE $p_u = p_u - \alpha (\text{err} \cdot q_i - \lambda p_u)$
         \ENDFOR
         \ENDFOR
\end{algorithmic}
\end{algorithm}

Another optimization technique can be used, by noting that if either $p_u$ or
$q_i$ is fixed, we obtain a convex problem that can be solved using classical
least squares methods. The idea is then to first consider all $q_i$ as
constants and solve the associated linear problem. Then, the $p_u$ are
considered constant and the associated problem is solved. Repeating these steps
an arbitrary number of time will also converge to a local solution, and this
methods is known as Alternating Least Squares \cite{BelKor07}.

\section*{Conclusion}

The purpose of this chapter was to provide the necessary background and tools,
in preparation of the next chapter where we will describe our contributions on
analogical recommendation.

We have seen that the three main families of recommender systems are the
content-based, collborative and knowledge-based techniques. Collaborative
filtering is by far the most popular family of techniques today, so we will try
to address this problem in our contributions. In particular, we will try to
build algorithms that are capable of predicting the rating $\rui$ that a user
$u$ has given to an item $i$, on the basis of some past rating history.

We thoroughly detailed two popular collaborative filtering techniques. The
first one is the neighborhood-based method, which is a direct implementation of
the general $k$-NN strategy. The neighborhood of each user is computed using a
similarity metric that is based  on the ratings: two users are considered close
if they like the same items and dislike the same items. Then, the prediction
$\predrui$ is set as an aggregate (usually a weighted mean) of the ratings of
the neighbors of $u$ for the item $i$. These neighborhood-based methods tend to
model local relationship in the data. In constrast, the matrix
factorization-based techniques, whose popularity has grown during the Netflix
Prize, tend to model global effects in the data. These techniques define ratings
in terms of user and item factors. Each user $u$ is modeled as a vector of
factors which indicate how strong are is taste of $u$ for the given factors.
Each item $i$ is also modeled as a vector of factors which indicate how well
$i$ corresponds to the given factors. The prediction $\rui$ is then set as the
scalar product between these two vectors, while the vectors are estimated using
a global estimation procedure (usually) based on stochastic gradient descent.
These two techniques (neighborhood and matrix factorization) will serve as
baselines in the next chapter to compare our algorithms performances.

We now have enough background on the recommender systems field to apply
analogical learning to a recommendation task. This is the purpose of the next
chapter.
