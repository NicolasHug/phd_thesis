\chapter{Analogy-preserving functions}

\initial{W}e first introduced the analogical inference principle in Chapter
\ref{CHAP:formal_analogical_proportions}. This principle states that if four
elements are in analogy, then their classes (or any relevent attribute) should
also be in proportion. This is of course an unsound inference principle, in
that the conclusion does not always follows from the premices. Nonetheless, we
have seen in Chapter \ref{CHAP:functional_definition} that it can still be used
to design classifiers, that we have named conservative and extended
classifiers.  Both in \cite{BayMicDelIJCAI07} and in our experiment section
(Section \ref{SEC:experiments_and_empirical_validation}), these classifiers
showed promising results, but were still only used in artificial Boolean
classification problems. Analogical classifiers still had to prove their worth
in real-world applications, and this was precisely the purpose of the last
chapter, where we designed various techniques to apply analogical inference on
a recommendation task.

Unfortunately, the results of a direct application of the analogical inference
principle did not quite live up to our expectations. The conclusion of the last
chapter offered a partial explaination to these modest results, stating that
there were just too few decent analogies to be found on the datasets that we
used. Even if this argument is perfectly admissible, we will not spare ourself
the very questionning of the analogical inference principle. We say that if
four elements are in proportion then so are their classes... But why should it
be true, or even useful? It is indeed pretty easy to build up simple problems
where the application of this principle leads to disastrous results.

Take for example the problem of Figure \ref{FIG:classif_in_R2}. We have
a set $S$ of points (on the left) that belong to two classes that are linarly
separable: an instance is a square if it is on the left side of the space, else
it is a circle. A naive application of the analogical inference principle would
lead us to believe that for any element that is in proportion with a $3$-tuple
in $S^3$ (i.e. the element is the fouth vertex of a parallelogram that we can
build with three elements in $S$), then the classes of the four vertices are in
proportion. In the right figure, we show all these elements, along with their
estimated classes. The result is absolutely terrible, because the whole space is
covered with instances belonging to both of the classes.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{figures/AE_in_R2_S.pdf}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{figures/AE_in_R2_AE.pdf}
  \label{fig:sub2}
\end{subfigure}
  \caption{A naive application of the analogical inference principle in
  $\mathbb{R}^2$.}
\label{FIG:classif_in_R2}
\end{figure}

But if it is so easy for analogical inference to fail, then why does it still
work in some cases? Back to Figure \ref{FIG:nan_vs_nn} on page
\pageref{FIG:nan_vs_nn}, the accuracies of the NAN algorithm were definitely
not as bad as that of the problem we have just seen, and some of them were
actually really good. A first obvious difference between the two experiments is
that the one we just described deals with the arithmetic proportion with
real-valued instances, while the other one deal with Boolean attributes. Does
it mean that the analogical inference principle is bound to fail with the
arithmetic proportion? We will probably not be able to completly answer this
question, even though we will provide some insights.  We will instead focus on
the Boolean setting which has seemed to be the most promising for analogical
inference so far, and from there we will derive some links with the real-valued
case.

In this chapter, we will provide a complete characterisation of functions that
are fully compatible with the analogical inference principle in a Boolean
setting. Let us restate the inference principle in this context to explain what
we mean: for any four elements $\mathbf{a}, \mathbf{b}, \mathbf{c}, \mathbf{d}$
in $\mathbb{B}^m$, the analogical inference principle states that if
$\mathbf{a}: \mathbf{b}:: \mathbf{c}: \mathbf{d}$, then $f(\mathbf{a}):
f(\mathbf{b}):: f(\mathbf{c}): f(\mathbf{d})$ where $f$ is a Boolean function
from $\mathbb{B}^m$ to $\mathbb{B}$. We have said it before:  this principle is
unsound. The goal of this chapter is to identify all of the functions $f$ that
lead to a valid conclusion, i.e. the functions such that $\mathbf{a}:
\mathbf{b}:: \mathbf{c}: \mathbf{d} \implies f(\mathbf{a}):f(\mathbf{b})::
f(\mathbf{c}): f(\mathbf{d})$ (with some minimal and natural requirements). We
will say that these functions are \textbf{analogy preserving}.

We will tackle this problem by adopting the general point of view of training
set extension. In Section \ref{TODO}, we first define the problem of training
set extension, described some previous works and explain how this problem will
motivate the research of analogy preserving functions.

\section{Extending a training set}

We have proved in Chapter \ref{CHAP:functional_definition} that the analogical
classification process can be formalized via two conceptual steps:
\begin{enumerate}
  \item First, the analogical extension of the training set is computed. This
    extension is the set of all elements in the instance space that are in
    analogy with one $3$-tuple in the training set, provided that the
    associated class equation is solvable. Each element of the extension is
    associated with an estimated label: the analogical label.
  \item Then, a $k$-NN algorithm is run, considering that the training set is
    actually the analogical extension. This $k$-NN classifier allows to
    classify all the elements of the universe that are not in the analogical
    extension.
\end{enumerate}

But then a natural question arises: why should we stick to a $k$-NN classifier?
The reason why the $k$-NN algorithm is used is because in their algorithmic
description, extended learners rely on an analogical dissimilarity, which is
strongly related to a distance on the instance space. But now that we have an
equivalent definition of extended classifiers that does not rely explicitely on
an analogical dissimilarity, nothing actually prevents us from using any other
classifier such as a decision tree, a support vector machine, or any of the
dozens of learners available out there. This would lead us to a different
paradigm for analogical learning: you first extend the training set by building
the analogical extension, and then you are free to do how you please and use
this extension as a bigger training set with any classifier.

In the next subsection, we will formally describe the problem of training set
extension, and present some previous works. Then in subsection
\ref{SEC:analogy_preserving_presentation}, we will introduce the class of
analogy prserving functions, which lead to a perfectly sound training set
extension in all cases.

\subsection{Previous works}

Machine learning algorithms usually require training on sufficiently large
datasets. The problem of learning from few examples is not new and is
referred-to as one-shot learning, where we rely on a transfer of knowledge from
known classes to unkown classes: see  for example \cite{LiFerPerPAMI06} in a
pattern recognition context.  But other options are available when the number
of training instances is small, and training set extension naturally comes in
as a handy tool. Indeed, the extension of the training set is a simple idea to
improve generalization power of the learner.  The more examples you have, the
better you learn! The point is here to add to the training set $S$ some new
examples, and to try do that in a way that preserves the \textit{quality} of
$S$.

Formally, we start with a set $S= \{\mathbf{x}^{(i)} \in \mathcal{X}| i \in
[1,n]\}$ of examples ($n$ is supposed to be small), where $\mathbf{x}^{(i)}$ is
an element of a Cartesian product $\mathcal{X} = X_1 \times \ldots \times X_m$.
For each element  $\mathbf{x}^{(i)} \in S$, we associate a target
$f(\mathbf{x}^{(i)})=y^{(i)} \in Y$.  In the case of regression, $y^{(i)} \in
\mathbb{R}$, and in the case of classification $y^{(i)}$ belongs to a finite
set.

Actually, only a few methods have been proposed for extending a sample set with
new examples, and most of them are specific to a particular application. For
example in \cite{CanPerArlLlo06}, some techniques are proposed but they are
only relevent for character recognition tasks. Intuitively, we may build a new
example starting from 1, 2 or 3 known examples.
\begin{enumerate}
\item With one example, a natural way to proceed is to use the classical
  neighborhood approach: given one  example $(\mathbf{a},f(\mathbf{a}))$, we
    can generate a new example $(\mathbf{b},f(\mathbf{b}))$ where  $\mathbf{b}$
    is not too far from $\mathbf{a}$ and $f(\mathbf{b})$ is not too far from
    $f(\mathbf{a})$. In the classification case, $f(\mathbf{b})$ may be chosen
    as $f(\mathbf{a})$. This is the setting in which \cite{CanPerArlLlo06}
    operates, where new characters are generated by slanting, shrunking, and
    dilation of training instances.
\item With two examples, the previous neighborhood option is still available
  and leads to interpolate the new example from the two given ones.  A somehow
    different option is the Feature Knockout procedure \cite{WolMar04}, which
    amounts to build a third example obtained by modifying a randomly chosen
    feature of the first example with that of the second one.  This way to
    proceed enjoys nice properties and appears to be equivalent to a popular
    regularization (Tikhonov) technique in the case of linear regression.  A
    related idea is used in a recent proposal \cite{BouPraRicECAI16} which
    introduces a measure of oddness w.r.t. a class that is computed on the
    basis of pairs made of two nearest neighbors in the same class; this is
    equivalent to replace the two neighbors by a fictitious representative of
    the class.

\item With three examples $(\mathbf{a},f(\mathbf{a})),
  (\mathbf{b},f(\mathbf{b})), (\mathbf{c},f(\mathbf{c}))$, the previous options
    remain available and lead to build a fourth example which is somehow
    in-between the three other ones: we still have some kind of interpolation.
    A quite different idea is to extrapolate the fourth item on the basis of
    analogical proportion.  In this perspective, this fourth element is not
    necessarily in the neighborhood of the three others. In fact, this idea has
    already been addressed in \cite{BayMouMicAnqECML07}.
\end{enumerate}

As far as we know, the problem of generation of new examples has never been
addressed from a theoretical point of view (except for the Knowkout procedure,
which is only relevent in some particular settings), and this is actually quite
understandable. So that they can be used in an extended training set, the new
generated examples have to be assigned an estimated label... But how do we
estimate these labels? Well there is a name for such problems: it is called
classification, and classification was the actual reason why we wanted to
extend the training set in the first place, so we have a vicous circle here.
In this some sense, extending a training set with potentially noisy data
amounts to nothing but performing classification on a specific set of
instances.

However, training set extension can still be extremely useful if we are
perfectly sure that the new added examples are sound, i.e. that their estimated
labels are the correct ones. In the next subsection, we will formally define
this problem in the context of analogical learning. Section \ref{TODO} will
then be devoted to the identification of the cases where such a perfect
extension is feasible.

\subsection {Analogy preserving functions: a safe way to extend}
\label{SEC:analogy_preserving_presentation}

Algorithmically, the generation of $\esf$ and the computation of the
analogical labels goes as follows:
\begin{enumerate}
  \item Add every $\mathbf{x} \in S$ to $\esf$. Then, for every
    $\mathbf{a},\mathbf{b},\mathbf{c} \in S$ such that $f(\mathbf{a}) :
    f(\mathbf{b}) :: f(\mathbf{c}) : y$ is solvable and such that there is
    $\mathbf{x} \in \mathbb{B}^m \setminus S$ with $\mathbf{a} : \mathbf{b} ::
    \mathbf{c} : \mathbf{x}$, add $\mathbf{x}$ to $\esf$ and save
    $y$ as a candidate for $\albl{\mathbf{x}}$.
\item Then for every $\mathbf{x} \in \esfs$, run a
  \textbf{majority-vote procedure}: set $\albl{\mathbf{x}}$ as the most
    common candidate among all solutions $y$ (in case of a tie, then randomly
    pick one of the values). For elements in $S$, $\albl{\mathbf{x}}$ is
    simply set to $f(\mathbf{x})$.
\end{enumerate}

\noindent
The extension $\esf$ is then considered to be an extended training set, where
the labels are the analogical labels, which are not necessarily correct. Indeed
for some elements $\mathbf{x} \in \esfs$, it may happend that
$\albl{\mathbf{x}} \neq f(\mathbf{x})$, simply because the analogical inference
principle may not be \textit{suitable} for the function $f$.  In this chapter,
we  are precisely interested in the identification of all Boolean functions $f$
that lead to perfectly sound extensions, i.e.  extensions where the analogical
labels $\albl{\mathbf{x}}$ are always equal to the ground truth values
$f(\mathbf{x})$. These functions will be called \textbf{Analogy preserving}
functions:

\begin{definition}
  We say that $\mathbf{E}_S(f)$ is {\bf sound} if
  $\albl{\mathbf{x}}_f=f(\mathbf{x})$, for every $\mathbf{x} \in
  \mathbf{E}^*_S(f)$. In other words, $\mathbf{E}_S(f)$ is sound if $\omegasf
  \eqdef P\left(\albl{\mathbf{x}} = f(\mathbf{x}) \given[\big] \mathbf{x} \in
  \esfs\right) = 1$.

  \noindent
  Also, if $\mathbf{E}_S(f)$ is sound for all $S \subseteq
  \mathbb{B}^m$, we say that $f$ is {\bf Analogy Preserving} (AP).
\end{definition}

\noindent
Proposition \ref{PROPOS:equivalent_def_AP} gives an equivalent definition of AP
functions, that will be more useful to convey our proofs:

\begin{proposition}
  \label{PROPOS:equivalent_def_AP}
  A function $f \colon \mathbb{B}^m \to \mathbb{B}$ is AP iff for every
  $\mathbf{a}, \mathbf{b}, \mathbf{c}, \mathbf{d} \in \mathbb{B}^m$, $f$ suits
  the following requirement:
  $$
  \begin{cases}
    \mathbf{a} :  \mathbf{b} ::  \mathbf{c} :  \mathbf{d} \emph{ and }\\
    \emph{solvable}(f(\mathbf{a}), f(\mathbf{b}),  f(\mathbf{c}))
  \end{cases}
  \implies \emph{sol}\left(f(\mathbf{a}),  f(\mathbf{b}),  f(\mathbf{c})\right) =
  f(\mathbf{d}). $$
\end{proposition}
\begin{proof}
  If $f$ fulfills  this requirement, then it is clear from the algorithmic
  description given above that for any $S \subseteq \mathbb{B}^m$ and
  $\mathbf{x} \in \esfs$, all the candidates for $\albl{\mathbf{x}}$ are equal
  to $f(\mathbf{x})$, so $\albl{\mathbf{x}}$ will be invariably set to
  $f(\mathbf{x})$ by the majority-vote procedure, which makes $f$ AP.

If $f$ does not suit this requirement, then there exist $\mathbf{a},
  \mathbf{b}, \mathbf{c}$, and $\mathbf{d} \in \mathbb{B}^m$ such that
  $\mathbf{a} : \mathbf{b} :: \mathbf{c} : \mathbf{d}$ but the solution
  $\sol(f(\mathbf{a}), f(\mathbf{b}), f(\mathbf{c}))$ is not equal to
  $f(\mathbf{d})$. Taking $S_0 = \{\mathbf{a}, \mathbf{b}, \mathbf{c}\}$ we
  obtain $\mathbf{E}^{f*}_{S_0} = \{\mathbf{d}\}$, and since $\albl{\mathbf{d}}
  = \sol(f(\mathbf{a}), f(\mathbf{b}), f(\mathbf{c})) \neq f(\mathbf{d})$, then
  $\mathbf{E}_{S_0}^f$ is not sound so $f$ is not AP.
\end{proof}

\noindent
The rest of this TODO will be devoted to a complete characterization of AP
functions.

First note that many natural functions are not AP. Consider for example the
binary function $f(x_1,x_2)= x_1 \wedge x_2$, along with $\mathbf{a},
\mathbf{b}, \mathbf{c}, \mathbf{d} \in \mathbb{B}^2$ in Table
\ref{exampleNotAP}.  We have $\mathbf{a} : \mathbf{b} :: \mathbf{c} :
\mathbf{d}$ and $f(\mathbf{a}) : f(\mathbf{b}) :: f(\mathbf{c}) : y$ is
solvable, yet the solution is $\sol(f(\mathbf{a}), f(\mathbf{b}),
f(\mathbf{c}))=0$, which  is different from $f(\mathbf{d})=1$ so $f$ is not AP.
This actually comes from the fact that analogical proportions are not stable by
conjunction combination. It is also the case for the disjunction
\cite{PraRic14}.

\begin{table}[ht]
  \center
$\begin{array}{cccc}
  \toprule
  ~ & x_1 & x_2 & f(\cdot) \\
  \midrule
  \mathbf{a} & 0 & 0 & 0\\
  \mathbf{b} & 0 & 1 & 0\\
  \mathbf{c} & 1 & 0 & 0\\
  \mathbf{d} & 1 & 1 & 1\\
  \bottomrule
\end{array}
$\bigskip
\caption{$f(x_1,x_2)= x_1 \wedge x_2$ is not AP.}
\label{exampleNotAP}
\end{table}

\section{A complete characterization of analogy preserving functions}

To complete our proofs, we will need to recall some background.

\subsection{Prelinimary background on Boolean functions}


Our first step will be to use a unifying notation for all of the Boolean
functions, and to adopt an algebraic point of view.  In the following, the AND
operator `$\wedge$' will be denoted `$\cdot$', and `$+$' will now denote the
modulo-2 addition, equivalent to the XOR operator.  We shall make use of the
polynomial representation of Boolean functions, that we now describe. A
\textbf{ monomial} is a term of the form:
$$
\mathbf{x}_I=\underset{i\in I}{\prod}x_i, $$ for some possibly empty finite set of
positive integers $I$, where $|I|$ is called the \textbf{degree} of
$\mathbf{x}_I$. Simply put, a monomiam is a conjunction of variables. We take
the convention that $1$ is the empty monomial $\mathbf{x}_\emptyset $. A
\textbf{ polynomial} is a sum of monomials and its
degree is the largest degree of its monomials.  It is well-known
\cite{StoneAlgebra36,ZhegalkinAlgebra27} that any function
$f:\mathbb{B}^m\rightarrow \mathbb{B}$ is \textbf{uniquely represented by a
polynomial}, also called the \textbf{Algebraic Normal Form} (ANF):
$$f(x_1,\ldots,x_m)=\sum_{I\subseteq \{1,\ldots,m\}}a_I\cdot \mathbf{x}_I,$$
where each $a_I$ belongs to $\mathbb{B}$. Note that the constant function $0$
is represented by $a_\emptyset\cdot \mathbf{x}_\emptyset$ with $a_\emptyset
=0$. The degree of a function $f:\mathbb{B}^m\rightarrow \mathbb{B}$, denoted
$d(f)$, is defined as the degree of the unique polynomial representing $f$. In
the following, we will often will refer to $f$ as both a function object and as
the associated ANF. Here are the ANF of some common Boolean functions in
$\mathbb{B}^2$ or $\mathbb{B}$:

\begin{itemize}
  \item $\text{AND}(x_1, x_2) = x_1 \cdot x_2$ ;
  \item $\text{OR}(x_1, x_2) = x_1 + x_2 + x_1 \cdot x_2$ ;
  \item $\text{XOR}(x_1, x_2) = x_1 + x_2$ ;
  \item $\text{NEG}(x_1) = x_1 + 1$.
\end{itemize}

Another key concept of our proof is the notion of essential and inessential
variables.  For $k\in [1,m]$, $\boldsymbol{\alpha}\in \mathbb{B}^m$, and $c \in
\mathbb{B}$, let ${\boldsymbol{\alpha}}_{k}^c$ be the tuple in $\mathbb{B}^{m}$
whose $i$-th component is set to $c$ if $i=k$, and to $\alpha_i$ otherwise.  A
variable $x_i$ is said to be \textbf{inessential} in $f\colon \mathbb{B}^m\to
\mathbb{B}$ if for all $\boldsymbol{\alpha} \in \mathbb{B}^m$ and $c \in
\mathbb{B}$, $f(\boldsymbol{\alpha}^c_i) = f(\boldsymbol{\alpha}^{\neg c}_i)$.
Otherwise, $x_i$ is said to be \textbf{essential} in $f$, or that $f$ depends
on $x_i$. In simple terms, an essential variable is a variable that has the
\textit{ability} to change the value of $f$. In fact, it can be shown that if
$x_i$ is an inessential variable for a function $f$, then $x_i$ does not appear
in the ANF of $f$.  For example in $f(x_1, x_2, x_3) = x_1 \cdot x_3$, $x_1$
and $x_3$ are essential variables while $x_2$ is inessential.  We denote by
$\ess(f)$ the number of essential variables of $f$ (or \textbf{essential
arity}).

Two functions $f\colon \mathbb{B}^m\to \mathbb{B}$ and $g\colon \mathbb{B}^n\to
\mathbb{B}$ are said to be {\bf equivalent} if there exist two mappings
$\sigma\colon
[1,n]\to [1,m]$ and $\sigma'\colon [1,m]\to [1,n]$ such that
\begin{align*}
  f(x_1,\ldots , x_m)&=g(x_{\sigma(1)},\ldots,x_{\sigma(n)}) \text{ and} \\
   g(x_1,\ldots , x_n)&=f(x_{\sigma'(1)},\ldots,x_{\sigma'(m)}).
\end{align*}
In other words, $f$ and $g$ are equivalent if one can be obtained from the
other by permutation of variables, addition of inessential variables, or
identification of inessential variables. Another point of view is to consider
that two functions are equivalent if their ANF is the same, up to the
renaming of variables. For example, $f(x_1, x_2, x_3) = x_1
\cdot x_3$ and $g(x_1, x_2)  = x_1 \cdot x_2$ are equivalent functions. Note
that two equivalent functions necessarily have the same number of essential
variables. For further background in the theory of essential variables of
functions, see \cite{CouceiroTCS08, CouceiroDM09, SalomaaAASF63, WillardDM96}.

In our demonstrations, we will use the following property:

\begin{property}\label{equivalent_functions}
Let $f\colon \mathbb{B}^m\to \mathbb{B}$ and $g\colon \mathbb{B}^n\to
  \mathbb{B}$ be equivalent functions. Then $f$ is AP if and only if $g$ is AP.
\end{property}

This can be verified by noting that as the analogy in $\mathbb{B}^m$ is defined
component-wise, the permutation of variables has no effect on the equation and
its solution. Also, manipulation of inessential variables do not change the
value of the function $f$, and thus the AP property still holds.

We now define the concept of \textbf{section} of a function, also known as a
\textit{restriction}, or equivalently as the result of \textit{partial
application} in computer science. Let $f$ be a function $\mathbb{B}^m\to
\mathbb{B}$, and $(I, J)$ be a partition of $[1, m]$. With $\mathbf{x} \in
\mathbb{B}^m$ and $\boldsymbol{\alpha} \in \mathbb{B}^{|I|}$, the $I$-section
(or simply section) $f^{\boldsymbol{\alpha}}_I \colon \mathbb{B}^{|J|} \to
\mathbb{B}$ is the function that is obtained after setting all variables in $I$
to the components of $\boldsymbol{\alpha}$.  More formally, let us define
$(\mathbf{x}^{\boldsymbol{\alpha}}_I) \in \mathbb{B}^m$ as follows:
$$
\begin{cases}
(\mathbf{x}^{\boldsymbol{\alpha}}_I)_i \eqdef x_i \mbox{ if } i \notin I\\
(\mathbf{x}^{\boldsymbol{\alpha}}_I)_i \eqdef \alpha_j \mbox{ where } j \mbox{
  is the index of } i \mbox{ in } I.
\end{cases}
$$
$\mathbf{x}^{\boldsymbol{\alpha}}_I$ is nothing but the vector $\mathbf{x}$
where the components at the  indices in $I$ have been replaces by the
successive components of $\boldsymbol{\alpha}$. For instance, with $m=5, \mathbf{x}=(0,1,1,0,0), I=\{1,3,4\}$ and $\boldsymbol{\alpha}=(1,0,0)$ then
$(\mathbf{x}^{\boldsymbol{\alpha}}_I)=(1,1,0,0,0)$.
The $I$-section $f^{\boldsymbol{\alpha}}_I$ is a function from $\mathbb{B}^{\mid
J \mid} \to \mathbb{B}$ defined as:
$$\forall \mathbf{y} \in \mathbb{B}^{|J|},
f^{\boldsymbol{\alpha}}_I(\mathbf{y}) \eqdef
f((\mathbf{x}^{\boldsymbol{\alpha}}_I)^\mathbf{y}_J).$$
Note that the arity of $f^{\boldsymbol{\alpha}}_I$ is $|J|$, and that
$\ess(f^{\boldsymbol{\alpha}}_I) \leq \ess(f)$.
As an example, let us consider the function $f\colon \mathbb{B}^3 \to
\mathbb{B}$ such as $$f(x_1,x_2, x_3) = x_1 \cdot x_2 +
x_3 + x_1 \cdot x_2 \cdot x_3.$$ The section $f^{(1, 0)}_{\{1, 3\}}   \in
\mathcal{F}_1$ is defined by $f^{(1, 0)}_{\{1, 3\}}(x_2) = x_2$.

A main result about sections that will be used in other proofs is stated in
Property \ref{section_preserve_wap}:

\begin{property}\label{section_preserve_wap}
If $f\colon \mathbb{B}^m\to \mathbb{B}$ is AP, then every section of $f$ is
  also AP.
\end{property}
\begin{proof}
  This statement is actually pretty obvious but sadly, its proof is quite ugly.
  Let $f$ be an AP function from $\mathbb{B}^m \to \mathbb{B}$, $(I,J)$
  a partition of $[1, m]$, and $\boldsymbol{\alpha} \in \mathbb{B}^{|I|}$. We
  will consider the section  $f^{\boldsymbol{\alpha}}_I$.  Let's now assume
  that we have $\mathbf{a},\mathbf{b},\mathbf{c}, \mathbf{d} \in
  \mathbb{B}^{|J|}$ with the 2 following properties:
  $$
  \begin{cases}
    \mathbf{a}:\mathbf{b}::\mathbf{c}:\mathbf{d}\\
    \solvable(f^{\boldsymbol{\alpha}}_I(\mathbf{a}),f^{\boldsymbol{\alpha}}_I(\mathbf{b}),f^{\boldsymbol{\alpha}}_I(\mathbf{c}))
  \end{cases}
  $$
  We want to prove that this implies that
  $\sol(f^{\boldsymbol{\alpha}}_I(\mathbf{a}),f^{\boldsymbol{\alpha}}_I(\mathbf{b})
  = f^{\boldsymbol{\alpha}}_I(\mathbf{d})$.
  First, let us note that the following property holds:
$$\forall \mathbf{a},\mathbf{b},\mathbf{c}, \mathbf{d} \in \mathbb{B}^{|J|},~J
  \subset [1,m], ~ \mathbf{x} \in \mathbb{B}^m,~~ \mathbf{a}: \mathbf{b} ::
  \mathbf{c} : \mathbf{d} \implies \mathbf{x}^{\mathbf{a}}_J :
  \mathbf{x}^{\mathbf{b}}_J:: \mathbf{x}^{\mathbf{c}}_J :
  \mathbf{x}^{\mathbf{d}}_J,$$
  simply due to the fact that $\mathbf{x}:\mathbf{x}::\mathbf{x}:\mathbf{x}$
  always holds for any $\mathbf{x} \in \mathbb{B}^m$.

Regarding the first condition $\mathbf{a}:\mathbf{b}::\mathbf{c}:\mathbf{d}$,
since $\forall \mathbf{x} \in \mathbb{B}^m,$ we have that $
  \mathbf{x}^{\boldsymbol{\alpha}}_I : \mathbf{x}^{\boldsymbol{\alpha}}_I ::
  \mathbf{x}^{\boldsymbol{\alpha}}_I : \mathbf{x}^{\boldsymbol{\alpha}}_I$, we
  deduce: $(\mathbf{x}^{\boldsymbol{\alpha}}_I)^{\mathbf{a}}_J :
  (\mathbf{x}^{\boldsymbol{\alpha}}_I)^{\mathbf{b}}_J ::
  (\mathbf{x}^{\boldsymbol{\alpha}}_I)^{\mathbf{c}}_J :
  (\mathbf{x}^{\boldsymbol{\alpha}}_I)^{\mathbf{d}}_J$.
By definition $f^{\boldsymbol{\boldsymbol{\alpha}}}_I(\mathbf{y}) =
  f((\mathbf{x}^{\boldsymbol{\alpha}}_I)^{\mathbf{y}}_J)$, and the second
  condition is just:
  $$\solvable(f((\mathbf{x}^{\boldsymbol{\alpha}}_I)^\mathbf{a}_J),f((\mathbf{x}^{\boldsymbol{\alpha}}_I)^\mathbf{b}_J),f((\mathbf{x}^{\boldsymbol{\alpha}}_I)^\mathbf{c}_J)).$$
  Since $f$ is AP, we have that:
  $\sol(f((\mathbf{x}^{\boldsymbol{\alpha}}_I)^\mathbf{a}_J),f((\mathbf{x}^{\boldsymbol{\alpha}}_I)^\mathbf{b}_J),f((\mathbf{x}^{\boldsymbol{\alpha}}_I)^\mathbf{c}_J))=
  f((\mathbf{x}^{\boldsymbol{\alpha}}_I)^\mathbf{d}_J)=f^{\boldsymbol{\alpha}}_I(\mathbf{d})$,
  which is the condition for  $f^{\boldsymbol{\boldsymbol{\alpha}}}_I$ to be
  AP.
\end{proof}


This section was quite dense so let us recap what we have seen. We will
describe functions by their ANF, which is a polynomial representation. This ANF
allowed us to define the degree of a function, which has a similar meaning to
the notion of degree for real valued polynomials. We also defined the notion of
equivalent functions, and showed that two equivalent functions are either both
AP, or none of them are. Finally, we defined the section of a function which is
a function of lower arity, and saw that sectioning an AP function leads to
another AP function.

We are now in a position to see some examples of AP functions. We will show that
any affine function is AP.



\subsection{The class affine functions}

\begin{proposition}
  \label{PROPOS:affine_functions_are_wap}
Let $L$ be the class of all affine functions, i.e. functions of the form:
  $$f(x_1,\ldots , x_m)=\alpha_1\cdot x_1+\ldots +\alpha_m\cdot  x_m+\alpha,$$
  with $\alpha_1,\ldots, \alpha_m,\alpha\in \mathbb{B}$. Every affine function
  (also called linear when $\alpha = 0$) is AP.
\end{proposition}

\begin{proof}
Let $f \colon \mathbb{B}^m \to \mathbb{B} \in L$. Using the obvious fact that
  $f$ is AP  iff $f + 1 = \neg f$ is AP, we may assume without loss of generality that
  $\alpha = 0$. Also, considering that $f$ essentially depends on $n \leq m$
  variables ($n$ is then the number of $\alpha_i$ equal to $1$), $f$ is
  equivalent to the function $g \colon \mathbb{B}^n \to \mathbb{B}$ defined by
  $g(x_1, \cdots, x_n) = x_1 +  \cdots + x_n$. Using Property
  \ref{equivalent_functions}, we just need to prove that $g$ is AP to show that
  $f$ is also AP.

  This function $g$ has the remarkable property\footnote{This is the reason why affine
  functions lead to classification problems that are, in fact, highly
  \textbf{non} linearly separable.} that changing the value of any
  $x_i$ changes the value of $g$: $\forall _i, \quad g(x_1, \cdots, x_i, \cdots, x_n) =
  \neg g(x_1, \cdots, \neg x_i, \cdots, x_n)$, as illustrated on
  Figure \ref{FIG:affine_functions_neighbors}. From this property, it is easy
  to see that:
  $$\forall \mathbf{x}, \mathbf{x}' \in \mathbb{B}^n, g(\mathbf{x}) =
  g(\mathbf{x}') \iff H(\mathbf{x}, \mathbf{x}') \text{ is even},$$
  where $h$ is the Hamming distance function.

  Let $\mathbf{a}, \mathbf{b}, \mathbf{c}, \mathbf{d} \in \mathbb{B}^m$  such
  that the two hypothesis in the definition of AP are satisfied, i.e.
  $$
  \mathbf{a} : \mathbf{b} :: \mathbf{c} : \mathbf{d}\quad \text{and}\quad
  g(\mathbf{a}) : g(\mathbf{b}) :: g(\mathbf{c}) : y\quad  \text{is  solvable}.
  $$

  This equation is solvable in two possible cases: either $g(\mathbf{a}) =
  g(\mathbf{b})$, or $g(\mathbf{a}) = g(\mathbf{c})$. This can be verified in
  Table \ref{TAB:six_valid_patterns} on page \pageref{TAB:six_valid_patterns},
  or by refering to Proposition \ref{PROPOS:equation_solving}.
  Also, we know from Property \ref{PROPER:hamming_distance_boolean_proportion},
  then if $\mathbf{a} : \mathbf{b} :: \mathbf{c} : \mathbf{d}$ then we have
  that $H(\mathbf{a}, \mathbf{b}) = H(\mathbf{c}, \mathbf{d})$ and
  $H(\mathbf{a}, \mathbf{c}) = H(\mathbf{b}, \mathbf{d})$.

  Therefore, we either are in one of the two following cases:
  \begin{enumerate}
    \item $g(\mathbf{a}) = g(\mathbf{b})$, and in this case the solution is
      $\sol(g(\mathbf{a}), g(\mathbf{b}), g(\mathbf{c})) = g(\mathbf{c})$. As
      $g(\mathbf{a}) = g(\mathbf{b})$, then $H(\mathbf{a}, \mathbf{b})$ is
      even, and so is $H(\mathbf{c}, \mathbf{d})$. Then, $g(\mathbf{c}) =
      g(\mathbf{d})$, and thus $\sol(g(\mathbf{a}), g(\mathbf{b}),
      g(\mathbf{c})) = g(\mathbf{d})$.
    \item $g(\mathbf{a}) = g(\mathbf{c})$, and in this case the solution is
      $\sol(g(\mathbf{a}), g(\mathbf{b}), g(\mathbf{c})) = g(\mathbf{b})$. As
      $g(\mathbf{a}) = g(\mathbf{c})$, then $H(\mathbf{a}, \mathbf{c})$ is
      even, and so is $H(\mathbf{b}, \mathbf{d})$. Then $g(\mathbf{b}) =
      g(\mathbf{d})$, and thus $\sol(g(\mathbf{a}), g(\mathbf{b}),
      g(\mathbf{c})) = g(\mathbf{d})$.
  \end{enumerate}

  In both cases we have $\sol(g(\mathbf{a}), g(\mathbf{b}), g(\mathbf{c})) =
  g(\mathbf{d})$, thus showing that $g$ is AP.
  As $g$ and $f$ are equivalent functions, then any function $f \in L$ is AP.
\end{proof}

\begin{figure}[!h]
\centering
  \includegraphics[width=2in]{figures/affine_functions_neighbors.pdf}
  \caption{The labels as determined by the affine function $g(x_1, x_2, x_3) =
  x_1 + x_3 + x_3$. Note that each $1$ is surrounded by three $0$, and each $0$
  is surrounded by three $1$. Also, note that solving any (solvable) analogical
  equation leads to the correct solution: This is because $g$ is AP.}
  \label{FIG:affine_functions_neighbors}
\end{figure}

Proposition \ref{PROPOS:affine_functions_are_wap} can now retrospectively
explain the results of Figure \ref{FIG:nan_vs_nn} on page \ref{FIG:nan_vs_nn}
for the two functions $f_1$ and $f_2$, which are both instances of affine
functions. We saw that for these two functions, $\omegasf$ (defined as the
proportion of elements $\mathbf{x}$ in $\esfs$ for which $\albl{x} = f(x)$) was
always equal to $1$. This result is now obvious and directly comes from the
fact that both $f_1$ and $f_2$ are actually analogy preserving functions.

We now know that every affine function is AP. We will here give a stronger
result: the affine functions are the \textbf{only} AP functions.
In Section \ref{TODO}, we introduce the ANF of Boolean functions and also
defined their degree. It should be clear that the class of functions with
degree at most $1$ is exactly the class $L$ of affine functions, which are AP.
We will show that the class of AP functions is the class of affine functions by
proving that if a function $f$ is AP, then $d(f)\leq 1$. We first consider the
case where $d(f) = 2$:

\begin{property} \label{degree_2_not_AP}
 Let $f\colon \mathbb{B}^m\to \mathbb{B}$ with $d(f)=2$. $f$ is not AP.
\end{property}
\begin{proof}
  Let's consider $f$ with $d(f) = 2$ and $\ess(f) >= 2$. We denote $\mathbf{x}_I$ one of the
  monomial of $f$ of degree $2$. We consider the section $f^{\mathbf{0}}_J$,
  where $J = [1, m] \setminus I$,  and $\mathbf{0}$ denotes the constant 0
  vector in $B^{|J|}$. All variables that are not part of the monomial
  $\mathbf{x}_I$ have been set to $0$. This section $f^{\mathbf{0}}_J$ has a
  unique monomial of degree $2$ (namely $\mathbf{x}_I$) and $\ess(f) = 2$.
  $f^{\mathbf{0}}_J$ is necessarily equivalent to one of the following
  functions:
  $$
  \begin{cases}
    f_1(x_1, x_2) = x_1 \cdot x_2 + \alpha \\
    f_2(x_1, x_2) = x_1 \cdot x_2 + x_1 + \alpha\\
    f_3(x_1, x_2) = x_1 \cdot x_2 + x_1 + x_2 + \alpha
  \end{cases}$$
  Now consider Table \ref{TAB:counter_examples}, which gives the values of
  $f_1, f_2$ and $f_3$ (with $\alpha = 0$) for the four-tuple $(\mathbf{a},
  \mathbf{b}, \mathbf{c}, \mathbf{d})$.
  \begin{table}[ht]
    \center
  $\begin{array}{cccccc}
    \toprule
    ~ & x_1 & x_2 & f_1 & f_2 & f_3\\
    \midrule
    \mathbf{a} & 0 & 0 & 0 & 0 & 0\\
    \mathbf{b} & 0 & 1 & 0 & 0 & 1\\
    \mathbf{c} & 1 & 0 & 0 & 1 & 1\\
    \mathbf{d} & 1 & 1 & 1 & 0 & 1\\
    \bottomrule
  \end{array}
  $\bigskip
  \caption{Examples for $f_1, f_2, f_3$ showing that they are not AP.}
  \label{TAB:counter_examples}
  \end{table}
  It is clear that $(\mathbf{a},\mathbf{b}, \mathbf{c}, \mathbf{d})$ is an
  example for $f_1$ because we have $\mathbf{a} : \mathbf{b} :: \mathbf{c} :
  \mathbf{d}$ and the equation $f_1(\mathbf{a}) :  f_1(\mathbf{b}) ::
  f(\mathbf{c}) : y$ is solvable, but the solution is $0$ while $f(\mathbf{d})$
  is $1$. Simmilarly, $(\mathbf{a},\mathbf{b}, \mathbf{c}, \mathbf{d})$ is also
  an example for $f_2$ and $(\mathbf{d},\mathbf{c}, \mathbf{b}, \mathbf{a})$ is
  an example for $f_3$.


  It is pretty straightforward to find examples of $\mathbf{a}, \mathbf{b},
  \mathbf{c}, \mathbf{d} \in \mathbb{B}^2$
  that show that none of these functions are AP, so $f^{\mathbf{0}}_J$ can't
  be AP either. As $f^{\mathbf{0}}_J$ is a section of $f$, $f$ cannot be AP.
\end{proof}

All we need now is a property that could allow us to decrease the degree of a
function without changing its AP property. This is the purpose of
Property \ref{section_degree_k_minus_1}.

\begin{property}\label{section_degree_k_minus_1}
Let $f:\mathbb{B}^m\rightarrow \mathbb{B}$ be a function with
  $d(f)=k\geq 2$. Then there is a section $g$ of $f$ with $d(g)=k-1$.
\end{property}
\begin{proof}
Suppose that  $d(f)=k\geq 2$, and let $\mathbf{x}_I$ be a monomial of $f$ of
  maximum degree, i.e. $|I|=k$.  Here again, consider the section $g =
  f^{\mathbf{0}}_J$ where $J = [1, m] \setminus I$ and $\mathbf{0}$ denotes the
  constant $0$ vector in $\mathbb{B}^{|J|}$. It is clear that $g$ is
  represented by a  polynomial that has a unique monomial of maximal degree
  $k$, namely $\mathbf{x}_I$, and maybe some other monomials of degree strictly
  less than $k$.  Let us choose any $i \in I$: then $g' = g^1_{\{i\}}$ is a
  section of $g$ of degree (and arity) $k-1$. As $g'$ is a section of $g$, it
  is also a section of $f$ which completes the proof.
\end{proof}

We are now in position to prove our main result.

\begin{proposition}\label{AP_is_L}
The class of AP functions is the class $L$ of affine functions.
\end{proposition}
\begin{proof}
We have seen that every affine function is AP, i.e. if $d(f)\leq 1$, then $f\in
  AP$. On the other hand, Property \ref{degree_2_not_AP} tells us that if
  $d(f)=2$, then $f \notin AP$. So suppose that  $d(f)\geq 3$. By successive
  applications of Property \ref{section_degree_k_minus_1}, it follows that
  there is a section $g$ of $f$ with $d(g)=2$.

  As $g$ is not AP, then $f$ is not AP either. All in all, if $d(f) \geq 2$
  then $f$ is not AP, so the class of AP functions is $L$.
\end{proof}

We can finally give a definite answer to our initial problem: {\bf the class of
functions that ensure a sound extension of any sample set $S \subseteq
\mathbb{B}^m$ is the class of affine functions $L$}. If the function is not
affine, then there exist a sample set $S \subseteq \mathbb{B}^m$ for which
$\mathbf{E}_S(f)$ is unsound.

Now, while this theoretical result is interesting on its own, it is obvious
that purely affine functions are not representative of what would be
encountered in a real-world environment.  This leads to {\bf Problem 2}: What
remains of the quality of the analogical extension $\mathbf{E}_S(f)$ when $f$
deviates from being AP in different ways?  The aim of the next section is to
empirically investigate this question.

\section{Approximately AP functions and experiments}
\label{approximate_ap_functions}

Given two Boolean functions $f$ and $g$, we define their distance
$\text{dist}(f, g) = P_\mathbf{x}\left[f(\mathbf{x}) \neq
g(\mathbf{x})\right]$, where $P_\mathbf{x}$ is the uniform distribution over
$\mathbb{B}^m$. Here, $\mathbf{x} \in \mathbb{B}^m$ is also considered as a
random variable. We say that $f$ is $\epsilon$-close to $g$ if $\text{dist}(f,
g) \leq \epsilon$, and that $f$ is $\epsilon$-close to $L$ if $\exists g \in L$
such that $f$ is $\epsilon$-close to $g$. Given a sample $S$, we define
$\omega$ as the quality of the extension $\mathbf{E}_S(f)$ with $\omega =
P_{\mathbf{x}\in \mathbf{E}^*_S(f)}\left[\albl{\mathbf{x}}_f =
f(\mathbf{x})\right]$. By definition, $\omega=1$ for all $S$ iff $f$ is AP.
Note that there exist statistical tests allowing the practitioner to query a
partially-known function to find out if it is $\epsilon$-close to L (see for
instance \cite{BluLubRub93}).\todo{BLR marche que pour fonctions lineaires
mais OSEF car omega de f = omega f + 1 et si f lineaire alors f + 1 est affine
donc on peut se contenter de BLR}. Therefore, studying $\omega$ w.r.t. $\epsilon$
is clearly of interest.

Starting from an affine function $g \colon \mathbb{B}^8 \to \mathbb{B}$ defined
as  $g(\mathbf{x}) = x_1 + \cdots + x_8$, we introduce some noise by negating
its output $g(\mathbf{x})$ with probability $\epsilon$. We thus obtain
functions $f_{\epsilon}$ that are $\epsilon$-close to $L$, and report the value
of $\omega$ averaged over $50$ experiments (using the Standard modeling).
Figure \ref{omega_vs_eps} gives an illustration of the variation of $\omega$
w.r.t.  $\epsilon$ for different sizes of $S$ (as a percentage of
$|\mathbb{B}^m|$).  Other experiments have been carried out with other affine
functions (i.e.  with less essential variables or different arity), leading to
very similar results.  Note that $\epsilon$ only needs to be taken in $[0,
\frac{1}{2}]$, because being $\epsilon$-close to $g$ is equivalent to be $1 -
\epsilon$-close to $g + 1 = \neg g$ which is still affine, so the curves are
symmetrical with respect to the axis $\epsilon = \frac{1}{2}$.

\begin{figure}
\begin{center}
\includegraphics[scale=0.6]{figures/omega_vs_eps_dim8_nexp50_std_nEss8.pdf}
  \caption{$\omega$ for $\epsilon$-close functions to $L$.}
\label{omega_vs_eps}
\end{center}
\end{figure}

When $\epsilon = 0$ we get $\omega = 1$, as expected from
Proposition \ref{AP_is_L}. We observe an almost linear decrease in $\omega$ as
$\epsilon$ grows to $0.3$ then leading to a plateau where $\omega =
\frac{1}{2}$, indicating that the analogical labels $\albl{\mathbf{x}}_f$ are
more or less random. Moreover, $\omega$ appears to decrease faster for small
samples $S$. This is due to the fact that the analogical labels
$\albl{\mathbf{x}}_f$ are the result of a majority-vote procedure among the
candidate solutions that one can build from $S$, and the number of candidates
becomes smaller as $|S|$ decreases, thus altering the quality of the
prediction. The determination of a functional dependence between $\omega$,
$\epsilon$ and $|S|$ is currently being investigated.

Now, let us note the following point: even if a function $f$ is far from being
AP, the quality $\omega$ of the extension $\mathbf{E}_S(f)$ may still be very
high. To illustrate this, let us define the value $\beta$ which is an indicator
of how far is $f$ from being completely AP.  For each $\mathbf{x} \in
\mathbf{E}^*_S(f)$, we define $\beta_\mathbf{x}$ as the proportion of
candidates $y$ that led to the the correct label, i.e. the proportion of $y$
such that $y = f(\mathbf{x})$. $\beta$ is defined as the average of all the
$\beta_\mathbf{x}$.  Obviously, a function $f$ is AP iff $\beta = 1$ for all
$S$, i.e. if $\beta_\mathbf{x} = 1$ for all $\mathbf{x} \in \mathbf{E}^*_S(f)$
and for all $S$.

Table \ref{table_monks} reports the values of $\omega$ and $\beta$ for the
Standard and the Klein modelings of analogy (respectively $\omega_S$,
$\omega_K$, $\beta_S$ and $\beta_K$) over three datasets from the UCI
repository, namely the three Monk's problems\footnote{As these datasets are
nominally-valued, they are binarized.} \cite{UCIrepo}. Results are averaged
over 100 experiments, where the sample set $S$ is each time randomly sampled
with a size of $30$\% of the universe of possible instances.

\begin{table}
\centering
\begin{tabular}{| c | c | c | c | c |}
\toprule
  & $\omega_S$  & $\omega_K$ & $\beta_S$  &  $\beta_K$ \\
\midrule
Monk 1 & .96 & .96 & .73 & .62 \\
Monk 2 & .96 & .84 & .69 & .60 \\
Monk 3 & .98 & .95 & .87 & .77 \\
\bottomrule
\end{tabular}
\caption{$\omega$ and $\beta$ for the Standard and Klein modelings over the
  Monk's problems.}
\label{table_monks}
\end{table}

We observe that for each dataset, $\beta_S$ is significantly lower than $1$.
This suggests that the Boolean functions underlying these
datasets are highly not AP, because on average, there is a high proportion
(around $20$\%) of candidates $y$ that predicted the wrong label. However,
$\omega_S$ is no lower than $96$\%, implying extensions of very high quality.
This is where the majority-vote comes into play: in some cases, it may be able
to compensate for the predictors $y$ that were wrong.  This is what happens
here in $96$\%, $96$\% and $98$\% of the cases respectively. Here again,
obtaining theoretical guarantees about the majority vote procedure is currently
investigated.

We note also that the Klein modeling achieves equal or lower quality than the
standard one, which suggests that the Standard modeling, which has the same
class of AP functions, is more useful in practice. Note that such a difference
between $\omega_S$ and $\omega_K$ has been consistently observed over many
other experiments, which we do not mention here for lack of space. This may be
explained by the fact that, as mentioned earlier, the Klein modeling obeys
the following property, unnatural  for an analogy: $A_K(a, b, c, d)
\iff A_K(b, a, c, d)$.
