\chapter{Recommender systems}

In a world of information overload, automatic filtering tools are essential to
extract relevant information from basic noise. In the field of e-commerce,
recommender systems play the role of search engines when surfing the entire
web: they filter available items to provide relevant suggestions to customers.

With the huge development of online businesses, recommender systems have become
more and more popular \cite{RecoSystemHandbook,AdoTuzIEEE2005}. They help to
answer questions as diverse as ``what movie to rent?", ``what item to buy?" or
``which restaurant to try?", but also such as ``what piece of code to
investigate ?" in the case of recommender system for software development.
They do so by providing the user with a list of recommendations.  Obviously,
the more personalized the recommendations, the better the system.  For
instance, a system recommending only the most popular items is useless as it is
likely that the standard customer knows these items. But the maximization of
business profit may also lead to suggest items which are not very popular (and
thus difficult to sell), provided they may be of interest for the user.

A very common way of providing personalized recommendations to a target user is
to estimate its taste for the items that the system provides. The taste of a user
$u$ for a given item $i$ is usually represented as a rating that $u$ would give
to $i$.  The scale of the ratings may vary, but the integer interval [1, 5] and
the \textit{like}/\textit{dislike} scales are very common in real world
systems.

Once these estimations are made, a simple option is to recommend the items with
the highest ratings among all the estimated scores for the considered user
(using the implicit assumption that a user should be interested in the items
with high scores).

Providing an accurate measure of the overall quality of a recommender system is
not a simple task and diverse viewpoints have to be considered (see
\cite{RecoSystemHandbook} for an extensive survey).  Obviously, accuracy of
its core prediction algorithm is an important matter, but other properties are
also desirable in order  to deploy an effective recommendation engine. For
instance {\it coverage} measuring what is the range of items in the catalogue
that could be recommended, or {\it serendipity} measuring in some sense the
surprise that a user could get when a new item is suggested.

\section{Background on recommender systems}
\label{sec:background_RS}

The aim of a recommendation system is to provide users with lists of relevant
personalized items.  Let us now formalize the problem.

\subsection{Problem formalization and notation}
Let $U$ be a set of users and $I$ a set of items. For some pairs $(u,i) \in U
\times I$, a rating  $\rui$ is supposed to have been given by $u$ to express
whether she likes the item $i$ or not.  It is quite common that  $\rui$ belongs
to the rating scale $[1, 5]$, 5 meaning a strong preference for item $i$, 1
meaning a strong rejection, and 3 meaning indifference, or just an average
note. When the rating scale is gradually valuated, we say that ratings are
\textbf{explicit}, in the sense that users have explicitely expressed their
preference towards items. \textbf{Implicit} ratings belong to a binary scale
$[0, 1]$, and usually correspond to the result of collected data about users
behaviour, without any active involvement. For example, if a user has listened
to a given track more than 10 times, we might consider this as an implicit
rating of $1$. We may also consider that an implicit rating captures the fact
that a user has bought a given item without giving it an explicit rating. These
two kinds of rating schemes (explicit and implicit) lead to quite different
prediction algorithms in practice. In our work, we will only focus on the
explicit rating scheme.\todo{Vrai?}

Let us denote by $R$ the set of known ratings recorded in the system. It is
well known that, in real systems, the size of $R$ is very small with regard to
the potential number of ratings which is $|U| \times |I|$, as a lot of ratings
are missing. The set $\Ui$ will denote the set of users that have rated item
$i$, and $\Uij$ is the set of users that have rated both item $i$ and item $j$.
Similarly, $\Iu$ is the set of items that user $u$ has rated, and $\Iuv$ is the
set of items that both users $u$ and $v$ have rated.

To recommend items to users, a recommender system usually proceed as follows:
\begin{enumerate}
\item Using a prediction algorithm $A$, estimate the unknown ratings $\rui$
  (i.e. $\rui \notin R$). This estimation $A(u, i)$ will here be denoted
    $\predrui$.
\item Using a recommendation strategy $S$ and in the light of the previously
  estimated ratings, recommend items to users. For instance, a basic yet common
    strategy is to suggest to user $u$ the items $i \notin I_u$ with the
    highest $\predrui$.
\end{enumerate}

When using the implicit rating scheme, these two stages are usually
indistinguishable: it is natural to recommend an item to a user if the
estimated predicted rating is $1$. In practice, the recommendation strategy
depends a lot on the pragmatic constraints of the system: a store manager way
want to push forward a given item to boost its sellings, regardless of the
rating predictions outputted by the algorithm. As a result, most of the
research has focused on the prediction algorithms, and so will we.

Prediction algorithms implementations are strongly influenced by the fields of
data mining and of course machine learning. However, we want to emphasize that
the general setting of recommender systems is actually quite different than
that of classification or regression. A na√Øve reasoning could lead us to
consider that the rating prediction problem with the rating scale $[1, 5]$ is
nothing but a classification task with five classes $1, 2, 3, 4, 5$. But this is
forgetting that the values $1, 2, 3, 4, 5$ are actually ordinal, and that the
difference between \textit{class} $1$ and \textit{class} $2$ is not the same as the
difference between \textit{class} $1$ and \textit{class} $4$. But most
importantly, in classification or regression all the instances belong to the
same space (that space was $X^m$ in our previous chapters), and have the same
features. This is not the case here! In a recommendation problem, if we chose
our instances to be the users and their feature space to be the items, we would
end up with instances that are only (very) partially described, because of
course no user has rated the whole set of items. All the more, as the number of
items is usually very high, we would end up with a very high dimensional
problem, where learning algorithm tend to fail due to the so-called curse of
dimentionality. Our point here is that recommender systems problems do not
really fit within the traditional machine learning setting, and represent a new
setting in their own right.

Traditionally, prediction algorithms are
considered to belong to one of the two main families of prediction techniques,
referred to as \textbf{content-based} methods and \textbf{collaborative
filtering} methods, that we both review below. In practice, real-world
recommender systems usually use a mix of both methods, and are referred to as
\textbf{hybrid} methods.

\subsection{Content-based techniques}
\label{SEC:collaborative_filtering}

Content-based algorithms try to recommender to a user some items that are
\textbf{similar} to those that the user has liked in the past. For example, if
the renting history of a user mainly contains science fiction movies, the aim
of the system is to learn these preferences and recommend some other science
fiction movies to the user. We recommend \cite[Ch.~3]{RecoSystemHandbook} for a
recent overview of content-based methods.

To find out similar items to those that a user has liked, these systems usually
rely on a similarity metric whose nature strongly depends on the representation
of the items. In large-scale online shopping systems were items are extremely
diverse and abundant, a similarity measure between two items would be for
example the number of web sessions on which the two item pages have jointly
been visisted (yes, this is the reason why some online systems will try to sell
you a fridge even though you just bought a brand new one). In systems were
items are more homogeneous, i.e. in a movie store system, more  sophisticated
approaches can be built relying on some metadata about the items (hence the
name \textit{contenat}-based), for example movies genre, main actors, film
director, etc\dots.

As content-based techniques do not rely on the set of ratings to compute item
similarities, a nice resulting feature is that new items that have not yet been
rated by anybody can still be recommended. Content-based techniques also stand
out by their explanatory potential: the motto \textit{Here are some items that
are akin to those you liked} is perfectly understandable and seems sound.
Recommender systems usually strive for explanatory power, because it is
recognized that when confronted with a given recommendation users are more
likely to accept the recommendation is they understand and acknowledge the
underlying process.

However, content-based systems are prone to various behaviour that tend to make
them less competitive with other collaborative filtering methods. The first
obvious drawback is that most of the time, metadata are required to describe
the items (remember our movie example with genre, actors etc.). Such
description can be extremely costly and can only capture a very limited subset
of item features, which are not necessarily the most important ones when it
comes to the users personal tastes. Going back to our science fiction fan, it
is very plausible that the user has a strong preference for the steampunk
genre, and yet is perfectly indifferent to the superhero fiction movies, and
both can still be considered as subgenres of science fiction. A system that
could not distinguish these two kinds of movies would fail to provide our
steampunk fan with relevant recommendations.

Another well-known drawback of content-based recommenders is their tendency to
recommend only items that users may already know or do not need anymore (such
as a fridge!), and therefore the recommendations lack in novelty, surprise and
diversity. Also, they tend to output much less accurate prediction the
collaborative filtering methods (the way accuracy is computed will be described
in Section \ref{TODO}). For all these reasons, our own analogy-based algorithms
we rather be of a collaborative filtering nature.

\section{Collaborative filtering techniques}
\label{SEC:collaborative_filtering}

The main idea behind collaborative filtering algorithms is to recommend items
to a users that other users with similar tastes have liked in the past. So for
example if Bob and Alice usually agree on their movie ratings, i.e. they like
the same movies and dislike the same movies, if Alice has not seen a movie that
Bob has liked, then it will be recommended to Alice. We will here present two
families of collaborative filtering algorithms: the neighborhood approach based
on the well known $k$-NN algorithm, and the matrix factorization techniques
whose groundings come from linear algebra and that lead to elegant and accurate
models.

\subsection{The neighborhood approach}

\subsubsection{Prediction framework}

Neighborhood approaches are instances of the general $k$-NN scheme. To estimate
the rating $\rui$ of a user $u$ for an item $i$, the most basic method consists
in computing  the set of $k$ users that are most similar to $u$ and
that have rated $i$. We will denote this set $N_i^k(u)$. The computation of
$N_i^k(u)$ depends of course on a similarity measure between users, which is
based on their respective ratings. The estimation $\predrui$ of $\rui$ is then
computed as an aggregate of the ratings $\rvi$, where $v$ is one of the
neighbors of $u$ in $N_i^k(u)$. Usually, the aggregation is simply a mean
weighted by the similarity between $u$ and $v$:

\begin{definition}
  The estimation of a rating $\rui$ using the neighborhood approach (also
  denoted $k$-NN here) is:
  $$\predrui = \frac{\sum\limits_{v \in N_i^k(u)} r_{vi} \cdot \ssim(u, v)}
  {\sum\limits_{v \in N_i^k(u)}\ssim(u, v)},$$
  where $N_i^k(u)$ is the set of users having rated $i$ with the highest $k$
  values of sim with $u$:
  $$N_i^k(u) \eqdef \Set{v \in U | \rvi \in R,~v\in \argmax_{u' \in
  U}^k\left[\ssim(u, u')\right]}$$
\end{definition}

The estimation process of the neighborhood approach is described in Algorithm
\ref{ALGO:neighborhood_prediction}.

\begin{algorithm}[!ht]
 \caption{The neighborhood recommender.}
       \label{ALGO:neighborhood_prediction}
       \begin{algorithmic}

         \STATE {\bf Input}: A set of ratings $R$, and a couple $(u, i)$ for
         which $\rui$ is  unknown.
         \STATE {\bf Output}: $\predrui$, an estimation of $\rui$.
         \STATE \textit{Training stage:}
         \FORALL{$(u, v) \in U^2$}
         \STATE Compute $\ssim(u, v)$.
	    \ENDFOR
       \STATE \textit{Prediction stage:}
         \STATE $\text{sims} = 0, ~ \text{ratings} = 0$
         \STATE $U_i^s = \text{Sorted}(U_i)$  // \textit{Sort by value of
         $\ssim$ with $u$}.

         \FORALL {$v \in U_i^s[:k]$}
         \STATE \textit{I.e. for the first $k$ users in $U_i^s$}
         \STATE \text{sims} += \ssim(u, v)
         \STATE \text{ratings} += $\rvi \cdot \ssim(u, v)$
         \ENDFOR

         \STATE $\predrui = \frac{\text{ratings}}{\text{sims}}$
\end{algorithmic}
\end{algorithm}

Obviously, the training stage only needs to be done once. Then, the
similarities can be reused for later predictions.

\subsubsection{The similarity measures}

There are many, many ways to define the similarity metric between two users.
Probably the most common one is the cosine similarity.

\begin{definition}
  The cosine similarity between two users $u$ and $v$ is defined as:
$$
\emph{cosine sim}(u, v) \eqdef \frac{ \sum\limits_{i \in \Iuv} \rui \cdot \rvi}
{\sqrt{\sum\limits_{i \in \Iuv} \rui^2} \cdot \sqrt{\sum\limits_{i \in \Iuv}
\rvi^2}}.
$$
\end{definition}

Here, users $u$ and $v$ are considered as vectors in a vector space defined by
the items they have both rated (the set of common items is $\Iuv$). Their
cosine similarity simply is the cosine of the angle between the two vectors. An
unnatural feature of this metric is that two vectors with (potentially
different) constant values will always have a similarity of $1$, because they
are collinear. For example if $u = (2, 2)$ and $v = (5, 5)$, $\text{cosine
sim}(u, v) = \frac{20}{\sqrt{8}\sqrt{50}} = 1$, while one would expect $u$ and
$v$ to be quite different w.r.t. their tastes.

Such a flaw can be overcame using the Pearson similarity, which can be viewed as
a mean-centered version of the cosine similarity.

\begin{definition}
  The Pearson similarity between two users $u$ and $v$ is defined as:
$$
\emph{Pearson sim}(u, v) \eqdef \frac{ \sum\limits_{i \in \Iuv}
(\rui -  \mu_u) \cdot (\rvi - \mu_{v})} {\sqrt{\sum\limits_{i
\in \Iuv} (\rui -  \mu_u)^2} \cdot \sqrt{\sum\limits_{i \in
\Iuv} (\rvi -  \mu_{v})^2}},
$$
where $\mu_u$ and $\mu_v$ are the average rating of users $u$ and $v$
respectively.
\end{definition}

One last similarity metric that we will use is the Mean Squared Difference
(MSD)\footnote{Strictly speaking MSD is actually a distance rather than a
similarity metric, so taking its inverse would do the job.}:
\begin{definition}
  The mean squared difference between two users $u$ and $v$ is defined as:
$$\emph{MSD}(u, v) \eqdef \frac{1}{|\Iuv|} \cdot \sum\limits_{i \in \Iuv} (\rui
- \rvi)^2$$
\end{definition}

Notice that none of these metrics take into account the support between the two
users $u$ and $v$, i.e. the number of items that they have commonly rated. It
would be foolish to have the same faith in a similarity computed over hundreds
of common items as in a similarity computed only over a few items. This is why
in practice, some sort of shrinkage is usually performed, giving a high value
to similarities with a high support:
$$\text{actual\_sim}(u, v) \eqdef \frac{\mid \Iuv \mid - 1}{\mid \Iuv \mid - 1
+ \lambda} \cdot \ssim(u, v),$$
where $\lambda$ is a sort of regularization constant. Such  a technique can be
motivated from a Bayesian perspective, and usually leads to significant
improvements in the system performances. We will not consider shrinkage in any
of our experiments.

\subsubsection{Similarity computation}

We will now describe how to practically compute these similarity metrics. We
see that all of the presented metrics rely on the set of common items $\Iuv$.
In practice, the explicit computation of this set for every single pair of
users is very expensive, and naive algorithms lead to poor performances.
Consider for example the naive similarity computation of Algorithm
\ref{ALGO:naive_sim}.
\begin{algorithm}[!ht]
 \caption{A general naive algorithm for similarity computation}
       \label{ALGO:naive_sim}
       \begin{algorithmic}

         \STATE {\bf Input}: A set of ratings $R$
         \STATE {\bf Output}: The similarity between all pairs of users.
         \FORALL{$u \in U$}
         \FORALL{$v \in U$}
         \FORALL{$i \in I_u$}
         \IF{$i \in I_v$}
         \STATE \textit{We know now that $i \in I_{uv}$, so we can use $r_{ui},
         r_{vi}$ as we please, depending on the measure that is computed}.
         \ENDIF
        \ENDFOR
        \ENDFOR
        \ENDFOR
\end{algorithmic}
\end{algorithm}
In the worst case, we can consider that $\mid I_u \mid = \mid I_v \mid = \mid I
\mid$ for all $u, v$, so the complexity of this naive algorithm is
$\mathcal{O}(\mid U \mid^2 \mid I \mid^2)$.

However, the complexity can be taken
down by a great deal if we proceed in sort of MapReduce fashion
\cite{DeaGhe04}, as described in Algorithm \ref{ALGO:cosine} which illustrate
how to compute cosine similarities.
\begin{algorithm}[!ht]
 \caption{Computation of the cosine similarity.}
       \label{ALGO:cosine}
       \begin{algorithmic}

         \STATE {\bf Input}: A set of ratings $R$.
         \STATE {\bf Output}: \text{cosine sim}(u, v) for all pairs of users.
         \STATE {\it Initialization (we set $n$ as the number of users, i.e. $n
         = \mid U \mid$):}
         \STATE $\text{sim} = \text{null\_array}[n][n]$
         \STATE $\text{sum\_prod} = \text{null\_array}[n][n]$
         \STATE $\text{sum\_squ} = \text{null\_array}[n][n]$
         \STATE $\text{sum\_sqv} = \text{null\_array}[n][n]$
         \FORALL{$i \in I$}
           \FORALL{$u \in U_i$}
             \FORALL{$v \in U_i$}
              \STATE $\text{sum\_prod}(u, v) \pluseq \text{sum\_prod}(u, v) +
              \rui \cdot \rvi$
              \STATE $\text{sum\_squ}(u, v) \pluseq \text{sum\_squ}(u, v) +
              {\rui}^2$
              \STATE $\text{sum\_sqv}(u, v) \pluseq \text{sum\_sqv}(u, v) +
              {\rvi}^2$
             \ENDFOR
           \ENDFOR
         \ENDFOR
         \FORALL{$u \in U_i$}
           \FORALL{$v \in U_i$}
           \STATE $\text{sim}(u, v) = \frac{\text{sum\_prod}(u,
           v)}{\sqrt{\text{sum\_squ}(u, v) \cdot \text{sum\_sqv}(u, v)}}$
           \ENDFOR
         \ENDFOR
\end{algorithmic}
\end{algorithm}
Actually, any other similarity measure that rely on the set $I_{uv}$ can be
computed this way (see \cite{SchBodVolRECSYS12}). Considering again the worst
case where $\mid U_i \mid = \mid U \mid$, the complexity of Algorithm
\ref{ALGO:cosine} is  $\mathcal{O}(\mid I \mid \cdot \mid U \mid^2 + \cdot \mid
U \mid^2) = \mathcal{O}( \cdot \mid U \mid^2 [ \mid I \mid + 1])$, which is far
less than that of Algorithm \ref{ALGO:naive_sim}. Another undeniable advantage
of Algorithm  \ref{ALGO:cosine} is that it follows the MapReduce framework, and
as such can be easily parallelized for another significant performance gain.
Indeed, the first three embedded for loops could be dispatched in various
cluster, each dealing with a given set of items (this is the MAP stage). Each
of the clusters would have its own three sum matrices $\text{sum\_prod}$,
$\text{sum\_squ}$ and $\text{sum\_sqi}$, which could then all be aggregated to
rebuild the entire sum matrices (REDUCE stage).

In the next section, we describe another very popular collaborative filtering
technique, which models the data in a significantly different (but meaningful!)
way.

\subsection{Matrix factorization techniques}

About ten years ago, the Netflix company organized a competition where the goal
was to improve the RMSE (defined in Section \ref{TODO}) of their standard
prediction algorithm by $10\%$. The challenge quickly became very popular, not
only because the winning prize was of \$1 million, but also because the dataset
was orders of magnitude larger than any other available dataset at the time:
about 100M of ratings of 480.000 user sand 17.700 movies. Unfortunately, their
dataset is no longer publicly available, but the challenge led to the emergence
of many successful recommendation techniques, among which matrix factorization
methods clearly stand out. Let's be honest: the author of this document has a
crush on matrix factorization techniques, so we will try to give here a
detailed description of such techniques.

The matrix factorization approach we will describe here is heavily inspired by
the Singular Value Decomposition (SVD) of a matrix, one of the highlights of
linear algebra. Because having a basic understanding of SVD will be very
insightful for us, we will briefly review it now.

\subsubsection{Background on SVD}

Let's first dive into the
theory (but not for long):

\begin{proposition}
  Any real-valued matrix $R \in \mathcal{M}^{m \times n}$ of rank $r$ can be
  decomposed as the product of three matrices\footnote{The matrix $I$ is
  \textbf{not} the identity matrix!}:

  $$R = U\Sigma I^t,$$
  where $U\in \mathcal{M}^{m \times r}$, $\Sigma\in \mathcal{M}^{r \times r}$
  is a diagonal matrix, and $I\in \mathcal{M}^{n \times r}$. Such a
  factorization is called the Singular Value Decomposition of $R$.
\end{proposition}

As $\Sigma$ is a diagonal matrix, it only act as a scaler for either $U$ or
$I$. For the sake of simplicity, we will consider that the decomposition can be
written $R = UI^t$, where $\Sigma$ has been merged into either $U$ or $I$. In
practice, we know how to compute the the two matrices $U$ and $I$: their
columns are are the
eigenvectors\footnote{For this reason, SVD and Principal Component Analysis are
strongly related.} of the two matrices $A^tA$ and $AA^t$, and their associated
eigenvalues are the squared entries of $\Sigma$, called the singular values
(hence the name of the factorization).

The key point is that the columns of $U$ are actually an orthonormal basis for
the column space of $R$, and the columns of $I$ are an orthonormal basis for the
row space of $R$. Maybe this statement is worth some explaination. The column
space of $R$ is the vector space that is spanned by the $n$ columns or $R$,
i.e. the set of vectors that are a linear combination of the columns of $R$. As
some of the columns of $R$ may be linearly dependent, this vector space is of
dimension $r\leq n$: the rank of a matrix is defined as the number of
independent columns, and equivalently as the number of independent rows. Our
statement says that the columns of $U$ span the same space, and that they form
an orthonormal basis of $r$ vectors. The same goes for the orthogonal columns
of $I$, which span the row space of $R$.

A particular case of this general statement is that \textbf{any column of $R$
can be expressed as a unique linear combination of the columns of $U$}. As the
columns of $U$ are orthonormal, each column has a unique contribution that
cannot be compensated by the others. Symmetrically, \textbf{any row of $R$ is a
linear combination of the columns of $I$.} What does it have to do with our
recommendation problem? Imagine for a moment that $R$ is a dense rating matrix,
where the rows reprensent items and the columns reprensent users:
$$
R = \begin{blockarray}{cccc}
  \text{Alice} & \text{Bob} & \text{Charlie} \\
\begin{block}{(ccc) c}
  4 & 5 & 5 & \text{Titanic} \\
  2 & 1 & 1 & \text{Toy Story} \\
  1 & 2 & 2 & \text{LORT} \\
  3 & 3 & 3 & \text{Mad Max} \\
  4 & 2 & 2 & \text{E.T.} \\
\end{block}
\end{blockarray}
=
\begin{pmatrix}
  \vertbar & \vertbar & & \vertbar\\
  U_1& U_2 & \cdots & U_r\\
  \vertbar & \vertbar & & \vertbar\\
\end{pmatrix}
\cdot
\begin{pmatrix}
  \horzbar& I_1 & \horzbar\\
  \horzbar& I_2 & \horzbar\\
   & \vdots & \\
  \horzbar& I_r &\horzbar \\
\end{pmatrix}
,
$$
where $U_1, \cdots U_r$ are the columns of $U$ and $I_1, \cdots I_r$ are the
columns of $I$ (i.e. the lines of $I^t$).

When we compute the SVD of $R$, we find in the columns of $U$ some
\textit{prototype} users that have no real existence, but that can be combined
(linearly) to build up any of the users: Alice, Bob and Charlie are linear
combinations of $U_1, U_2, \dots U_r$.  Similarly, the columns of $I$ are
\textit{prototype} movies that do not properly exist but that can be combined
to build up Titanic, Toy Story, etc\dots We can consider each vector $I_i$
to be some sort of idealized movie type, for example $I_1$ is a typical action
movie, while $I_2$ would be a typical romantic movie, etc. Similarly, the
$U_i$'s can be viewed as some idealized critics, with specific tastes toward
some kind of movies. Obviously in practice, there is no way to assoociate a
clear semantic meaning to the different $U_i$ or $I_i$, and the way they are
defined highly depends on the matrix $R$.

Moving further, each rating $\rui \in R$ is defined as a dot product $\rui =
q_i^t \cdot p_u$, where $q_i \in \mathbb{R}^r$ is a row in $U$ and represents
the item $i$, and $p_u \in \mathbb{R}^r$ is a column in $I$ and represents the
user $u$. We can now understand that, \textbf{$p_u$ represents how well $u$
agrees with each of the $r$ prototype movies, and $q_i$ represents how well $i$
suits the tastes of the $r$ prototype users}.

\subsubsection{Application to recommendation}

This statement is the principal motivation for the use of SVD in recommendation
settings. It postulates the existence of $f$ factors/criteria (whose nature is
not necessarily known) that determine the value of any rating $\rui$.  A user
$u$ is modeled as a vector $p_u \in \mathbb{R}^f$, where each component of
$p_u$ models the importance of the corresponding factor for $u$.  Similarly, an
item $i$ is modeled as a vector $q_i \in \mathbb{R}^f$, where each component of
$q_i$ models how well $i$ fits the corresponding criteria.  From then, a rating
prediction $\predrui$ is calculated as the dot product of the two vectors $p_u$
and $q_i$:
$$\predrui = p_u^t \cdot q_i.$$

Once the number of factors $f$ is set, the problem is here to estimate the
vectors $p_u$ and $q_i$ for every possible user and item. We saw that when the
matrix $R$ is dense (i.e. there is no unknown entry), there exists an analytic
solution for computing the two matrices $U$ and $I$. An equivalent solution is
to solve the following minimization problem\footnote{From then on we will abuse
notation and use $R$ either as a matrix (sparse or dense) and as a rating
dataset as defined in Section \ref{TODO}.}:
$$
\sum_{\rui \in R} \left(\rui - q_i^t \cdot p_u \right)^2.
$$

But what happens in a recommendation setting, where the matrix $R$ is extremely
sparse? The SVD of $R$ is not even properly defined! One of the main
contribution during the Netflix competition was made by Simon Funk, who
empirically showed that we should actually just not care that the matrix $R$ is
sparse, and still solve the same optimization problem only on the known
ratings.  When $R$ is sparse, the problem is not convex in both $p_u$ and $w_i$
so this optimization will only lead to a local minimum,
but this solution turned out very efficient in practice. Before that,
techniques usually involved  a preliminary step where the matrix $R$ was filled
using some heuristic, in order to have a fully defined problem. Funk's
contribution led him to the top 10 competitors of the Netflix competition at
the time, and was heavily used by other competing teams, including the BelKor
team who ended up winning the contest.

The appeal of this basic approach does not only lie in its link to SVD, but
also because it is easily extendable. It was later improved and theoretically
studied in many ways, for example by incorportating user and items biases (that
will be reviewed later in Section \ref{TODO}) (see \ref{TODO}), or by adding
some time-dependend features, taking into account items popularity over time
\ref{TODO}. In \ref{TODO}, this technique was justified from a Bayesian
perspective, and was named Probabilistic Matrix Factorization (PMF), leading to
the following regularized least squares problem:

$$
\sum_{\rui \in R} \left(\rui - q_i^t \cdot p_u \right)^2 +
\lambda\left(\norm{}{q_i}^2 + \norm{}{p_u}^2\right),
$$
where $\lambda$ is a regularization term that corresponds to \todo{what}. When
$R$ is a dense matrix with no unknown entry (and when $\lambda$ is null), the
solution of this optimization problem exactly corresponds that of the SVD.  Not
surprisingly, stochastic gradient descent (SGD) tend to perform really well on
this task and is quite simple to implement. Because we have already talked too
much about matrix factorization, and because a few lines more won't hurt, we
now propose to derive the associated SGD algorithm. Let $L_{ui}$ be the cost
associated with a single prediction $\rui$:
$$L_{ui} \eqdef \left(\rui - q_i^t \cdot p_u \right)^2 +
\lambda\left(\norm{}{q_i}^2 + \norm{}{p_u}^2\right).$$

Denoting err the prediction error ($\text{err} = \rui - q_i^t \cdot p_u$), the
derivative of $L_{ui}$ with respect to $q_i$ is (remember that $q_i$ is a
vector:
$$\frac{\partial L_{ui}}{\partial q_i} = -\text{err} \cdot p_u + 2\lambda
q_i.$$
Similarly, the derivative of $L_{ui}$ with respect to $p_u$ is:
$$\frac{\partial L_{ui}}{\partial p_u} = -\text{err} \cdot q_i + 2\lambda
p_u.$$
With these derivatives in mind, the optimization problem can then be easily
solved by following Algorithm \ref{ALGO:matrix_facto_sgd}.

\begin{algorithm}[!ht]
 \caption{Stochastic Gradient Descent for matrix factorization.}
       \label{ALGO:matrix_facto_sgd}
       \begin{algorithmic}

         \STATE {\bf Input}: A set of ratings $R$, a learning rate $\alpha$ and
         a regularization penalty $\lambda$, a number of iterations $n$.
         \STATE {\bf Output}: $p_u$ and $q_i$, the user and item biases.
         \STATE {\it Initialization:}
         \STATE Randomly initialize $p_u$ and $q_i$
         \FOR{$\text{iteration} \in [1, n]$}
         \FORALL{$\rui \in R$}
         \STATE $\text{err} = \rui - q_i^t p_u$
         \STATE $q_i = q_i - \alpha (\text{err} \cdot p_u - \lambda q_i)$
         \STATE $p_u = p_u - \alpha (\text{err} \cdot q_i - \lambda p_u)$
         \ENDFOR
         \ENDFOR
\end{algorithmic}
\end{algorithm}

Another optimization technique can be used, by noting that if either $p_u$ or
$q_i$ is fixed, we obtain a convex problem that can be solved using
classical least squares methods. The idea is then to first consider all $q_i$
as constants and solve the associated linear problem. Then, the $p_u$ are
considered constant and the associated problem is solved. Repeating these steps
an arbitrary number of time will also converge to a local solution, and this
methods is known as Alternating Least Squares.

\todo{Mettre des refs}

\section{Recommender system evaluation}
\label{SEC:Recommender_system_evaluation}
Providing an accurate measure of the overall quality of a recommender system is
not a simple task and diverse viewpoints have to be considered (see \cite[Ch.
??]{RecoSystemHandbook} for an extensive survey).
% Obviously accuracy is an important matter as  any system includes a
% prediction algorithm, but other properties are also desirable in order  to
% deploy an effective recommendation engine.

In general, we will use $k$-folds cross-validation procedures to reliably
evaluate each of the performence measure. The set of all ratings $R$ is divided
into $k$ (typically $5$) disjoint sets of equal size ; at each of the $k$ iterations, the test
set $\Rtest$ is set to the $k^{\text{th}}$ subset and and the training set
$\Rtrain$ is set as the union of the $k - 1$ remaining subsets: the system will
be trained on $\Rtrain$ and tested on $\Rtest$. The reported performances are
then the average of the $k$ measures.

\paragraph{Accuracy\\}
The performance of the algorithm $A$ is usually evaluated in terms of accuracy,
which measures how close the rating predictions $\predrui$ are to the true
ratings $\rui$, for every possible prediction. The Root Mean Squared Error
(RMSE) is probably the most common indicator of how accurate an algorithm is,
and is calculated as follows:
$$\text{RMSE}(A) = \sqrt{\frac{1}{|\Rtest|} \cdot \sum_{\rui \in
\Rtest}(\predrui - \rui)^2}.$$

Another common indicator for accuracy is the Mean Absolute Error (MAE), where
important errors are not penalized more than small ones:
$$\text{MAE}(A) = \frac{1}{|\Rtest|} \cdot \sum_{\rui \in \Rtest}|\predrui -
\rui|.$$

RMSE probably is by far the most important the most popular measure for
evaluating the performance of a recommender system. The simple fact that \$1
million was awarded for a 10\% improvement of RMSE during the Netflix competition
illustrates the supremacy of RMSE. Yet this measure is also criticized, let
only because it is not very easy to interpret in a meaningful way: if you
end-up with an RMSE of $0.5$, how do you know if it is good or not? You would
need to compare multiple algorithms before knowing what can be thought as a
good RMSE, which by the way highly depends on  the rating scale.
\todo{continuer en citant papier}
To better reflect the user-system interaction, other precision-oriented metrics
are sometimes used in order to provide a more informed view.

\paragraph{Precision and recall\\}
Precision and recall help measuring the ability of a system to provide relevant
recommendations, and are therefore indicators of the performance of the
recommendation strategy $S$. They are defined by means of the number of true
postives, true negatives, false positives and false negatives. As such, they
are mostly used in an implicit rating scheme (or at least with binary rating
scales), but they can also be used when the rating scale is gradual. In this
latter case, the recommendation strategy has to be clearly defined.

In the following, we denote by $I^S$ the set of items that the strategy $S$
will suggest to the users using the predictions coming from $A$. For
ratings in the interval $[1, 5]$, a simple strategy could be for example to
recommend an item $i$ to user $u$ if the estimation rating $\predrui$ is
greater than $4$:
$$I^S \eqdef \Set{i \in I | \exists u \in U, ~\predrui \geq 4,~ \rui \in \Rtest}.$$

We also define $I^*$ as the set of items that are \textbf{actually} relevant to
the users, i.e.  the set of items that would have been recommended to users if
all the predictions made by $A$ were exact:
$$I^* \eqdef \Set{i \in I | \exists u \in U, ~\rui \geq 4,~ \rui \in \Rtest}.$$

\noindent
The \textbf{precision} of the
system is
defined as the fraction of recommended items that are relevant to the users,
and the \textbf{recall} is defined as the fraction of relevant recommended
items over all relevant items:
\begin{align*}
  \text{Precision} &\eqdef \frac{\mid I^S \cap I^*\mid}{\mid I^S \mid},\\
  \text{Recall} &\eqdef \frac{\mid I^S \cap I^*\mid}{\mid I^* \mid}.
\end{align*}

Precision and recall are complementary metrics: it would not make sense to
evaluate an algorithm performance by only looking at the precision, without
considering the recall. It is indeed quite easy to obtain a high precision (by
simply recommending all of the items), but that would lead to a terrible
value of recall. Precision and recall are commonly summarized into the
F-measure, which is their harmonic mean:
  $$
  \text{F} \eqdef 2 \cdot \frac{\text{Precision} \cdot
  \text{Recall}}{\text{Precision} + \text{Recall}}.
  $$

If accurate predictions are crucial, it is widely agreed that it is
insufficient for deploying an effective recommendation engine. Indeed, still
other dimensions are worth estimating in order to get a complete picture of the
performance of a system
\cite{NeeRieKonACM2006,HerKonJohTerRieACM2004,KamBriRecSys2014}.  For instance,
one may naturally expect from a recommender system not only to be accurate, but
also to be surprising, and to be able to recommend a large number of items.

\paragraph{Coverage\\}
Coverage, in its simplest form, is used to measure the ability of a system to
recommend a large amount of items: it is quite easy indeed to create a
recommender system that would only recommend very popular items. Such a
recommender system would drop to zero added value. We here define the
\textbf{coverage} as the proportion of recommended items out of all existing
items:
$$\text{Coverage} \eqdef \frac{\mid I^S\mid}{\mid I\mid}.$$

\paragraph{Surprise\\}
Users expect a recommender system to be surprising: recommending an extremely
popular item is not really helpful. Following the works of
\cite{KamBriRecSys2014}, the surprise associated with a recommendation can be evaluated with
the help of the pointwise mutual information (PMI). The PMI between two items
$i$ and $j$ is defined as follows:
$$\text{PMI}(i, j) \eqdef -\log_2 \frac{P(i, j)}{P(i)P(j)} / \log_2 P(i, j),$$
where $P(i)$ and $P(j)$  represent the probabilities for the items to be rated
by any user, and $P(i, j)$ is the probability for $i$ and $j$ to be rated
together. They are estimated by $P(i) \approx \frac{\mid U_i \mid}{\mid U
\mid}$ and $P(i, j) \approx \frac{\mid U_i \cap U_j \mid}{\mid U\mid}$. PMI
values fluctuate between the interval $[-1, 1]$, $-1$ meaning that $i$ and $j$
are never rated together and $1$ meaning that they are always rated together.
To estimate the surprise of recommending an item $i$ to a user $u$, the authors
propose two definitions:
\begin{itemize}
\item either to take the maximum of the PMI values for $i$ and all other items
  rated by $u$, with $\surpmax(u, i) \eqdef \max\limits_{j\in I_u}
    \text{PMI}(i, j),$
\item
 or to take the mean of these PMI values with $\surpavg(u, i) \eqdef
    \frac{\sum_{j \in I_u} \text{PMI}(i, j)}{\mid I_u\mid }$.
\end{itemize}

The overall capacity of a recommender to surprise its users can be defined as
the mean of all the surprise values for each perdiction. Because they are
defined using the pointwise mutual information, the \textbf{lower} the values
of $\surpmax$ and $\surpavg$, the most \textit{surprising} the recomendations.
