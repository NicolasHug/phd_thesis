\documentclass{llncs}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{wasysym}
\usepackage{fixltx2e}
\usepackage{marvosym}
\usepackage{graphicx}
\hbadness=10000

% command for aggregate
\newcommand{\aggr}[1]{\underset{#1}{\operatorname{aggr}}\;}

%\annee{2015}
\title{Experimenting analogical reasoning in recommendation}

\author{Nicolas Hug(\Letter)\inst{1} \and Henri Prade(\Letter)\inst{2} \and Gilles Richard} 

\institute{Institut de Recherche en Informatique de
Toulouse, Universit\'e Paul Sabatier,\\31062 Toulouse
Cedex 09\\
\email{\{nicolas.hug, henri.prade, gilles.richard\}@irit.fr}}

\begin{document}
\maketitle

\begin{abstract}
Recommender systems aim at providing suggestions of interest for end-users.
Two main types of approach underlie existing recommender systems: content-based
methods and collaborative filtering. In this paper, encouraged by good results
obtained in classification by analogical proportion-based techniques, we
investigate the possibility of using analogy as the main underlying principle
for implementing a prediction algorithm of the collaborative filtering type. The
  quality of a recommender system can be estimated along diverse dimensions.
  The accuracy to predict user's rating for unseen items is clearly an
  important matter. Still other dimensions like {\it coverage} and {\it
  surprise} are also of great interest. In this paper, we describe our
  implementation and we compare the proposed approach with well-known
  recommender systems. 
\end{abstract}

\section{Introduction}\label{intro}
In a world of information overload, automatic filtering tools are essential to
extract relevant information from basic noise. In the field of e-commerce,
recommender systems play the role of search engines when surfing the entire
web: they filter available items to provide relevant suggestions to customers.

%With the huge development of online businesses, recommender systems have become
%more and more popular \cite{handbookRecoSys2011,AdoTuzIEEE2005}. They help to
%answer questions as diverse as ``what movie to rent?", ``what item to buy?" or
%``which restaurant to try?", but also such as ``what piece of code to
%investigate ?" in the case of recommender system for software development.
%They do so by providing the user with a list of recommendations.  Obviously,
%the more personalized the recommendations, the better the system.  For
%instance, a system recommending only the most popular items is useless as it is
%likely that the standard customer knows these items. But the maximization of
%business profit may also lead to suggest items which are not very popular (and
%thus difficult to sell), provided they may be of interest for the user. 

%A very common way of providing personalized recommendations to a target user is
%to estimate its taste for the items the system provides. The taste of a user
%$u$ for a given item $i$ is usually represented as a rating that $u$ would give
%to $i$.  The scale of the ratings may vary, but the integer interval [1, 5] and
%the \textit{like}/\textit{dislike} scales are very common in real world
%systems.
%
%Once these estimations are made, a simple option is to recommend the items with
%the highest ratings among all the estimated scores for the considered user
%(using the implicit assumption that a user should be interested in the items
%with high scores).  
%
%Providing an accurate measure of the overall quality of a recommender system is
%not a simple task and diverse viewpoints have to be considered (see
%\cite{handbookRecoSys2011} for an extensive survey).  Obviously, accuracy of
%its core prediction algorithm is an important matter, but other properties are
%also desirable in order  to deploy an effective recommendation engine. For
%instance {\it coverage} measuring what is the range of items in the catalogue
%that could be recommended, or {\it serendipity} measuring in some sense the
%surprise that a user could get when a new item is suggested.

Besides, analogical reasoning is widely acknowledged as an important feature of human
intelligence \cite{GenHolKok2001,MelVel1998}. It is a powerful way for
establishing parallels between apparently non related objects, and then
guessing relations, properties, or values, on the basis of the observed
similarities and dissimilarities. As a consequence, we may consider an
analogy-based system as a suitable candidate for providing a user with
relevant, and possibly surprising, recommendations.  Indeed, providing the user
with both accurate and surprising recommendation has become a key challenge.
This is the option that we investigate in this paper. 

%An important type of analogical relation is expressed by means of ``analogical
%proportions", which are statements involving 4 items, of the form ``$a$ is to
%$b$ as $c$ is to $d$".   
%%and denoted $a:b::c:d$.
%This expresses that the way $a$ and $b$ differ is similar to the way $c$ and
%$d$ differ. This provides a richer view than  just considering the nearest
%neighbours of items. The use of analogical proportions have led to the design of
%very effective classifiers
%\cite{BayMicDel2007,PraRicYao2010,BouPraRicECAI2014}, but analogy has also been
%proved successful in other classification-related tasks
%\cite{CorPraRicECAI2012}.

%Prediction and classification may be considered as similar tasks, since $n$
%possible scores can be regarded as $n$ classes, even if they are linearly
%ordered. Taking inspiration from previous works in classification, but also
%trying to cope with some apparent specificities of the recommendation problem,
%two analogy-based algorithms are proposed in this paper, each one relying on
%slightly different definitions of the induction principle based on analogical
%proportions. 

Our paper is organized as follows. In the next section, we provide a brief
survey of recommender system technologies. We also review diverse dimensions
along which such a system can be evaluated.  In Section \ref{backAnalogy}, we
provide the necessary background about analogical proportions and how they can
be the basis of an inference process. In Section \ref{anareco}, we investigate
how analogical proportions may provide a clean underlying framework to design a
recommender system.  In Section \ref{results}, we report  the results obtained
on a benchmark and we compare them to those of well-known existing approaches.
Finally, we conclude and provide lines for future research in Section
\ref{conc}.


\section{Background on recommender systems}\label{backReco}
The aim of a recommendation system is to provide users with lists of relevant
personalized items.
Let us now formalize the problem.

\subsection{Problem formalization}
Let $U$ be a set of users and $I$ a set of items. For some pairs $(u,i) \in U
\times I$, a rating  $r_{ui}$ is supposed to have been given by $u$ to express
if he/she likes or not the item $i$. 
It is quite common that  $r_{ui} \in [1, 5]$, 5 meaning a strong preference for
item $i$, 1 meaning a strong rejection, and 3 meaning indifference, or just an
average note.  Let us denote by $R$ the set of ratings recorded in the system. It
is well known that, in real systems, the size of $R$ is very small with regard
to the potential number of ratings which is $|U| \times |I|$, as a lot of
ratings are missing. In the following, $U_i$ denotes the set of users that have
rated item $i$, and $I_u$ is the set of items that
user $u$ has rated. 

To recommend items to users, a recommender system will proceed as follows:
\begin{enumerate}
\item Using a prediction algorithm $A$, estimate the unknown ratings $r_{ui}$
(i.e. $r_{ui} \notin R$). This estimation $A(u, i)$ is usually denoted
$\hat{r}_{ui}$.  
\item Using a recommendation strategy $S$ and in the light of the previously
estimated ratings, recommend items to users. For instance, a basic yet
common strategy is to suggest to user $u$ the items $i \notin I_u$ with the
highest $\hat{r}_{ui}$.
\end{enumerate}
The two main prediction techniques are commonly referred to as {\it
content-based} and {\it collaborative filtering}, that we both briefly review
below.

%depending on the techniques
%deployed by their prediction algorithm $A$. Hybrid methods actually are a mix
%of collaborative and content-based techniques. We will here give a short
%overview of content-based and collaborative filtering techniques, without
%lingering too much over current content-based ones as they have shown to be
%outperformed by collaborative techniques.

\subsection{Content-based techniques}

Content-based algorithms use the metadata of users and items to estimate a
rating. Metadata are external information that can be collected. Typically:
\begin{itemize}
\item  for users: gender, age, occupation, location (zip code), etc.
\item for items: it depends on the type of items, but in the case of movies, it
could be their genre, main actors, film director, etc.  \end{itemize}
 
Based on these metadata, a content-based system will try to find items that are
similar to the ones for which the target user has already expressed a
preference (for instance by giving a high rating). This implies the need for a
similarity measure between items.  A lot of options are available for such
metrics. They will not be discussed here as our method is of a collaborative
nature. 

Indeed, a well-known drawback of content-based techniques is their tendency to
recommend only items that users may already know, and therefore the
recommendations lack in novelty, surprise and diversity. In the following, we
only use collaborative filtering techniques, so we do not consider the use of
any metadata.

\subsection{Collaborative filtering techniques}
\label{CollabFil}
By collaborative filtering, we mean here algorithms that only rely on the set
of known ratings $R$ to make a prediction: to predict $r_{ui} \notin R$, the
algorithm $A$ will output $\hat{r}_{ui}$ based on $R$ or on a carefully chosen
subset.  The main difference between collaborative and content-based method is
that in the former, metadata of items and users are not used, and in the latter
the only ratings we may take into account are that of the target user.  A
popular collaborative filtering technique is neighbourhood-based, that we
describe here in its simplest form.

%The neighbourhood-based approaches have proven
%to be fairly accurate. 
To estimate the rating of a user $u$ for an item $i$, we
select $N_i^k(u)$, the set of $k$ users that are most similar to $u$ and that
have rated $i$. Here again, there is a need for a similarity measure (between
users, and based on their respective ratings). The estimation of $r_{ui}$ is computed as follows:
$$\hat{r}_{ui} = \aggr{v \in N_i^k(u)}r_{vi},$$
where the aggregate function $\aggr{}$ is usually a mean weighted by the similarity between $u$
and $v$. A more sophisticated prediction, popularized by
\cite{BelKorSIGKDD2007} is as follows:
$$\hat{r}_{ui} = b_{ui} + \aggr{v \in N_i^k(u)}(r_{vi} - b_{vi}),$$
where $b_{ui}$ is a baseline (or bias) related to user $u$ and item $i$. It
is supposed to model how $u$ tends to give higher (or lower) ratings than the
average of ratings $\mu$, as well as how $i$ tends to be rated higher or lower
than $\mu$. As it uses the neighbourhood of users to output a prediction, this
technique tends to model local relationships in the data.

Note that it is perfectly possible to proceed in an item-based way. Indeed,
rather than looking for users similar to $u$, one may look for items similar to
$i$, which leads to formulas dual of the above ones. 

%The matrix factorization approach is heavily inspired by singular value
%decomposition (SVD) : it postulates the existence of $f$ factors/criterias
%(whose nature is not necessarily known) that determine the value of any rating
%$r_{ui}$.  A user $u$ is modeled as a vector $p_u \in \mathbb{R}^f$, where each
%component of $p_u$ models the importance of the corresponding factor for $u$.
%Similarly, an item $i$ is modeled as a vector $q_i \in \mathbb{R}^f$, where
%each component of $q_i$ models how well $i$ fits the corresponding criteria.
%From then, a rating prediction $\hat{r}_{ui}$ is calculated as the dot product
%of the two vectors $p_u$ and $q_i$: $$\hat{r}_{ui} = p_u^t \cdot q_i$$
%For the interested reader, see the work of
%\cite{FunkSVD,Koren:2010:FNS:1644873.1644874}
%The number of factors being set, the problem is here to estimate the vectors
%$p_u$ and $q_i$ for every possible users and items. Two methods are commonly
%used for this task: alternating least squares and stochastic gradient descent,
%first popularized by \cite{FunkSVD}.
%
%Contrary to the neighbourhood-based approach, the matrix factorization
%technique tends to model global aspects of the data.  Matrix factorization and
%neighbourhood based techniques have been successfully mixed in the works of
%\cite{Koren:2010:FNS:1644873.1644874}.  


\subsection{Recommender system
evaluation}\label{eval} Providing an accurate measure of the overall quality of
a recommender system is not a simple task and diverse viewpoints have to be
considered (see \cite{handbookRecoSys2011} for an extensive survey). 
% Obviously
%accuracy is an important matter as  any system includes a prediction algorithm,
%but other properties are also desirable in order  to deploy an effective
%recommendation engine.



\paragraph{Accuracy\\}
The performance of the algorithm $A$ is usually evaluated in terms of accuracy,
which measures how close the rating prediction $\hat{r}_{ui}$ is to the true
rating value $r_{ui}$, for every possible prediction. To evaluate the accuracy
of a prediction algorithm, one usually follows the classical machine learning
framework: the set of ratings $R$ is divided into two disjoint sets $R_{train}$
and $R_{test}$, and the algorithm $A$ has to predict ratings in $R_{test}$
based on the ones belonging to $R_{train}$.

The Root Mean Squared Error (RMSE) is a very common indicator of how accurate
an algorithm is, and is calculated as follows:
$$\text{RMSE}(A) = \sqrt{\frac{1}{|R_{test}|}\sum_{r_{ui} \in
R_{test}}(\hat{r}_{ui} - r_{ui})^2}$$
%Another common indicator for accuracy is the Mean Absolute Error (MAE), where
%important errors are not penalized more than small ones:
%$$\text{MAE}(A) = \frac{1}{|R_{test}|}\sum_{r_{ui} \in R_{test}}|\hat{r}_{ui} -
%r_{ui}|.$$

To better reflect the user-system interaction, other precision-oriented metrics
are generally used in order to provide a more informed view.

\paragraph{Precision and recall\\}
Precision and recall help measuring the ability of a system to provide
relevant recommendations.
In the following, we denote by $I_{S}$ the set of items that the strategy
$S$ will suggest to the users using the predictions coming from $A$. For
ratings in the interval $[1, 5]$, a simple strategy could be for example to
recommend an item $i$ to user $u$ if the estimation rating $\hat{r}_{ui}$ is
greater than $4$.  $$I_S = \{i \in I | \exists u \in U, \hat{r}_{ui} \geq
4\}.$$

Let $I_{relev}$ be the set of items that are actually relevant to users (i.e.
the set of items that would have been recommended to users if all the
predictions made by $A$ were exact). 
The precision of the system is defined as the fraction of recommended items that
are relevant to the users:
$$\text{Precision} = \frac{|I_{S} \cap I_{relev}|}{|I_{S}|},$$
and the recall as the fraction of recommended items that are relevant to the
users out of all possible relevant items:
$$\text{Recall} = \frac{|I_{S} \cap I_{relev}|}{|I_{relev}|},$$

If accurate predictions are crucial, it is widely agreed that it is
insufficient for deploying an effective recommendation engine. Indeed, still
other dimensions are worth estimating in order to get a complete picture of the
performance of a system.
\cite{NeeRieKonACM2006,HerKonJohTerRieACM2004,KamBriRecSys2014}.
For instance, one may naturally expect from a recommender system
not only to be accurate, but also to be surprising, and to be able to recommend
a large number of items. When evaluating the recommendation strategy, one must
keep in mind that its performance is closely related to that of the algorithm
$A$, as the recommendation strategy $S$ is based on the predictions provided by
$A$.


\paragraph{Coverage\\}
Coverage, in its simplest form, is used to measure the ability of a system to
recommend a large amount of items: it is quite easy indeed to create a
recommender system that would only recommend very popular items. Such a
recommender system would drop to zero added value. Coverage can be defined as the
proportion of recommended items out of all existing items:
$$\text{Coverage} = \frac{|I_{S}|}{|I|}.$$



\paragraph{Surprise\\}
Users expect a recommender system to be surprising: recommending an extremely
popular item is not really helpful. Following the works of \cite{KamBriRecSys2014}, surprise of a recommendation can be evaluated with the help of the pointwise mutual
information (PMI). The PMI between two items $i$ and $j$ is defined as follows:
$$PMI(i, j) = -\log_2 \frac{p(i, j)}{p(i)p(j)} / \log_2 p(i, j),$$
where $p(i)$ and $p(j)$  represent the probabilities for the items to be rated
by any user, and $p(i, j)$ is the probability for $i$ and $j$ to be rated
together : $p(i) = \tfrac{|U_i|}{|U|}$ and $p(i, j) = \tfrac{|U_i \cup
U_j|}{|U|}$. PMI values fluctuate between the interval $[-1, 1]$, $-1$ meaning
that $i$ and $j$ are never rated together and $1$ meaning that they are always
rated together. To estimate the surprise of recommending an item $i$ to a user
$u$ we have two choices: 
\begin{itemize}
\item either to take the maximum of the PMI values for $i$ and all
other items rated by $u$, with $Surp^{max}(u, i) = \max\limits_{j\in I_u} PMI(i, j)$
\item
 or to take the mean of these PMI values with $Surp^{avg}(u, i) =
\frac{\sum_{j \in I_u} PMI(i, j)}{|I_u|}$
\end{itemize}
Then the overall capacity of a recommender to surprise its users is the mean of the
surprise values for all predictions.

\section{Background on analogical proportions}\label{backAnalogy}
An analogical proportion ``$a$ is to $b$ as $c$ is to $d$'' states analogical relations between the pairs $(a,b)$ and $(c,d)$, as well as between the pairs $(a,c)$ and $(b,d)$. 
%There are numerous examples of such statements, with which everybody will more or less agree, such as  ``calf is to cow as foal is to mare'', or ``brush is to painter as chalk is to teacher''. However, 
It is only rather recently that formal definitions have been proposed for analogical proportions, in different settings  \cite{StroppaYvon2006,LepageHDR2003,MicPraECSQARU2009}. In this section, we provide a brief account of a formal view of analogical relations that underlie their use in the proposed algorithm.
% For general introductions and details on the Boolean and multiple-valued logic views, the reader is referred to 
For more details, see \cite{PraRicLU2013,PraRicECSQARU2013,PraRicIFCOLOG2014}. 
 
 \paragraph{Formal framework\\}
It has been agreed, since Aristotle time, that an analogical proportion $T$, as a quaternary relation, satisfies the three following characteristic properties:
\begin{enumerate}
\item $T(a,b,a,b)$ (reflexivity)
\item $T(a,b,c,d) \implies T(c,d,a,b)$ (symmetry)
\item $T(a,b,c,d) \implies T(a,c,b,d)$ (central permutation)
\end{enumerate}
There are various models of analogical proportions, depending on the target domain.
When the underlying domain is fixed, $T(a,b,c,d)$ is simply denoted $a:b::c:d$. Standard examples are: 
\begin{itemize}
\item Domain $\mathbb{R}$: $a:b::c:d \text{ iff } a/b= c/d \text{ iff } ad = bc$ (geometric
proportion)
\item Domain $\mathbb{R}$: $a:b::c:d \text{ iff } a-b= c-d \text{ iff } a + d= b + c$ (arithmetic proportion)
\item Domain $\mathbb{R}^n$:
$\vec{a}:\vec{b}::\vec{c}:\vec{d}
\mbox{ iff } \vec{a}-\vec{b} =
\vec{c}-\vec{d}$. This is just the extension of arithmetic proportion to real vectors. In that case, 
the 4 vectors $\vec{a}, \vec{b}, \vec{c}, \vec{d}$ build up a parallelogram.
%\footnote{The idea of the parallelogram as providing a geometrical view of analogy has been often emphasized \cite{LepageHDR2003,BayMicDelMouIAF2007,StroppaYvon2006,PraRicLU2013}. However note that this does not necessarily lead to a graded view of an analogical proportion, since the definition is stated here in an all-or-nothing way. Obviously, some tolerance might be introduced when judging if $\vec{a}, \vec{b}, \vec{c}, \vec{d}$ make a parallelogram, or not.}
\item Domain $\mathbb{B}=\{0,1\}$: $a:b::c:d \text{ iff } (a \wedge d \equiv b \wedge c) 
\wedge (a \vee d \equiv b \vee c)$
%\footnote{Note that this is indeed a Boolean counterpart to the idea that the product (or the sum) of the means is equal to the product (or the sum) of the extremes. Moreover, this logical expression is equivalent to $(a \wedge \neg b \equiv c \wedge \neg d) 
%\wedge (\neg  a \wedge b \equiv \neg  c \wedge d)$ \cite{MicPraECSQARU2009}. This latter expression, which parallels $a-b= c-d$ in the numerical case, has two main multiple-valued logic extensions, replacing $\mathbb{R}$ by $[0,1]$  \cite{PraRicECSQARU2013}. With one of these extensions $a:b::c:d=1$ iff $a-b= c-d$, while the other further requires that $a=c$. Besides, both extensions are graded.}
\item Domain $\mathbb{B}^n$: $\vec{a}:\vec{b}::\vec{c}:\vec{d}
\mbox{ iff }
\forall i \in [1,n], \quad a_i:b_i::c_i:d_i$.
\end{itemize}
Other definitions are available when dealing with matriceces, formal concepts or lattices, etc. (see \cite{StroppaYvon2006,LepageHDR2003} for other options). In this paper, we are interested in evaluating analogical proportions between ratings, which might be Boolean (\textit{like}/\textit{dislike}), or in our case study, integer-valued (using the scale $\{1, 2, 3, 4, 5\}$). 
%From the very beginning, the idea of analogical proportion parallels the notion of numerical proportion, and numerical proportions provide potential definitions for analogical proportions in numerical domains.

\paragraph{Equation solving\\}
Starting from such an analogical proportion, the equation solving problem
amounts to finding a fourth element $x$ to make the incompletely stated proportion $a:b::c:x$ to hold.  
As expected, the solution of this problem depends on the domain on which the
analogy is defined. For instance, in
the case of extended arithmetic proportions, the solution always exists and is unique:
$\vec{x}= \vec{b}-\vec{a} +
\vec{c}$. In terms of geometry, this simply tells us that given 3 points, we can always find a fourth one  to build a parallelogram. 

The existence of a unique solution is not always granted: for instance in the
Boolean setting, the solution may not exist \cite{MicPraECSQARU2009}.

%In the case of Boolean proportions, if it is easy to complete $1:0::1:x$ to make it valid, there is no option to complete $1:0::0:x$ in a valid way.
%\footnote{In the Boolean setting, $a:b::c:d=1$ for only six patterns: $1:1::1:1$, $0:0::0:0$, $1:1::0:0$, $0:0::1:1$, $1:0::1:0$, and $0:1::0:1$, as can be checked from the definition.}
%In other words, the equation $a:b::c:x=1$ is solvable under some constraints.
\paragraph{Analogical inference\\}
In this perspective, analogical reasoning can be viewed as a way to infer new
plausible information, starting from observed analogical proportions.
%Let us formalize this in the case where the items at hand
%$\vec{a},\vec{b},\vec{c}, \vec{d}$  are vectors of
%dimension $n$ (real valued or Boolean valued vectors).
The {\it analogical jump} is an unsound
inference principle postulating that, given 4 vectors
$\vec{a},\vec{b},\vec{c}, \vec{d}$
such that the proportion holds  on some components, then it should also hold on the
remaining ones.  This can be stated as (where $\vec{a} = (a_1, a_2, \cdots
a_n)$, and $J \subset [1,n]$): $$\frac{\forall j \in J,
a_j:b_j::c_j:d_j}{\forall i \in [1,n] \setminus J, a_i:b_i::c_i:d_i} \quad
(analogical ~ inference)$$ 
\noindent
This principle leads to a prediction rule in the following context:  
\begin{itemize}
\item 4 vectors $\vec{a},\vec{b},\vec{c},
\vec{d}$ are given where $\vec{d}$ is partially known: only the
components of $\vec{d}$ with indexes in $J$ are known. 
\item Using analogical inference, we can
predict the missing components of $\vec{d}$ by solving (w.r.t $d_i$) the set of
equations (in the case they are solvable): $$\forall i \in [1,n]
\setminus J, \quad a_i:b_i::c_i:d_i.$$
\end{itemize}
In the case where the items are such that their last component is a label,
applying this principle to a new element $\vec{d}$ whose label is
unknown leads to predict a candidate label for $\vec{d}$. 
%When
%several triples $\vec{a},\vec{b},\vec{c}$ can
%be found, making analogical proportions on $J$, several distinct candidate
%labels may be obtained, and a  ``synthesis'' has to be done (using a voting
%procedure, or any applicable aggregation technique) to output a final label.
%As mentioned in the introduction, this inference principle has been
%successfully applied for classification purposes.


\section{Analogical recommendation}\label{anareco}
%We have developed two different algorithms for recommendation, each one being
%inspired by two different views of the analogical inference principle. In both
%cases, 
The main idea is that if an analogical proportion stands between four
users $a, b, c, d$, meaning that for each item $j$ that they have commonly
rated, the analogical proportion $r_{aj} : r_{bj} :: r_{cj} : r_{dj}$ holds,
then it should also hold for an item $i$ that $a, b, c$ have rated but $d$ has
not (i.e. $r_{di}$ is the missing component). This leads us to estimate
$r_{di}$ as the solution $x = \hat{r}_{di}$ of the following analogical equation:
$$r_{ai} : r_{bi} :: r_{ci} : x.$$
\noindent
Given a pair $(u,i)$ such that $r_{ui} \notin R$ (i.e. there is no available 
rating from user $u$ for item $i$), the main procedure is as follows:
\begin{enumerate}
\item find the set of 3-tuples of users $a, b, c$ such that an analogical proportion
stands between $a, b, c,$ and $u$ and such that the equation $r_{ai} : r_{bi} :: r_{ci} : x$
is solvable.
\item solve the equation $r_{ai} : r_{bi} :: r_{ci} : x$ and consider the solution
$x$ as a candidate rating for $r_{ui}$.
\item set $\hat{r}_{ui}$ as an aggregate of all candidate ratings.
\end{enumerate}
%\noindent Let us consider the 2 options that we have implemented as this
%procedure heavily relies on the view we have for the analogical
%proportion-based inference.

%\subsection{Algorithm using arithmetic proportion}
%In the following, let $I_{abcu}$ be the set of items $i$ that users $a, b, c,$
%and $u$ have commonly rated ($r_{ai} \in R, r_{bi} \in R, r_{ci} \in R \text{
%and }r_{ui}\in R$).
The first step simply states that users $a, b, c$ and $u$, considered as
vectors of ratings, constitute a parallelogram.  This condition is a bit strong
and we may want to relax it by allowing some deformation of this parallelogram.
This can be done by choosing another condition, such as $||(a-b) - (c-d)|| \leq
\lambda$ where $\lambda$ is a suitable threshold and $||.||$ denotes the
Euclidean norm. 

Another modification to the algorithm would be to only search for the users
$a, b,$ and $c$ in a subset of $U$. One may consider the set of the $k$-nearest
neighbours of $d$, using the assumption that neighbours are the most relevant
users to estimate a recommendation for $d$.

Obviously, analogical proportion may be applied in an item-based way rather
than in a user-based way, as in the case of standard techniques. Both views
will be considered in the experimentation.

\paragraph{Implementation\\}
In our implementation, we consider the basic definition of analogy using the
arithmetic definition in $\mathbb{R}^n$: $\vec{a}:\vec{b}::\vec{c}:\vec{d}
\iff \vec{a}-\vec{b}=\vec{c}-\vec{d}$.
Given 4 users, they are represented as real vectors of dimension $m$, where $m$
is the number of item they have rated in common. This dimension can
change with the 4-tuples of users that we consider. 
Then, the threshold $\lambda$ has to be a function of this dimension $m$,
as the range of values that $||.||$ may take depends on it. Using
cross-validation, we have found that $\lambda = \frac{3}{2} \cdot\sqrt{m}$ showed the
best results and acts as a kind of normalization factor.
Our pseudo-code is described in
Algorithm \ref{algo}.
%When extended to vectors, this means that $(a,b,c,d)$ are the vertices of a
%parallelogram.  A direct implementation of the above procedure leads to the
%algorithm described in Algorithm \ref{algo1}.
 \begin{algorithm}[!ht]
     \caption{Analogy}
       \label{algo}
       \begin{algorithmic}

      \STATE {\bf Input}: A set of known ratings $R$, a user $u$, an item
      $i$ such that $r_{ui} \notin R$.
      \STATE {\bf Output}: $\hat{r}_{ui}$, an estimation of $r_{ui}$

      \STATE {\bf Init}:
      \STATE $C = \varnothing$ \quad \quad // list of candidate ratings
      \FORALL{
        users $a, b, c$ such that\\ 
        \begin{enumerate} 
        \item $r_{ai} \in R, r_{bi} \in R, r_{ci} \in R$
        \item $r_{ai} - r_{bi} = r_{ci} - x$ is solvable  // i.e. the solution $x \in [1,5]$
        \item $||(a-b)-(c-d)||\leq \lambda$ // Analogy almost stands between $a,b,c,d$ considered as real vectors
        \end{enumerate}
      }
      
      \STATE  $x \leftarrow r_{ci} - r_{ai} + r_{bi}$
      \STATE $C \gets C \cup \{x\}$ \quad // add x as a candidate rating
	  \ENDFOR

    \STATE $\hat{r}_{ui} = \aggr{x \in C} x$

\end{algorithmic}
\end{algorithm}
%Some options are available at this stage. 

%Here again, there is no need to choose the Euclidean norm (i.e. $L_2$ norm) as
%other norms also make sense: $L_1$ norm using the absolute value of the
%components or even $L_\infty$ using the maximum of the absolute value of the
%components. These options have not been tested so far.

%We have considered a strict condition for the solvability of the equation
%$r_{ai} - r_{bi} = r_{ci} - x$: as the exact arithmetic result $x=r_{ci} +
%r_{bi} -r_{ai}$ does not necessarily belong to the scale interval $[1,5]$, we
%consider solvability only when $r_{ai}=r_{bi}$ or $r_{ai}=r_{ci}$. In both
%cases, we ensure the solution $x \in [1,5]$.



%\subsection{Algorithm based on a pattern-directed inference principle} An
%intuitive idea underlying recommendation is that users who have already
%expressed similar tastes on a number of items should have similar tastes on
%other items. Still one may observe that in a group of users having similar
%tastes on many items, they may disagree on some particular items. With the
%previous algorithm, nothing forbids that i) four users $a, b, c, d$, described
%by the vectors of their rates on movies graded by all of them, have quite
%similar grades on all movies (with maybe a few cases where one disagrees with
%respect to the three others), thus making an approximate  parallelogram (thanks
%to the tolerance), while ii) the equation to solve $r_a, r_b, r_c, x$ on the
%movie not seen by $d$, is such that $r_a$ approximately equals $r_c$, and
%strongly differs from $r_b, $. In such a situation, we may consider that it is
%dubious to apply the analogical inference principle. Indeed there is no pattern
%among the movies seen by the four users that resembles the one of the equation
%to solve, i.e., where $b$ departs from $a$ and $c$, but agrees with $d$. We may
%think that the existence of such patterns suggesting that while $a, b, c, d$
%have often similar tastes, there are cases where $a, c$ depart from $b, d$, are
%needed in order to support the analogical inference. This observation leads to
%modify the inference principle, as explained now.
%
%Just as for the previous algorithm, we will need to find 3-tuples of users $a,
%b, c$ such that the the analogical equation $r_{ai} : r_{bi} :: r_{ci} : x = 1$
%is solvable. 
%
%For each item $j \in I_{abcu}$ we classify the 4-tuples $(r_{aj}, r_{bj},
%r_{cj}, r_{uj})$ into 3 different categories:
%
%\begin{enumerate}
%\item \textit{agreement}: $(r_{aj}, r_{bj}, r_{cj}, r_{uj}) \in \text{Cat
%1}$ iff three of them are equal and the fourth one differs with a difference of
%at most 1.\\ Ex: $(4, 4, 4, 4) \in \text{Cat 1}$, $(4, 3, 4, 4) \in \text{Cat
%1}$, and $(4, 3, 4, 3) \notin \text{Cat 1}$
%
%\item \textit{Change in the same direction}:\\ $(r_{aj}, r_{bj}, r_{cj},
%r_{uj}) \in \text{Cat 2}$ iff 
%\begin{enumerate}
%\item $r_{aj}$ differs from $r_{bj}$ with a change of at least $2$, and the
%same goes for $r_{cj}$ and $r_{uj}$
%\item the change between $r_{aj}$ and $r_{bj}$ is in the same direction of that
%of $r_{cj}$ and $r_{uj}$.
%\end{enumerate}
%
%\item \textit{others} (Cat 3) : 4-tuples that do not belong to the first two categories
%belong to the third one.
%\end{enumerate}
%
%It is clear that especially when $|\text{Cat 3}|$ is much smaller than
%$|\text{Cat 1}| + |\text{Cat 2}|$, the analogical proportion holds (at least
%approximately) between the four set of grades.
%
%\paragraph{Equation solving and candidate solution filtering\\}
%We have 3 cases depending on the pattern of the analogical equation $r_{ai} :
%r_{bi} :: r_{ci} : x $:
%
%\begin{enumerate}
%\item $a:a::a:x \implies x = a$. $x$ is a candidate solution iff $|\text{Cat
%1}| \geq |\text{Cat 2}| + |\text{Cat 3}|$
%\item $a:b::a:x, |a - b| \geq 2 \implies x = b$.\\ $x$ is a candidate solution
%iff $|\text{Cat 2}| > |\text{Cat 3}|$ 
%\item $a:b::a:x, |a - b| = 1 \implies x = b$.\\ $x$ is a candidate solution iff
%$|\text{Cat 1}| \geq |\text{Cat 2}| + |\text{Cat 3}|$ or $|\text{Cat 2}| >
%|\text{Cat 3}|$
%\end{enumerate}
%
%Then, all the candidate solutions are aggregated, just as in the first algorithm.
%
\section{Experiments and results}\label{results}
%The different algorithms we have tested are our two analogy-based algorithms,
%referred to as \textit{Paral} and \textit{Pattern}. 

Our algorithm \textit{Analogy} ($k=20$) is compared to the neighbourhood-based algorithms
described in section \ref{CollabFil}, referred to as \textit{kNN} for the basic
model and \textit{Bsln-kNN} for the extended model using a baseline predictor
(with $k=40$). For each of the algorithms, we have estimated the metrics described
in section \ref{eval}. The recommendation strategy $S$ is to recommend $i$ to
$u$ if $\hat{r}_{ui} \geq 4$.

%\subsection{Dataset and evaluation protocol}
\paragraph{Dataset\\}
We have tested and compared our algorithm on the Movielens-100K
dataset\footnote{http://grouplens.org/datasets/movielens/}, composed of 100,000
ratings from 1000 users on 1700 movies. Each rating belongs to the interval
$[1, 5]$.

\paragraph{Evaluation protocol\\}
In order to obtain meaningful measures, we have run a five-folds
cross-validation procedure: for each of the five steps, the set of ratings $R$ is
split into two disjoint sets $R_{train}$ and $R_{test}$, $R_{train}$ containing
five times more ratings than $R_{test}$. The reported measures are averaged
over the five steps.

%Each dataset is constituted by a matrix $R$ of actual ratings. 
%This matrix has been split into a training part $Train$ and a testing part $Test$.
%$Test$ represents the set of pairs (user,item) for which the prediction algorithm
%$A$ of the target system is supposed to provide a score prediction $A(u,m)$, taking
%into account the available information from $Train$. It does not mean, for instance, that $u$ does not
%appear in $Train$: $u$ appears but $Train(u,m)$ is hidden in the sense 
%$Train(u,m)=0$.
%\textcolor{red}{TO BE EXPLAINED - should be crystal clear ;-)}
%$Train$ is just 80\% of $R$ and $Test$ the remaining 20\%. We proceed that way 5 times and
%we then average the resulting values (5-fold cross validation).

%\subsection{Results and comments}
\paragraph{Results and comments\\}

Table \ref{table:res} shows the performances of the algorithms applied in a
user-based way. Similar experiments have been led in a movie-based setting, exhibiting
very similar results, slightly worse for RMSE (about 5\permil\ higher).

\begin{table}[!ht]
\centering
\caption{Performance of algorithms}
\label{table:res}
\begin{tabular}{| c || c | c | c | c | c | c | c | c |}
\hline
& RMSE & Prec & Rec & Cov & $Surp^{max}$ & $Surp^{avg}$ & Time \\
\hline
Analogy   & .898 & 89.1 & 43.3 & 31.2 & 0.433 & 0.199 & 2h \\
%Pattern & .927 & 84.9 & 50.0 & 47.7 & 0.433 & 0.198 & 5h \\
kNN     & .894 & 89.1 & 44.1 & 27.8 & 0.432 & 0.198 & 10s \\
Bsln-kNN & .865 & 88.4 & 44.0 & 44.7 & 0.431 & 0.199 & 10s \\
\hline
\end{tabular}

\end{table}

As expected, the Bsln-kNN algorithm is more accurate than the basic
collaborative filtering method (KNN). The two classical collaborative
algorithms perform better than the new proposed analogy-based method in terms
of RMSE. Still, there seems to be some room for improvement for the analogical
approach, with the help of a careful analysis of the behaviour of the
algorithm.

As for performances other than RMSE, we see that the figures obtained by the
three algorithms are quite close. Regarding surprise, which is a delicate
notion to grasp, one may also wonder if the used measure is fully appropriate.

As usual, analogy-based algorithms suffer from their inherent cubic complexity.
In the case of recommender systems where millions of users/items are involved,
this is also a serious issue. 

\section{Conclusion and future research}\label{conc}

This is clearly a preliminary study of an analogical approach to prediction in
recommendation, a topic that has never been addressed before. First results are
not better than the ones obtained with standard approaches. Even if they do not
look that far, the difference is still significant enough to have a genuine
impact on the users' experience. However, it is interesting to observe that
approaches based on quite different ideas may lead to comparable results.

Besides, it should be recognized that the prediction part of the recommendation
problem, although somewhat similar to a classification problem (for which
analogical proportion-based classifiers are successful), presents major
differences, since grades on items (playing here the role of descriptive
features) may be both quite redundant and somewhat incomplete for providing a
meaningful profile of a user. This may explain that the application of an
analogical proportion-based approach is less straightforward in recommendation
than in classification. 

Analogical proportion-based methods have also been used recently for predicting
missing Boolean values in databases \cite{DBLP:conf/ipmu/BeltranJP14}. The
recommendation problem can be also viewed as a problem of missing values, but
here the proportion of unknown data is very high, and data are not Boolean. This again suggests that the recommendation task is more difficult.

There are quite a number of issues to further explore, such as understanding on
what types of situation an analogical proportion-based approach would perform
better, and when another view is preferable. Another basic issue is the fact
that users likely use the rating scale $\{1, 2, 3, 4, 5\}$ in an ordinal way
rather than in an absolute manner, since the meaning of a rating may change
with users. This calls for the use of analogical proportion between ordinal
data \cite{MicletBarbot09}.

Lastly, the ideas of the exploitation of the creative power of analogy for i)
proposing items never considered by a user, but having some noticeable common
features with items (s)he likes \cite{Takagi2011}, ii) of the explanation power
of analogy for suggesting to the user why an item may be of interest for him,
are still entirely to explore.

\newpage

\bibliographystyle{plain}
\bibliography{biblio,biblioRecSys}
\end{document}
