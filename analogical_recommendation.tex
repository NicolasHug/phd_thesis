\chapter{Analogical recommendation}
\label{CHAP:analogical_recommendation}

\initial{I}n this chapter, we present our contribution to...


\section{An algorithm using arithmetic proportion}
We will here describe our first attempt to use an analogical proportion-based
algorithm for recommender systems. The main idea is quite simple and is
directly inpspired by the analogical classifiers that we have extensively
studied in Chapter \ref{CHAP:functional_definition}.
\subsection{Algorithm}

Here again, we will make use of the analogical inference principle, but we will
state it in slightly different terms. In chapters
\ref{CHAP:formal_analogical_proportions} and \ref{CHAP:functional_definition},
the analogical inference principle stated that if four vectors $\mathbf{a},
\mathbf{b}, \mathbf{c}, \mathbf{d}$ are in proportion, then their labels should
also be in proportion.  Here, we will say that if an analogical proportion
stands between four users $a, b, c, d$, meaning that for each item $j$ that
they have commonly rated, the analogical proportion $r_{aj} : r_{bj} :: r_{cj}
: r_{dj}$ holds, then it should also hold for an item $i$ that $a, b, c$ have
rated but $d$ has not (i.e. $r_{di}$ is the missing component):
$$
\infer{
r_{ai} : r_{bi} :: r_{ci} : r_{di} ~~ \forall i \in I_a \cap I_b \cap I_c,
\text{ and } i\notin I_d }{r_{aj} : r_{bj} :: r_{cj} : r_{dj} ~~ \forall j \in
I_a \cap I_b \cap I_c \cap  I_d}
$$
This leads us to estimate $r_{di}$ as the solution $y$ of the following
analogical equation:
$$r_{ai} : r_{bi} :: r_{ci} : y.$$
Given a pair $(u,i)$ such that $\rui \notin R$, the main procedure to
estimate $\rui$ is as follows:
\begin{enumerate}
\item Find the set of 3-tuples of users $a, b, c$ such that an analogical
  proportion stands between $a, b, c,$ and $u$ and such that the equation
    $r_{ai} : r_{bi} :: r_{ci} : x$ is solvable.
\item Solve the equation $r_{ai} : r_{bi} :: r_{ci} : y$ and consider the
  solution $x$ as a candidate rating for $r_{ui}$.
\item Set $\hat{r}_{ui}$ as an aggregate of all candidate ratings.
\end{enumerate}

This pseudo-algorithm is exactly that of a conservative classifier described in
Section \ref{SEC:conservative_classifier}, except that here, the instance space
(i.e. the space of $a, b, c, u$) changes with every pair $(u, i)$.

In our implementation, we have used the arithmetic proportion in
$\mathbb{R}^m$: $a:b::c:u \iff a - b = c - u$.  The four users $a, b, c, d$ are
here considered as vectors of ratings in a space defined by their common items.
For the sake of clarity though, we will not write them with boldface letters
and $a$ will denote both the user $a$ as well as its vectorial representation.
Let us consider Figure \ref{FIG:analogical_recommendation}: the four users $a,
b, c$ and $u$ are in proportion, i.e. they make up a parallelogram in the
2-dimensional space of their common items, here Titanic and Toy story. If the
three users $a, b, c$ have rated a movie $i$ that $u$ has not rated, the
estimation of $\rui$ will be such that $a, b, c$ and $u$ still make up a
parallelogram, but this time in a 3-dimensional space. This process is also
explained in Table \ref{TAB:analogical_recommendation}

\begin{figure}[!h]
\centering
  \includegraphics[width=2.5in]{figures/analogical_recommendation.pdf}
  \caption{Four users $a, b, c, u$ that are in proportion.}
\label{FIG:analogical_recommendation}
\end{figure}

\begin{table}[h!]
\centering
  \begin{tabular}{ c   c  c  c  c  c  }
\toprule
 & $j_1$ & $j_2$ & $j_3$ & $\cdots$ & $i$\\
  \midrule
$a$ & 1 & 4  & 3 & $\cdots$ & 2 \\
$b$ & 5 & 2  & 3 & $\cdots$ & 4 \\
$c$ & 1 & 5  & 3 & $\cdots$ & 3 \\
$u$ & 5 & 3  & 3 & $\cdots$ & ? \\
\bottomrule
\end{tabular}
\caption{The four users $a, b, c, u$ are in proportion for every item $j$ that
  they have commonly rated. For an item $i$ that $u$ has not rated, the
  prediction $\predrui$ is set as the solution of the analogical equation
  $2:4::3:?$, i.e. $\predrui = 3 - 2 + 4 = 5$, using the arithmetic proportion.}
\label{TAB:analogical_recommendation}
\end{table}

Just like with the conservative classifier, we will face the problem that in
practice, we may not find any $3$-tuple $a, b, c$ such that a perfect
proportion stands between $a, b, c$ and $u$. We will thus allow some distortion
of shape for the parallelogram $abcu$ by choosing another condition, for
example that $\norm{}{(a-b) - (c-u)} \leq \lambda$, where $\lambda$ is a
suitable threshold and $\norm{}{\cdot}$ is any $p$-norm. Note that this
relaxing of the definition of an analogical proportion exactly corresponds to
the usage of an analogical dissimilirity, so technically our algorithm is very
close to that of an extended classifier.

Our analogical proportion-based algorithm for recommendation is described by
Algorithm \ref{ALGO:analogical_recommendation}.
 \begin{algorithm}[!ht]
       \begin{algorithmic}

      \STATE {\bf Input}: A set of known ratings $R$, a user $u$, and an item
         $i$ such that $r_{ui} \notin R$.
      \STATE {\bf Output}: $\hat{r}_{ui}$, an estimation of $r_{ui}$.

      \STATE {\bf Init}:
      \STATE $C = \varnothing$ \quad \quad // The set of candidate ratings
      \FORALL{
        users $a, b, c$ in $U_i$ such that:\\
        \begin{itemize}
        \item $\norm{}{(a-b)-(c-d)}\leq \lambda$
        \item $r_{ai} : r_{bi} :: r_{ci} : y$ is solvable
        \end{itemize}
      }

      \STATE  $y \leftarrow r_{ci} - r_{ai} + r_{bi}$
      \STATE $C \gets C \cup \set{y}$ \quad // Add $y$ as a candidate rating
	  \ENDFOR

    \STATE $\hat{r}_{ui} = \aggr{y \in C} y$

\end{algorithmic}
     \caption{Analogical proportion-based algorithm for recommendation.}
       \label{ALGO:analogical_recommendation}
\end{algorithm}

We will note here an important point: while an extended classifier would look
for the $k$ $3$-tuples with the least values of analogical dissimilarity, we
here look for all three tuples whose AD is below some given threshold. This is
a common variant of the $k$-NN algorithm: you can either look for the $k$
nearest neighbors, or look for all instances that are within a given distance.

We have considered a strict condition for the solvability of the equation
$r_{ai} - r_{bi} = r_{ci} - y$: as the exact arithmetic result $y=r_{ci} +
r_{bi} -r_{ai}$ does not necessarily belong to the rating scale used in our
experiments (which is $[1,5]$), we have
considered that the equation is solvable only when $r_{ai}=r_{bi}$ or
$r_{ai}=r_{ci}$. In both cases, we ensure the solution $x \in
[1,5]$.\todo{tester avec juste appartenance}

Just like for the neighborhood approach described earlier, it is perfectly
possible to apply this algorithm in an in an item-based way rather
than in a user-based way. I.e. instead of looking for $3$-tuples of users, we
may look for $3$-tuples of items, etc. As both methods lead to similar result
with regard to the performances of the others recommendation algorithms, we
will only focus on the user-based approach described here.

In the next section, we will extensively study the performances of our
analogical recommender and compare it other standard recommendation algorithms.

\subsection{Experiments and results}
\label{results}

The performances of our analogical recommender are summed up in Table
\ref{TAB:parall_performances_comparison}. Various options were considered, that
we will describe in a moment. We also report the performances of other standard
recommendation algorithms that we have already mentioned:
\begin{itemize}
  \item The basic neighborhood approach (denoted $k$-NN) with different
    similarity metrics, namely Mean Squared Difference, Cosine similarity and
    Pearson similarity.  For these three algorithms, the size of the
    neighborhood was set to $k=40$
  \item The matrix-factorization algorithm (denoted PMF), as described in section \ref{TODO}.
    The number of factors was set to $f = 100$, and the optimization problem
    was solved by a stochastic gradient descent of $20$ iterations with
    a constant learning rate of $0.005$ and a regularization penalty of $0.02$.
    These values are those recommended by the authors in \ref{TODO} for the
    Netflix dataset, and turned out to be quite efficient for our experiments.
  \item For the sake of completness we also report results for an algorithm
    that always predict the average of all ratings ($\predrui = \mu$ for all
    $u, i$), and for a random algorithm that predicts random ratings based on
    the distribution of the dataset, which is assumed to be normal.
\end{itemize}

The reported metrics are RMSE, precision, recall, F-measure, coverage, and
surprise. Precision, recall, F-measure and coverage  are reported as
percentages. Remember that for these dimensions high values mean high
performance, while for RMSE and surprise low values are better.  RMSE is the
only measure that only depends on the prediction algorithm $A$. All the other
dimension depend on the items that we actually choose to recommend, and as such
rely on the the recommendation strategy $S$. Here, we have chosen is to
recommend $i$ to $u$ if $\hat{r}_{ui} \geq 4$.

All reported results are averaged over a 5-folds cross-validation
procedure. Obviously, the five folds are the same for all of the algorithims,
to allow for meaningful comparisons.  The dataset that we used is the
Movielens-100K dataset\footnote{http://grouplens.org/datasets/movielens/},
composed of 100,000 ratings from 1000 users and 1700 movies. Each rating
belongs to the interval $[1, 5]$, and the sparsity of this dataset is of about
94\%, i.e. only 6\% of all possible ratings are actually known. We also
report the computation time of each algorithm (roughly estimated), which is not
an average but the total over the five folds.

\begin{table}[ht]
  \centering
\begin{tabular}{ l  l l  l  l  l  l   l  l  l  l }
\toprule
  Algorithm & \multicolumn{2}{ l }{Details}  & RMSE & Cov &  Prec & Rec & F & $\surpmax$ & $\surpavg$ & Time \\
\midrule
  \multirow{4}{*}{Parall} & \multirow{2}{*}{Sample}& $n=100$ & 1.18 &  57& 95 & 68 & 79
                          & .419& .166& 10m \\

                          & & $n=1000$ & 1.04 & 23 & 95 & 27 & 42
                          & .418 & .168 & 1h\\

                          & \multirow{2}{*}{$k$-NN}& $k=20$ & 1.00 & 31 & 97 & 38 & 54
                          & .421 & .176 & 6h\\

                          & & $k=30$ & 0.99 & 25 & 96 & 31 & 47
                          & .422 & .177  & 19h\\
\cmidrule(lr){1-3}
  \multirow{3}{*}{$k$-NN} & MSD& $k=40$ & 0.98 & 23 & 96 & 28 & 43
                          & .423 & .175 & 30s\\
                          & Cos& $k= 40$& 1.02 & 21 & 96 & 26 & 41 &
                          .426 & .180 & 30s\\
                          & Pears& $k=40$ & 1.01 & 25 & 95 & 30 & 46 &
                          .425 & .170 & 30s\\
\cmidrule(){1-3}
                   PMF & & $f = 100$ & 0.95 & 38 & 99 & 47 & 64 &  .422 & .178 & 45s\\
                   Mean &  & & 1.13 &  &  &  &  &   & & 1s\\
                   Random &  & &  1.52& 81 & 86 & 89 & 88 &  .432 & .155 & 1s\\
\bottomrule
\end{tabular}
\caption{Performances of recommendation algorithms on the Movielens-100k
  dataset.}
\label{TAB:parall_performances_comparison}
\end{table}

We have have considered various alternatives for our analogical
recommender. In fact, when trying to predict a single rating $\rui$, looking
for \textbf{all} the $3$-tuples of users in $U_i$ as described in Algorithm
\ref{ALGO:analogical_recommendation} is simply impractical\footnote{Remember
that $U_i$ is the set of user that have rated item $i$.}: there are often too
many users in $U_i$. As the complexity of this search is in $\mathcal{O}(\mid
U_i\mid^3)$ and as $\mid U_i\mid$ can be quite large for some items, strictly
sticking to Algorithm \ref{ALGO:analogical_recommendation} would lead to
even worse computation times. We thus have chosen two different strategies:
\begin{itemize}
  \item The first one is to randomly sample a $3$-tuple in $U_i^3$ $n$ times.
    As in the strict version of Algorithm \ref{ALGO:analogical_recommendation}
    the prediction is an average from all the $3$-tuples in $U_i^3$, choosing a
    large value of $n$ should lead to a fairly good estimate. We have reported
    the results for $n = 100$ and $n = 1000$. Note though that $\mid U_i
    \mid^3$ is usually much, much larger than 1000, so we only have very rough
    estimates here.
  \item The second one is to consider the $3$-tuples $(a, b, c)$ in the
    neighborhood of $u$, using the assumption that the neighbors of $u$ are
    probably more reliable to yield a prediction where $u$ is involved. We have
    used the MSD metric to compute neighborhood, and we have considered various
    sizes of neighborhood, namely $k = 20$  and $k = 30$. Unfortunately, values
    of $k$ greater than $30$ lead to basically never-ending algorithms.
\end{itemize}


\paragraph{Analysis\\}

Out of all the algorithm, the PMF algorithm is by far the most
accurate, with and RMSE of 0.95. Matrix factorization  models were extremely popular during the
Netflix competition precisely because of their high accuracy. Note however that
even if the matrix factorization algorithms cannot be overlooked when it comes
to performance comparison, their philosophy remains quite different than that
of other classical neighborhood approaches. They tend to model data structures
at a high-level of abstraction, while neighborhood methods tend to model local
features of the data. As such, it makes more sense to compare the performances
of analogical recommenders to those of neighborhood-based techniques, rather
than use matrix factorization-based models as a baseline.

As expected, the RMSE of the
Parall Sample algorithm gets better while $n$ grows, but still
remains quite far from that of the $k$-NN algorithms. By paying attention to
the Parall $k$-NN algorithms, we see that looking for the
three-tuples in a the neighborhood of the users significantly improves the
accuracy of our analogy-based algorithms. Their RMSE is better than that of the
neighborhood approaches when using cosine and Pearson similarity, and it may
expected that using a neighborhood size of $k=40$ would lead the an RMSE close
to that of the $k$-NN algorithm with MSD similarity.

Let us now consider the coverage measure. At first sight, it may appear that
the Random and Parall Sample 100 algorithm have the best recall values. But this
would be missing a very important detail: any measure that evaluates the
fitness of the recommendation strategy $S$ (such as the coverage) greatly
depends \textbf{also} on the fitness of the prediction algorithm $A$, simply
because $S$ highly depends on $A$. Therefore, the coverage of the Random
algorithm cannot be taken very seriously, because its accuracy (RMSE) is
disastrous: it's only by chance that some items were estimated with a rating
greater than $4$, and then were recommended. The same goes for the Parall
Sample 100 algorithm, which has a quite bad accuracy.  Actually, the most
reasonable choice would probably the PMF algorithm again, which has the best
RMSE and very decent coverage. With respect to coverage, our other
analogy-based algorithms yield comparable performances to the
neighborhood-based methods, with a slight advantage for analogy-based methods.

As for precision, recall and F-measure, we somehow have the same situation: the
Random algorithm seems to be the best one, but this need to be taken very
carefully. Here again, the PMF algorithm seems to yield the best trade-off.
Comparatively, analogy-based algorithms have a slightly better F-measure than
the neighborhood approaches. This may describe the fact that our algorithms
tend to yield more diverse recommendations, as would also suggest the results
on the coverage.

The surprise measures are a bit more tricky to analyse. Looking at $\surpmax$,
which assess the least surprising recommendation (average over all users), we
see that the analogy-based algorithms tend to perform better, but the
$\surpavg$ measure counterbalances this observation. Undoubtely, the surprise
associated to a recommendation remains an extremely tricky concept to grasp and
to model, and the measures presented here only allow to assess a very rough
aspect.

As a side note, we have only reported the RMSE for the Mean algorithm, because
this algorithm always outputs a prediction of $\predrui = \mu = 3.53$ which is
lower than $4$, the threshold we chose for our recommendation strategy $S$.
Therefore, not a single item is recommended with this algorithm, and the
precision, recall, etc. are not defined (or simply 0).

We are now led to computation time... The nightmare of every analogical
proportion-based algorithm. All of the other algorithms can manage through the
five-folds cross-validation procedure in less than a minute, but our
analogy-based algorithms need hours to yield only decent performances. This
issue is linked to the cubic complexity of the analogy-based learners, which
cannot cope with big amounts of data. For now, this limitation prevents
analogy-based learners to be relevent in real-world applications such as
recommender systems, where computation time is one of the most decisive factor.

Nonetheless, it is still possible to design analogy-inspired solutions for
recommendation. In the next section, we describe other algorithms that rely on
the concept of \textbf{clones} and the generalize the classical neighborhood
approach.

\section{A clone-based view of analogical recommendation}

Considering analogies between four users has shown to be computationally
intensive, thus not really suitable for recommendation purposes, where time is
a highly critical dimension. Yet, other forms of analogy can be addressed in
the recommendation task, based on the observation that some users may be more
inclined to give good (or bad) ratings than others. Indeed, ratings are in no
way absolute and greatly depend on the subjective appreciation each user has
about the rating scale. In the $[1, 5]$ scale for example, two users $u$ and
$v$ might semantically agree on an item $i$ describing it as $bad$, but there
is a chance that this agreement is not perfectly reflected in the ratings: $u$
might have rated $i$ with $r_{ui} = 1$ and $v$ with $r_{vi} = 3$, simply
because from $v$' point of view $3$ is a \textit{bad} rating, while for $u$ a
rating of $3$ would simply mean \textit{decent} or \textit{good enough}.  In
the following, we refer such users that \textit{semantically} agree on their
common items (but not necessarily \textit{numerically}) as \textit{clones}, as
illustrated in Figure \ref{FIG:alice_and_bob_clones}. Please note that the word
$clone$ is not used here to mean \textit{strictly identical}, but rather in the
sense that two clones are two users following parallel paths.

\begin{figure}[!h]
\centering
\includegraphics[width=4in]{figures/clones.pdf}
\caption{Bob is a perfect clone of Alice, and Alice is a perfect clone of Bob.}
\label{FIG:alice_and_bob_clones}
\end{figure}

It is obvious that in collaborative filtering, clones are of great interest
when it comes to predict a user's ratings, and yet the information they provide
is often discarded. Indeed, in Figure \ref{FIG:alice_and_bob_clones}, Alice and
Bob would definitly not be considered as neighbors, so Bob would not be used to
predict Alice's ratings, and Alice would not be used to predict Bob's ratings.
The principle underlying the analogical clone-based view is the following: for
predicting a missing rating for $u$, we not only look at its nearest neighbors
but also to those $v$ whose rating are such that $r_{ui} = r_{vi} + t_{vu}$
where $t_{vu}$ is a more or less constant \textbf{correction term} that can be
either positive or negative. This correction term is the difference between
Bob's ratings and those of Alice. When two users $u$ and $v$ are clones, we can
come back to an analogical proportion-based viewpoint by noticing that we have:
$$r_{ui} : r_{vi} :: r_{uj} : r_{vj},~~ r_{uj} : r_{vj} :: r_{uk} : r_{vk},
\dots,$$
where $i, j, k, \cdots$ are the common items of $u$ and $v$. The algorithms we
will derive will however be much more efficient than those of the previous
section, because they will not rely on an extensive search of 3-tuples of
users.


\subsection{Two new analogical algorithms}

In the following, $C_i(u)$ will denote as the set of users that are clones of
$u$ and that have rated item $i$. From the previous unformal definitions, one
can easily derive a very general collaborative filtering framework for
predicting a user's rating by taking into account its clones: $$\predrui =
\aggr{v \in C_i(u)}(r_{vi} + t_{vu}),$$
where $t_{vu}$ is a \textit{correction term} that we need to add to $v$'s
ratings so that they correspond to those of $u$. We clearly have a
generalization of the neighborhood approach defined in Section \ref{TODO},
which we could write as:
$$\predrui = \aggr{\begin{subarray}{l}v \in C_i(u),\\ t_{vu} = 0\end{subarray}}(r_{vi} + t_{vu}).$$

Following this general framework, one can construct a great variety of
algorithms with various level of complexity. In the next subsections, we
propose a very straightforward algorithm, and a more efficient one.

\subsubsection{A straightforward prediction algorithm}
\label{STRAIGHTFORWARD}

We will first introduce the notion of $t$-clone. In its most simple form, a
user $v$ can be considered to be a $t$-clone of $u$ if the ratings of $v$
exactly differ from those of $u$ from a constant $t$:
$$
t\text{-}C(u) \eqdef \Set{v \in U | \forall i \in I_{uv},~ r_{ui} = r_{vi} + t}.
$$
From then on, computing $\predrui$ amounts to finding all the users $v$ that
satisfy this criteria, and computing an aggregation of their rating for $i$,
which can simply be an average. We implemented this basic algorithm described
by algorithm \ref{ALGO:bruteforce}, and referred to as \textit{brute-force}.

 \begin{algorithm}[!ht]
   \caption{A brute-force algorithm for clone-based recommendation.}
       \label{ALGO:bruteforce}
       \begin{algorithmic}

      \STATE {\bf Input}: A set of known ratings $R$, a user $u$, an item
      $i$ such that $r_{ui} \notin R$.
      \STATE {\bf Output}: $\hat{r}_{ui}$, an estimation of $r_{ui}$

      \STATE {\bf Init}:
      \STATE $C = \varnothing$ \quad \quad // list of candidate ratings
      \FORALL{ users $v \in U_i$}
        \FORALL{$t$}
          \IF{$v \in t\text{-Clones}(u)$}
          \STATE $C \gets C \cup \{r_{vi} + t\}$ \quad // add x as a candidate rating
          \ENDIF
        \ENDFOR
	    \ENDFOR
    \STATE $\hat{r}_{ui} = \aggr{c \in C} c$
\end{algorithmic}
\end{algorithm}

Of course, one may want to relax the definition of a $t$-clone, as the current
one is too strict and only very few users will satisfy this criteria. In our
implementation, we chose the following condition:
$$
t\text{-}C(u) \eqdef \Set{v \in U |  \sum_{i \in I_{uv}} |(r_{ui} - r_{vi}) -
t| \leq \mid I_{uv}\mid},$$
which amounts to accept $v$ as a $t$-clone of $u$ if on average, the
difference $|r_{ui} - r_{vi}|$ is equal to $t$ with a margin of $1$.
The values of $t$ clearly depend on the rating scale. The datasets on which we
tested our algorithms use the $[1, 5]$ interval, so possible values for $t$
that were have considered are integer values in $[-4, 4]$.

This is obviously a very rough algorithm, to which one could point out numerous
flaws. The first obvious one is its time complexity which is very high, but the
purpose of this brute-force algorithm is simply to show that even such a basic
clone-based approach can lead to better results than a basic neighborhood
method, as we will see later.

\subsubsection{Modeling clones with the similarity measure}
Another option to consider clones is to use the well known neighborhood-based
formula, and capture their effect inside an appropriate similarity measure.
Recall that the general neighborhood formula is as follows:

$$\predrui = \frac{\sum\limits_{v \in N_i^k(u)} r_{vi} \cdot \ssim(u, v)}
{\sum\limits_{v \in N_i^k(u)}\ssim(u, v)}.$$

We have seen that this formula is commonly used with classical similarity
metrics such as Pearson similarity, cosine similarity, or inverse of MSD.
However, these similarities are not plainly satisfactory when it comes to
clones. Indeed with these metrics, two users are considered to be close if
their common ratings are often the same, but two perfect clones $u$ and $v$
with a significant correction term $t_{vu}$ would be considered as far from
each other, thus involving a loss of information.

We propose the following simple choice of metric to measure how two users
relate as clones:
$$\clonedist(u, v) \eqdef  \frac{1}{\mid I_{uv} \mid} \cdot
\sum\limits_{i \in I_{uv}} \left[(r_{ui} - r_{vi}) - \mu_{uv}\right]^2$$
where $\mu_{vu}$ is the mean difference between ratings of $u$ and $v$:
$$\mu_{uv} \eqdef \frac{1}{\mid I_{uv}\mid}\sum_{i \in I_{uv}} (r_{ui} -
r_{vi}).$$

We can understand this distance in two ways:
\begin{itemize}
\item it can be regarded as the variance of the difference of ratings between
  $u$ and $v$,
\item or it can be regarded as a simple MSD measure (defined in Section
  \ref{TODO} to which the mean difference of ratings between $u$ and $v$ has
    been subtracted.
  \end{itemize}

As our measure $\clonedist$ is a distance, it is necessary to transform it into
a similarity measure. Common choice is to take its inverse (while accounting
for zero division):
$$\clonesim(u, v) = \frac{1}{\clonedist(u, v) + 1}.$$

\noindent
Once we know how to find the clones of a user, it is a simple matter to output
a prediction using the classical neighborhood approach:
$$\predrui = \frac{\sum_{v \in N_i^k(u)} (r_{vi} + \mu_{uv}) \cdot \clonesim(u,
v)}{\sum_{v \in N_i^k(u)} \clonesim(u, v)}.$$

This algorithm will be referred to as Clone. For the sake of completeness, we
also tried the same formula but with a more basic similarity metric that does
not care about clones: MSD.

\subsection{Current advances in neighborhood-based techniques}

What we have seen so far in terms of neighborhood methods are the rough, basic
techniques that have existed for a long time. Actually, more sophisticated
approaches have been developed, in particular during the Netflix competition.
The one we will describe here has been popularized in \cite{BelKorSIGKDD2007},
and makes use of \textbf{baseline predictors}:
$$\predrui = b_{ui} + \frac{\sum\limits_{v \in N_i^k(u)} (r_{vi} - b_{vi})
\cdot \ssim(u, v)} {\sum\limits_{v \in N_i^k(u)}\ssim(u, v)}.$$
where $b_{ui}$ is a baseline (or bias) related to user $u$ and item $i$. Its
expression is $b_{ui} = \mu + b_u + b_i$, where $b_u$ is supposed to model how
$u$ tends to give higher (or lower) ratings than the average of all ratings
$\mu$, and $b_i$ is supposed to model how $i$ tends to be rated higher or lower
than $\mu$. For example, if the mean of all rating is $\mu = 3$, and the
ratings of a user are $(2, 2, 1)$, its bias $b_u$ would be close to $-1$.

Baselines are computed by solving a regularized least squares problem:
$$\min_{b_u, b_i} \sum_{r_{ui} \in R} \left[r_{ui} - (\mu + b_u + b_i)\right]^2
+ \lambda \left(b_u^2 + b_i^2 \right).$$
which can be achieved efficiently by stochastic gradient descent, or
alternating least squares. The regularization terms are here to avoid
overfitting: they allow to give more confidence to biases that are computed on
a high number of ratings. In our previous example, the user had only rated $3$
items so we cannot say reliably say that its real bias is really close $-1$.

As a side note, notice that this optimization problem has the same look as the
one we used for the PMF algorithm of Section \ref{TODO}. In fact, the use of
baselines can be easily combined with the matrix factorization model, leading
to the following optimization problem:
$$\min_{b_u, b_i, p_u, q_i} \sum_{r_{ui} \in R} \left[r_{ui} - \predrui\right]^2
+ \lambda \left(\norm{}{q_i}^2 + \norm{}{p_u}^2 + b_u^2 + b_i^2 \right),$$
with $\predrui = \mu + b_u + b_i + q_i^tp_u$. This prediction algorithm is
called SVD \cite{TODO}, and is nothing but the PMF algorithm with user and
items biases. Its extension to handling implicit ratings is called SVD++, but
we will not describe it here.

In their work, the author have used this particular similarity metric, that is
in perfect accordance with their prediction formula:
$$\text{sim}(u, v) = \frac
{ \sum\limits_{i \in I_{uv}} (r_{ui} -  b_{ui}) \cdot (r_{vi} - b_{vi})}
{\sqrt{\sum\limits_{i \in I_{uv}} (r_{ui} -  b_{ui})^2} \cdot
\sqrt{\sum\limits_{i \in I_{uv}} (r_{vi} -  b_{vi})^2}}.$$

It is simply a Pearson correlation coefficient, except that instead of
centering ratings by their means, they are centered with the baseline
predictors. This simingly simple tweaks actaually has various consequences that
are very interesting. An intuitive and illuminating way to look at this
algorithm as a whole is to see that it conceptually follows these steps:
\begin{enumerate}
  \item Compute $R'$, the set of all ratings normalized by the corresponding
    baselines: $r'_{ui} = r_{ui} - b_{ui}$.  $R'$ can be regarded as the set
    where all ratings are given from the same frame of reference, thus
    discarding any bias coming from the users or from the items. In $R'$
    ratings can then be considered as absolute, in the sens that they are not
    spoiled by the users moods or the items inherent popularity.
  \item Using $R'$, compute similarities between users using the cosine
    similarity (the cosine similarity is the same as the Pearson correlation
    coefficient, except that quantities are not centered).
  \item Output a prediction using the basic neighborhood formula. As this
    prediction belongs to the same space of $R'$ where ratings have no bias, it
    needs to be transposed back to the space of $R$ for performance evaluation
    purposes. This is why $b_{ui}$ and $b_{vi}$ are added (or subtracted) in
    the prediction formula of $\predrui$.
\end{enumerate}
\noindent
In what follows, this algorithm will be referred to as $\knns$.

It is very clear that the use of the baseline predictors and the use of
clone-based recommendation are motivated by the same reasons: they both come
from the fact that users (and items) tend to interpret differently the rating
scale.  This means that $\knns$ implicitly takes the idea of clones into
account, and thus a form of analogical reasoning.  Differences and resemblances
of these two approaches will be discussed in the next section.

\subsection{Experiments and discussion}
\label{expeDiscuss}

To assess the suitability of our clone-based view of recommendation, we have
evaluated the accuracy of the brute-force and Clone algorithms, and compared
them to the previously mentioned approaches: the basic neighborhood method
($k$-NN), and
the neighborhood method taking into account user and item biases ($\knns$). For
reasons that will become clear at the end of this discussion, we also evluated
the Bsl algorithm, whose prediction is simply the baseline predictor, i.e.
$\predrui = b_{ui}$. The evaluation protocol is the same as that of Section
\ref{TODO}, i.e. results are averaged over a 5-folds cross-validation
procedure. In addition to the Movielens-100k dataset presenter earlier, we also
used the Movielens-1M dataset containing 1 million ratings with 6000 users and
4000 movies.  For each of these algorithms, the number of neighbors or clones
used to output a prediction is $k = 40$, except for the brute-force algorithm
where the number of clones can not be controlled. The RMSE and MAE of the
algorithms are reported on Table
\ref{TAB:clone_based_rmse_mae}.

\begin{table}[ht]
  \centering
  \begin{tabular}{ l  l  l  l  l l l }
\toprule
    & & &  \multicolumn{2}{ c }{ML-100k}  & \multicolumn{2}{ c }{ML-1M} \\
  \cmidrule(lr){4-5}
  \cmidrule(lr){6-7}
    Algorithm& Details  & & RMSE & MAE  & RMSE & MAE \\
\midrule
    $k$-NN & MSD & $k=40$&  .979 & .773 & .921 & .725\\
    Brute-Force &  & & .948 & .737 & & \\
  \cmidrule(lr){1-2}
    \multirow{2}{*}{Clone} & \clonesim & $k=40$& .936 & .733 & .899& .705\\
                           & MSD & $k=40$ &  .931 &  .732 & .897 &  .707\\
  \cmidrule(lr){1-2}
    $\knns$ & & $k=40$ & .921 & .721 & .869& .680\\
    Bsl & &  & .945 & .749 & .909& .719\\
\bottomrule
\end{tabular}
  \caption{RMSE and MAE of our clone-based algorithms on the movielens 100k and
  1M datasets.}
  \label{TAB:clone_based_rmse_mae}
\end{table}

It is very clear that even a very straightforward approach of the clone-based
recommendation principle significantly outperforms the most basic $k$-NN
algorithm, and thus validates the need to take into account biases between
users. The brute-force is however a lot heavier to compute, and thus not very
suitable for real world recommendation purposes (its performances on the
Movielens-1M dataset simply could not be computed). The two other clone-based
algorithms however, have the exact same complexity of any $k$-NN-based
algorithm which is a significant improvement from the algorithm described in
section \ref{TODO}.

Our two Clone algorithm output (almost) exactly the same
accuracies. This may seem a bit surprising, because the Clone MSD algorithm
does not take into account clones in the similarity measure, and we would
expect it to yield a lower accuracy. But this result still gives a further
reason to consider clones as useful predictors: the only difference between the
Clone MSD algorithm and the $k$-NN MSD algorithm (whose accuracy is much worse)
is that in the prediction of Clone MSD, the mean differences $\mu_{uv}$ between
the ratings of $u$ and its neighbors are taken into account. If this
(seemingly) simple change can make such a signiciant difference in the
accuracies, this means that the way users relate as clones is an important
feature of the dataset and should not be discarded.

Performances of the Clone algorithms are close to those of the state of the
art $\knns$ algorithm, yet the difference is more striking on the Movielens-1M
dataset. It is however important to understand that these algorithms differ on
the following points:
\begin{itemize}
\item The Clone algorithms do not address item bias, which is a significant
  drawback. It may not be unreasonable to believe that incorporating item bias
  in the prediction would lead to better results.
\item There is a subtle yet meaningful difference of interpretation between the
  biases induced by both algorithms. In the Clone algorithm, biases are all
  pairwise, meaning that they involve two users, and they are computed on items
  that both users have rated. As for the $\knns$ algorithm, there is no such
  thing as a pairwise bias. Bias for a given user is computed using only its
  own ratings, and is a result of a global optimization problem involving the
  global mean of all ratings, which means that every single rating in $R$ has
  an impact on the bias. As baselines are computed on the whole training set,
    they tend to capture most of the noise when the training set gets bigger.
    This may explain why the difference between the Clone algorithms and
    $\knns$ is more striking on the Movielens-1M dataset.
\end{itemize}

As previously mentioned in Section \ref{TODO}, it is recommended to perform a
shrinkage on the similarity measure of algorithm $\knns$, in order to take into
account the number of common items between two users: the more items they
share, the more confident we are when computing their similarity
\cite{KorACM2010}. Such techniques can further improve both RMSE and MAE of the
algorithm.  Similarly, in the clone-based approach, it might be of interest to
discount clones that rely on a too small number of common items.

Let us focus now on the Bsl predictor. Bsl outperforms by far the RMSE of the
$k$-NN algorithm, and in fact if we go back to table
\ref{TAB:parall_performances_comparison}, we see that it outperforms also the
algortihms mentioned there. Yet, this algorithm is unable to output
personalized recommendation: all recommendations would be the same for
\textbf{any} user! This is simply due to the fact that in the prediction
$\predrui = \mu  + b_u + b_i$, the only user-dependent factor is the term
$b_u$, which is the \textbf{same} in all the predictions regardless of the
item. The point of this remark is to exhibit one of the drawbacks of RMSE that
we already mentioned before: RMSE is not an appropriate measure when it comes
to assess user-system interactions.

Indeed, what ultimately matters is how well a recommender system can
\textbf{rank} the preferences of a user for all the items. Also, rather than
defining clones with the general numerical criteria ($r_{ui} = r_{vi} +
t_{vu}$), we may instead consider that two users are clones if they order their
common items in the same way. Back to Figure \ref{FIG:alice_and_bob_clones}, we
see that Alice and Bob are perfect clones and we actually do not need to know
the real rating values: only the order matters. The goal of the next section is
precisely to investigate this ordinal view of ratings in our clone-based
framework.

\subsection{Towards an ordinal view of ratings}

We here investigate if we can devise a counterpart of the numerical clone-based
approach, which would be compatible with an ordinal view of the ratings.
Indeed, an extreme way for unbiasing and comparing two sets of ratings is to
forget about their numerical values, and only consider their rankings. The
idea of viewing ratings in a ordinal manner is not new, and has been advocated
for example in \cite{KorSillRECSYS11}.  In this section, we discuss an ordinal
counterpart of the analogical approach previously presented.  Analogical
reasoning with ordinal data has first been proposed in \cite{MicBarCAP09}, yet
with a different concern.

\subsubsection{An algorithm for rank prediction}
Indeed the idea that ``\textit{the rating of user $u$ for item $i$ is to the
rating of user $v$ for item $i$ as the rating of user $u$ for item $j$ is to
the rating of user $v$ for item $j$}’’ may be understood  as well in an ordinal
manner. This leads to state that ``\textit{the relative ranking of item $i$
  among the ratings given by user $u$  is to the relative ranking of item $i$
  among the ratings given by user $v$ as the relative ranking of item $j$ among
  the ratings given by user $u$  is to the relative ranking of item $j$ among
the ratings given by user $v$}.

This means that we need to compare the rankings given by two users $u$ and $v$
on their common items. In the following, $\rho_{ui}$ denotes the relative
ranking of item $i$ out of all the items rated by $u$. Our goal is to estimate
all values of $\rho_{ui}$, for any user and any item. The main steps of a
possible algorithm would be as follows:
\begin{enumerate}
  \item Compute similarities between users, based on their rankings. A very
    popular similarity ranking measure is the Spearman's rank correlation
    coefficient (or Spearman's rho), that will be described later.
  \item Compute an estimated rank $\hat{\rho}_{ui}$ as an aggregation of all the
    rankings $\rho_{vi}$ extracted from the $k$ nearest neighbors (using
    Spearman's rho as similarity):
    $$\hat{\rho}_{ui} = \frac{\sum\limits_{v \in N_i^k(u)} \rho_{vi} \cdot
    \ssim(u, v)}{\sum\limits_{v \in N_i^k(u)} \ssim(u, v)}.$$
\end{enumerate}

This is obviously very similar to the neighborhood approach described in
section \ref{MODELING_CLONES}, but instead of predicting a rating, we output a
predicted rank. This approach is denoted as \textit{RankAnlg}.

The computation of Spearman's rho takes into account the relative rankings of
the ratings, and is mmoputed as follows:
$$
\text{Spearman}(u, v) \eqdef \frac{\mathop{\text{Cov}}\limits_{i \in I_{uv}}(\rho_{ui},
\rho_{vi})}{\mathop{\text{std}}\limits_{i \in I_{uv}}(\rho_{ui}) \cdot
\mathop{\text{std}}\limits_{i \in
I_{uv}}(\rho_{vi})},
$$
where Cov naturally stands for covariance and std is the standard deviation.
Spearman's rho is usually defined as a statistic for random variables, and we
here have translated it as a realization of this statistic in our
recommendation setting. A common issue when dealing with rankings is that there
may be ties in the data. Consider for example user $u$ from Table
\ref{TAB:rankings}, who's ratings are $1, 1, 2, 3, 3, 3$ for items $i_1, i_2,
i_3, i_4, i_5$ and $i_6$.
\begin{table}[h!]
\centering
  \begin{tabular}{ l  l  l  l  l  l  l }
\toprule
    & $i_1$ & $i_2$ & $i_3$ & $i_4$ & $i_5$ & $i_6$\\
  \midrule
    $r_{ui}$ & 1 & 1 & 2 & 3 & 3 & 3\\
    Raw rankings & 1 & 2 & 3 & 4 & 5 & 6 \\
    Processed rankings $\rho_{ui}$ & 1.5 & 1.5 & 3 & 5 & 5 & 5 \\
\bottomrule
\end{tabular}
  \caption{Values of $\rho_{ui}$ for the user $u$.}
\label{TAB:rankings}
\end{table}
As $i_1$ and $i_2$ have the same ratings, there is no reason to rank $i_1$
before $i_2$, or $i_2$ before $i_1$. The same goes for $i_4, i_5$ and $i_6$,
and this is why the first ranking option (raw rankings) is not a suitable one.
Instead, it is common to consider that the ranking of a given item is the mean
of the raw rankings for the items that share the same rating value. In our case,
$\rho_{u{i_1}} = \rho_{u{i_2}} = \frac{1 + 2}{2}$, and $ \rho_{u{i_4}} =
\rho_{u{i_5}} =  \rho_{u{i_6}} = \frac{4 + 5 + 6}{3}$.
This way to
proceed leads to a better estimation of Spearman's rho, and is equivalent to
averaging over all possible permutations between the raw ranks of items with
equal ratings.



\subsubsection{Experiments}

We evaluated the performance of our algorithm and compared it to other
previously described approaches, using the exact same evaluation protocol as in
the previous sections. The Movielens-1M dataset was not benchmarked, because
the computation of Spearman'rho was too computationally intensive.

RMSE and MAE are good measure for evaluation rating prediction accuracy, but
are not suitable when it comes to evaluate rankings. A better measure is the
Fraction of Concordant Pairs (FCP), which evaluates the probability that given any two
items $i$ and $j$ rated by any user $u$, the system has correctly estimated
whether $u$ prefers $i$ over $j$, or the reverse. To compute the FCP, we need to
use intermediate measures. Following the notation of \ref{TODO}, $c_u$ defines the
number of concordant pairs for user $u$, and $d_u$ is the number of discordant
pairs. The FCP is then computed over all users as the proportion of concordant
pairs.

\begin{align*}
  c_u &\eqdef \Set{(i, j) \in I^2 \quad | \quad \predrui > \predruj \text{ and
  } r_{ui} > r_{uj}},\\
  d_u &\eqdef \Set{(i, j) \in I^2 \quad | \quad \predrui \geq \predruj \text{
    and } r_{ui} < r_{uj}},\\
  \FCP &\eqdef \frac{\sum\limits_{u \in U} c_u}{\sum\limits_{u \in U} c_u +
  \sum\limits_{u \in U} d_u}.
\end{align*}
Here, $\predrui$ here may represent either a rating prediction or a ranking
prediction $\hat{\rho_{ui}}$.

Results are reported in table \ref{table:res100kRank}.
\begin{table}[!ht]
\centering
\begin{tabular}{ l l  l l }
\toprule
     & RankAnlg &  $k$-NN & $\knns$\\
\midrule
\FCP  &  .7063   & .7096 &  .7163   \\
\bottomrule
\end{tabular}
\caption{\FCP of our rank prediction algorithm on the Movielens-100k
  dataset.}
\label{table:res100kRank}
\end{table}
Unfortunately, even a basic algorithm such as $k$-NN that was not designed for ranking
prediction performs better in terms of FCP. To explain this difference, one may
look at the distribution of average support over all the predictions, as shown
on figure \ref{FIG:support_spearman_MSD}. Between
two users $u$ and $v$, the support is defined as the number of common items
$|I_{uv}|$ which was used to compute the similarity between $u$ and $v$. For
a given prediction $\predrui$, the average support is the average of all the
supports $|I_{uv}|$ over all users $v \in N_i^k(u)$.

\begin{figure}[!h]
\centering
\includegraphics[width=2.5in]{figures/support.pdf}
\caption{Distribution of average support for Spearman's rho and MSD.}
\label{FIG:support_spearman_MSD}
\end{figure}

The use Spearman's rho tends to provide with neighbors that have smaller
support, thus leading to a less significant and less accurate estimation of the
neighborhood, which may explain the differences in performance.

This closes for now our contributions to analogical recommendation. We will
come back to it in the conclusion of this chapter, and we will now focus on a
somewhat unrelated problem: analogical proportion mining in databases. As we
will see though, analogical recommendation can in fact be view as both a motivation
and as a purpose for analogical proportion mining.

\section{Mining analogical proportions}

Let us recap a bit the first section of this chapter, where we desribed our
first attempt to design an analogical-based recommendation algorithm. As it is
a direct application of the analogical inference principle, our algorithm
suffers from the same drawbacks as the conservative and extended learners
introduced in Chapter \ref{TODO}: the time complexity is cubic, which makes it
impossible to use in actual systems and hard to evaluate accurately. Also, the
performances of this algorithm were quite modest, even in its most elaborated
forms.

Our analogical inference principle states that if four users are in analogy,
then their ratings for an item $i$ that $d$ has not rated should also be in
analogy:
$$
\infer{ r_{ai} : r_{bi} :: r_{ci} : r_{di} ~~ \forall i \in I_a \cap I_b \cap
I_c, \text{ and } i\notin I_d }{r_{aj} : r_{bj} :: r_{cj} : r_{dj} ~~ \forall j
\in I_a \cap I_b \cap I_c \cap  I_d}
$$
But we have so far neglected a central question: do such proportions actually
exist in our database? Can we find enough 4-tuples of users such that their
common ratings are in proportion? How good are these proportions? We already
acknowledged that absolutely perfect proportions are actually hard to find, and
this is why we have relaxed the condition for an analogy to hold in Algorithm
\ref{ALGO:analogical_recommendation}.

In this section, we will design an algorithm that is able to extract all the
analogical proportions underlying a database satisfying some given quality
criteria. This will allow us to answer the previous questions: how many
proportion are there, and how \textit{good} are they? We will apply this
algorithm to the Movielens database, and see how this relates to ou
recommendation task.  We will here look for analogies between items (movies in
our case) instead of looking for analogies between users, because the
interpretation of a movie-based proportion is actually easier, and because the
parallel with association rules (described below) will be more natural. Both
problems are symmetric though, and our solution can be adapted to user
proportions in a straighforward manner.

Because our method for mining analogical proportion is inspired from the mining
of association rules, we first review this topic in the next nection.

\subsection{Association rules}

Association rules are pieces of information that one can extract from a
database, and that reveal dependencies between items. Recommendation is one of
the principal application of association rules. Strating from the association
rule $i \implies j$, which means that users that buy $i$ tend to buy $j$ with
high probability, a recommendation system can suggest $j$ as soon as we bought
$i$ but not yet $j$. A well-known example of association rule is the famous
$\text{beer} \implies \text{diapers}$ association, who was revealed after
mining association rules in a supermarket selling history, suggesting that
people tend to by beer and diapers altogether. We now formally introduce the
problem of association rule mining.

Let $I = \Set{i_1, i_2, ..., i_m}$ be a set of $m$ items, and let $T =
\Set{t_1, t_2, ..., t_n}$ be a multiset of transactions, where each transaction
is a subset of $I$: $t_i \subseteq I~\forall i$. A transaction simply is a set of items
that are purchased together. An association rule is expressed in the form $ X
\implies
Y$, where $ X \subset I, ~ Y \subset I$, and $X \cap Y = \varnothing$. $X$ and
$ Y $ are sets of items that we will call \textbf {itemsets}, and usually, $Y$
is restricted to a single item. We define the \textbf{support} $\supp (X) $ of
an itemset $X$ as the proportion of transactions that contain it:
$$\supp(X) \eqdef \frac{\mid \Set{t \in T | X \subseteq t}\mid}{\mid T \mid}.$$
Sometimes, the support is not defined as a proportion but rather as the
absolute number $\mid \Set{t \in T | X \subseteq t}\mid$, but it only makes
sense to consider it with respect to the total number of transactions $\mid T
\mid$. Various
measures can be used to evaluate the quality of an association rule, such as
the \textbf{confidence } which can be expressed as:
$$\text{Conf}(X \implies Y)~\eqdef~\frac{\supp (X \cup Y)}{\supp (X)}.$$
As could be naturally expected, $\text{Conf}(X \implies Y)$ is equal to $1$
when the items of $X$ and $Y$ are systematically bought together and decreases
if the set $X$ is sometimes found in a transaction that does not contain $Y$.

The mining of association rules is a popular research topic, and has been
extensively studied. In a perfect world with unlimited computational resources,
a naive association rules mining algorithm would be as follows:
\begin{enumerate}
\item First, generate the power set of $I$, i.e. the set of all subsets of $I$
  that we denote $2^I$.  We know that the elements in $2^I$ are partially
  ordered with respect to inclusion, and $2^I$ can be represented as a lattice
  where the join is the union and the meet is the intersection.
\item Let $I_S$ be any of the $2^m$ itemsets in $2^I$. Then for each $I_S$,
  compute all partitions $(X, Y)$ of $I_S$ and calculate the confidence
    associated with the rule $X \implies Y$. If the confidence is below a given
    threshold, then keep the association rule, else descard it.
\end{enumerate}

The second step remains reasonable and cannot really be optimized, but the
first one is obviously impossible in practice due to the terrific size $2^I$,
when real-world databases usually contain hundreds or thousands of items.
The most famous algorithm for association rule mining probably is the
\textbf{Apriori} algorithm introduced in \cite{AgrSriVLDB94}, that we will
briefly review. Using Apriori allows to scan the itemset lattice in an
efficient way, avoiding many useless nodes.

Ultimately, we are only interested in association rules where the support of
the involved itemsets is high. An itemset whose support is above some given
threshold $\alpha$ is called a \textbf{frequent} itemset. The downward-closure
property of support states that if $I_S$ is a frequent itemset, then all of its
subsets are also frequent itemets. For example, if the itemset
$\Set{\text{apple, banana, orange}}$ is found in more than 30 transactions,
then the three subsets $\Set{\text{apple, banana}}$, $\Set{\text{apple,
orange}}$ and $\Set{\text{banana, orange}}$ must also be found in \textbf{at
least} 30 transactions. Conversely, if the two sets $\Set{\text{kiwi, pear}}$ and
$\Set{\text{kiwi, strawberry}}$ are found in less than $30$ transactions, we
are sure that their union $\Set{\text{kiwi, pear, strawberry}}$ will also be
found in less than $30$ transactions. So if we are only interested in itemsets
whose support is higher than $30$, there is no point in building the set
$\Set{\text{kiwi, pear, strawberry}}$, or any of its supersets.

Taking advantage of this fact, a basic version of the Apriori can be described
in the following steps:
\begin{enumerate}
  \item Consider all itemsets of size $1$ whose support is above $\alpha$.
  \item By joining these $1$-itemsets, build all possible $2$-itemsets and only
    keep those whose support is above $\alpha$.
  \item By joining these $2$-itemsets, build all possible $3$-itemsets and only
    keep those whose support is above $\alpha$.
  \item Etc, etc... All the frequent $k$-itemsets are built by joining the
    frequent $k-1$ itemsets.
  \item Once all the frequent itemsets have been computed, the second step of
    the above naive algorithm is used to assess the confidence of the
    association rules.
\end{enumerate}

In the end, we are provided with a set of association rule than comply with
some quality requirements, and that give us insightful information that link
the elements of our database.

%Sur le mode des proportions analogiques, on admettra que {\it le dentifrice
%est à la brosse à dent ce que le beurre est à la biscotte}.  Dans ce
%cas, on peut penser recommander à quelqu'un ayant acheté dentifrice,
%brosse à dent et beurre, d'acheter des biscottes. Sur quelle base?  Sur la
%base que la relation liant dentifrice et brosse à dent est la même que
%celle liant beurre et biscottes.

Just like association rules, the identification of analogical proportions in a
database is an additional information source, and deserves to be addressed. We
now get to the heart of the matter.

\subsection{Looking for analogies in an incomplete database}

In order to avoid any ambiguity, we formally define our problem first. 
\subsubsection{Problem definition and links to association rules}

We will be looking for analogies between items in the Movielens database, but
our method can naturally be extended to any other database with the same
structure.  We dispose of a set of users $U$ and a set of items $I$. Each user
$u$ has rated (or purchased) a given set of movies $I_u \subseteq I$. In this
setting, each user $u$ is actually considered to be a transaction $t_i$, as
defined in the previous section. The set of all the ratings $\rui$ is denoted
$R$ and can be viewed as a sparse matrix where user are columns and items are
rows (or the reverse).  For now, we do not care about the rating values and
just focus on whether the rating exists or not (just like we only care about
the fact that an item has been bought in the case of association rules). We
will only make use of the rating values when we will evaluate how good the
proportions are, in later subsections.

We need to insist here on the fact that we are dealing with a \textbf{sparse}
database.  In the case where all values in $R$ are known, the problem of mining
analogical proportion is pretty trivial: all we need is to look for all
$4$-tuples of items, and evaluate how good the proportion $a:b::c:d$ is. This
search is in $\mid I \mid^4$ which is absolutely aweful, but there is simply no
other way around. This problem is not really interesting in the sense that
there is no need to come up with an elaborated algorithm. Its parallel in the
association rules world corresponds to the case where the each transaction
$t_i$ contains all the items ($\forall i, t_i = I$): the very idea of
association rule does not make sense anymore. The problem we propose to address
here is different: the matrix $R$ is sparse and the set of users that have
rated (purchased) the items is different each time: for all items $i$ and $j$
(or at least for most), we have $U_i \neq U_j$. We thus cannot just look at all
$4$-tuples of items and check if they are in analogy. Or more accuratly we
could, but as we are ultimately interested in good analogies (i.e. analogies
involving a sufficient number of ratings), we can make use of the
downward-closure property just like the Apriori algorithm, which will allow us
to avoid many $4$-tuples that could not lead to good analogies.

Let's now consider four items $a, b, c, d$, and our task is to find out if
these four users make up a valid analogy. For now, we do not know in which
order we need to consider them.  Just like for association rules, the notion of
support still makes sense here because these four users make up a $4$-itemset:
$$\supp(a, b, c, d) \eqdef \frac{\mid U_{abcd}\mid}{\mid U\mid},$$
where $U_{abcd}$ is the set of user that have jointly rated $a, b, c$ and $d$.
Here again, we will only be interested in proportions whose support is greater
than some threshold $\alpha$: a proportion built on only two components is a
lot less meaningful than a proportion built on dozens of components.

When given four items $a, b, c, d$, the question is know to find out which is
the proportion that actually holds. It could be $a:b::c:d$, but it could just
as well be $a:b::d:c$ or $a:c:d:b$, or any of the $24$ ($4!$) combination of
these four elements. Fortunately, we do not have to test all the $24$
orderings.  We know from Chapter \ref{TODO} that there are exactly 3 equivalent
classes of analogies, which are represented by:
\begin{itemize}
  \item $a:b::c:d$,
  \item $a:b::d:c$,
  \item $a:d::c:b$.
\end{itemize}
Thus, testing these three orderings is enough to find out about the $24$
possibles forms of analogies. To assess the quality of a proportion, we will
use a function $f$ that plays a similar role to the confidence function for
association rules. Then, it will be natural to only consider analogies that
are below some given quality threshold. Simply put, we would consider
$a:b::c:d$ as a valid analogy if $(a:b::c:d) \geq \beta$. In the end we are
left with a set of proportions that represent analogical realtions between four
items.

\subsubsection{Assessin the quality of a proportion}

We here describe various functions $f$ that can assess the quality of a
proportion. The four items $a, b, c, d$ are considered as vectors of ratings in
the space of their common users,
and we will consider two cases: that of the binary rating scale $[0, 1]$, and
that of a gradual rating scale (e.g. $[1, 5]$). Note that here, in case of the
binary rating scale, the value $1$ is associated with \textit{like} and the
value $0$ is associated with \textit{dislike}, but a value of $0$
\textbf{still} means that the user $u$ has rated the item $i$. In some
settings (e.g. with the unary rating scale), a value of $0$ can be interpreted
as the absence of rating, but this view is not compatible with our problem: if
$\rui = 0$ means $\rui \notin R$, the four items $a, b, c, d$ would only be
represented as vectors of constant value $1$, where analogies are all trivial
because their only pattern is $1:1::1:1$. Instead, when $0$ still means that
the user has rated the item, the items can be represented as boolean vectors,
which is fortunate because we know how to deal with Boolean proportions.

Naturally, the quality functions that we can define will depend on the nature
of the rating scale.

\begin{itemize}
  \item When we have a binary rating scale, the obvious choice for assessing
    the quality (or rather the \textit{badness}) of a proportion is the
    analogical dissimilarity defined in Section \ref{TODO}. For Boolean
    vectors, the analogical dissimilarity is defined as the number of
    components that need to change in order to have a perfect proportion. Note
    however, that we ultimately want to \textbf{compare} the quality of
    different proportions, and the fact is that no two $4$-tuples of items will
    have the same common users, so the item vectors of the two different
    $4$-tuples will likely have different dimensions. Therefore, it might be
    wise to consider the \textbf{relative} analogical dissimilarity, which is
    the classical AD divided by the dimension of the vectors.
  \item Another obvious quality measure in $\mathbb{B}^m$ simply is the number
    of components where a Boolean proportion perfectly holds. It is a slightly
    less conservative approach than the above one but still very similar. In
    practice, this means that the two proportions $0:1::1:0$ and $1:0::0:1$
    will be given a cost of $1$ instead of $2$. Here again, considering a
    proportion rather than an absolute number may be more meaningful, because
    of the different dimensions.
  \item When the rating scale is numerical (e;g. $[1, 5]$), the analogical dissimilarity is
    defined as $\norm{p}{(a - b) - (c - d)}$, and indicates how well the
    parallelogram $abcd$ holds. Clearly, this can also be used as a measure of
    quality of the proportion.
  \item Another option for the gradual rating scale is to use the definitions
    of fuzzy analogy of section \ref{TODO}\todo{completer}. As the quality
    each proportion is evaluated in a component-wise fashion, we can choose
    various aggregation functions to assess the overall quality: mean, max,
    min, or also compare two proportion by lexicographic order. We will give
    further details in the experiments section.
  \item Finally, by adopting a statistical point of view, we can try to
    evaluate the probability of observing the proportion $a:b::c:d$, and
    consider it as a meaningful proportion if it is unlikely that we could have
    observed this proportion by random chance alone. This method is highly
    linked to statistical test theory and will be discussed further in the
    experiments sectios.
\end{itemize}

\subsubsection{Algorithm}

Our algorithm for mining analogical proportion immitates an association rule
mining process: we preliminarly set a threshold $\alpha$ for the support, and a
quality evaluation $f$ along with a threshold $\beta$.

After having built all the $4$-itemsets whose support is above the threshold
$\alpha$ with the Apriori algorithm, we compute the quality of the proportions
associated with the three equivalent classes and keep those that satisfy our
criteria. These steps are described in Algorithm \ref{ALGO:proportion_mining}.

 \begin{algorithm}[!ht]
   \caption{Analogical proportion mining.}
       \label{ALGO:bruteforce}
       \begin{algorithmic}

      \STATE {\bf Input}: A set of known ratings $R$, a quality function $f$,
         and two threshold $\alpha$ and $\beta$.
         \STATE {\bf Output}: A set $\mathcal{P}$ of analogical proportions
         between items.
         \STATE $\mathcal{P} = \varnothing$
      \STATE Using Apriori, derive all the $4$-itemsets whose support is
         greater than $\alpha$.
         \FORALL{$(a, b, c, d)$ in the set of $4$-itemsets}
         \FORALL {prop $\in \Set{(a:b::c:d), (a:b::d:c), (a:d::c:b)}$}
         \IF{$f(\text{prop)} \geq \beta$}
         \STATE $\mathcal{P} = \mathcal{P} \cup \Set{\text{prop}}$
         \ENDIF
         \ENDFOR
         \ENDFOR
\end{algorithmic}
\end{algorithm}

In theory, it is possible to end up with two non-equivalent proportions in
$\mathcal{P}$ that still relate to the same four items, i.e. we could find in
$\mathcal{P}$ the proportion $a:b::c:d$ as well as the non-equivalent
proportion $a:b::d:c$. It should not seem natural to have these two proportions
in $\mathcal{P}$, so if this happens this is probably because the quality
fonction $f$ that is too permisive, or because the threshold $\beta$ is not
correctly tuned.

We also want to stress the point that the actual rating values are only used in
the second part of the algorithm, i.e. when we evaluate the quality of the
proportions. In the first part involving the Apriori algorithm, the only
information that matter is that a rating exists. Its value is not taken into
account.

\subsection{Experiments and discussion}

As previously indicated, we have considered the Movielens-100k dataset for our experiments: 100,000
ratings in $[1, 5]$ from $1000$ users and $1700$ movies.

For our purpose, we actually do not need to set a quality threshold $\beta$,
which would be byt the way quite arbitrary. Instead, we will only be interested
in comparing proportions. We have chosen to compare two proportions by first
computing the truth value of each of their component-wise proportions using
$A$, and then by comparing these truth values in lexicographic order. Consider
for example the two $4$-itemsets of Table \ref{TAB:lexicographic_order}, where
ratings $[1, 2, 3, 4, 5]$ have been mapped to $[0, .25, .5, .75, 1]$. We
will consider that the first $4$-tuple (on the right) is a better proportion
than the second one because once their truth values are sorted, the one of the
right comes first. Had we chosen to compare them with the mean of the truth
values, the best proportion would have been the second one.

\begin{table}[h!]
\centering
  \begin{tabular}{ c c  c  c  c }
\toprule
 $i_1 $ & $i_2$ & $i_3$ & $i_4$ & $A$\\
  \midrule
    0 & 1 & 0 & 1 & \textbf{1} \\
    0 & .5 & .25 & .75 & \textbf{1} \\
    1 & .5 & 1 & .25 & \textbf{.75} \\
    0 & .5 & 1 & .25 & \textbf{.25} \\
\bottomrule
\end{tabular}
\quad
  \begin{tabular}{ c c  c  c  c }
\toprule
 $i_1 $ & $i_2$ & $i_3$ & $i_4$ & $A$\\
  \midrule
    0 & 0 & 0 & 0 & \textbf{1} \\
    0 & .25 & 1 & .75 & \textbf{.75} \\
    1 & .5 & 1 & .25 & \textbf{.75} \\
    1 & 1 & 1 & .75 & \textbf{.75} \\
\bottomrule
\end{tabular}

\caption{Two $4$-itemsets and their related proportions, with corresponding
  truth value ($A$).}
\label{TAB:lexicographic_order}
\end{table}

Using this comparison procedure, we have looked for the 10 best proportions
with a minimum support of $200$ common users. The results are reported on Table
\ref{TAB:best_prop_num_basic_200}.
\begin{table}[h!]
\centering
  \begin{tabular}{l l  l  l l}
\toprule
    &\multicolumn{1}{c}{$i_1$}  & \multicolumn{1}{c}{$i_2$} &
    \multicolumn{1}{c}{$i_3$} & \multicolumn{1}{c}{$i_4$}\\
  \midrule
 1&   Star Wars & The Empire Strikes Back & Raiders of the Lost Ark & Return of the Jedi  \\
 2&   Star Wars & Return of the Jedi & Raiders of the Lost Ark & The Empire Strikes Back  \\
 3&   Star Wars & The Empire Strikes Back & Return of the Jedi & Raiders of the Lost Ark  \\
 4&   Star Wars & Raiders of the Lost Ark & Return of the Jedi & I.J. and the Last Crusade  \\
 5&   Star Wars & Return of the Jedi & Raiders of the Lost Ark & The Fugitive  \\
 6&   Star Wars & Raiders of the Lost Ark & Return of the Jedi & Back to the Future  \\
 7&   Star Wars & The Empire Strikes Back & Raiders of the Lost Ark & I.J. and the Last Crusade  \\
 8&   Star Wars & The Empire Strikes Back & Return of the Jedi & I.J. and the Last Crusade  \\
 9&   Star Wars & The Empire Strikes Back & I.J. and the Last Crusade & Return of the Jedi  \\
 10&   Star Wars & Raiders of the Lost Ark & The Empire Strikes Back & The Fugitive\\
\bottomrule
\end{tabular}
\caption{The ten best item proportions with a support of more than $200$ common
  ratings, using $A$.}
  \label{TAB:best_prop_num_basic_200}
\end{table}

The first obvious observation is that only a few movies (exactly $7$) make up
these 10 proportions. This is not really surprising, as we have set the support
threshold quite high, and only a few movies are rated by more than 200 users.
In fact, we found 331 4-itemsets with more than 200 common users, but they only
contain 37 unique movies. Clearly here the involved movies (Star Wars, Indiana
Jones\dots) are extremely popular, and people tend to give them high ratings.
Using notions introduced in Section \ref{TODO}, we can say that these movies
have a high item bias $b_i$. To confirm this claim, we can check that in
average, the mean rating of these 7 movies is of 4.09, while the average rating
of all movies is only 3.53.

Some of the analogies are actually really good though. For example the fourth
one \textit{Star Wars (1977)} is to \textit{Raiders of the Lost Ark (1981)} as
\textit{Return of the Jedi (1983)} is to \textit {Indiana Jones and the Last
Crusade (1989)}, or the seventh one which is similar: \textit{Star Wars (1977)}
is to \textit{The Empire Strikes Back} as \textit{Raiders of the Lost Ark
(1981)} is to \textit{Indiana Jones and the Last Crusade (1989)}

Looking at the first and third proportions though, we see that the two forms
$i_1:i_2::i_3:i_4$ and $i_1:i_2::i_4:i_3$ are present in the table. These two
forms are non-equivalent, and it does not seem natural to consider these two
proportions as (almost) equally valid. But this result can be explained by
looking at the actual rating values: a great number of them are of the form
$r:r::r:r$ (where $r$ is usually a high rating, given that these movies are
popular) or $r:r::r':r'$. In the two cases, a swith between $i_3$ and $i_4$ has
absolutely no effect on the truth values of the analogies.

This leads us to another point: all these analogies are, after all, quite
trivial. They all concern quite similar movies. We do not mean to offend any
cinema fan, but we believe it is still fair to say that the differences between
a Star Wars movie and an Indiana Jones movie are quite shallow. And these
similarities are reflected in the ratings, where the most common pattern is
$r:r::r:r$ (or close to it), suggesting that people like all these movies the
same.

To try to discover some more surprising analogies, we basically have two
options. The first one is to tweak a bit our quality function $f$ (and in our
case, our comparison function) by penalizing proportions with the pattern
$r:r::r:r$, or inversely by promoting those proportions $r : r' :: r :r'$ where
$r$ and $r'$ are quite different. The second option is to lower the minimum
support threshold, to allow movies that are rated by a reasonable (and not
necessarily very high) number of users. As we have seen, movies with a very
high number of ratings are likely yo lead to trivial proportions precisely
because they are popular, and because people like them all equally well in
general. If we really want to ban popular movies, we can also set a
\textbf{maximum} support threshold. This will also have the benefit to limit
the number of itemsets that we consider, leading to a significant improvement
in computation time. These two options will be explored.

Table \ref{TAB:best_prop_num_customTV_200} shows the 10 best proportions with a
minimum support of $200$, with a tweaked comparison function. Here, the truth
value of a component-proportion $r_1 : r_2 :: r_3 : r_4$ is defined by:
$$A'(r_1, r_2, r_3, r_4) = \frac{1}{2} A(r_1, r_2, r_3, r_4) + \frac{1}{4} |r_1
- r_2| + \frac{1}{4} |r_3 - r_4|.$$
We are here promoting proportions where rating values tend to disagree, which
should lead to different proportions to that of Table
\ref{TAB:best_prop_num_basic_200}. Proportions are still compared using the
lexicographic order of all the $A'$ values.
\begin{table}[h!]
\centering
  \begin{tabular}{ l l  l  l l }
\toprule
    & \multicolumn{1}{c}{$i_1$}  & \multicolumn{1}{c}{$i_2$} &
    \multicolumn{1}{c}{$i_3$} & \multicolumn{1}{c}{$i_4$}\\
  \midrule
    1& Star Wars  & Pulp Fiction  & The Empire Strikes Back  & The Fugitive   \\
    2& Star Wars  & Pulp Fiction  & Return of the Jedi  & The Fugitive   \\
    3&Star Wars  & Pulp Fiction  & The Empire Strikes Back  & The Silence of the Lambs   \\
    4&Star Wars  & Twister  & Return of the Jedi  & Independence Day  \\
    5&Star Wars  & Return of the Jedi  & The Silence of the Lambs  & Fargo   \\
    6&Star Wars  & The Terminator  & Return of the Jedi  & Pulp Fiction   \\
    7&Star Wars  & Pulp Fiction  & Return of the Jedi  & Terminator, The   \\
    8&Star Wars  & The Fugitive  & The Empire Strikes Back  & The Silence of the Lambs   \\
    9& Star Wars  & Pulp Fiction  & Return of the Jedi  & The Silence of the Lambs   \\
   10&Star Wars  & Pulp Fiction  & Raiders of the Lost Ark  & The Silence of the Lambs   \\
\bottomrule
\end{tabular}
\caption{The ten best item proportions with a support of more than $200$ common
  ratings, using $A'$.}
\label{TAB:best_prop_num_customTV_200}
\end{table}

We still have a lot of Star Wars movies (and a few Indiana Jones), but the
movies seem more diverse: we now have 11 unique movies, instead of just 7
before. Nine of these proportions comply with the pattern \textit{Star Wars
movie} is to \textit{Other Star Wars movie} what \textit{movie A} is to
\textit{movie B}. This suggests that we should find strong links underlying the
connexion betwwen \textit{Movie A} and \textit{Movie B}, because the two Star
Wars movies are deeply related. Is \textit{Pulp Fiction} related to \textit{The
Fugitive}? Is \textit{Twister} related to \textit{Independence Day}? Is
\textit{Fargo} related to \textit{The Silence of the Lambs}? We will not
venture to answer these questions, and leave them to the reader's appreciation.
All we can say is that some forms of analogy seem to emerge from their
respective ratings.  One thing we can note though, is that even if all these
movies are very popular just like in Table \ref{TAB:best_prop_num_basic_200},
their genres are however quite different: while we have considered Star Wars
and Indiana Jones to be of the same kind, we can fairly claim that a Star Wars
movie is very far from Fargo, Pulp Fiction or The Silence of the lambs.

Table \ref{TAB:best_prop_num_basic_10_50} illustrates the ten best proportions
we have found after setting the minimum support at 10, and a maximum support at
50. This drastically reduces the number of $4$-tuples that are explored, and
avoids any highly popular movie.
\begin{table}[h!]
\centering
  \begin{tabular}{ l l  l  l l }
\toprule
    &\multicolumn{1}{c}{$i_1$}  & \multicolumn{1}{c}{$i_2$} &
    \multicolumn{1}{c}{$i_3$} & \multicolumn{1}{c}{$i_4$}\\
  \midrule
  1&   To Catch a Thief  & Laura  & Gigi  & An American in Paris  \\
  2&   Nick of Time  & It Could Happen to You  & Milk Money & Only You  \\
  3& To Catch a Thief  & An American in Paris  & Gigi  & Meet John Doe   \\
  4& Dangerous Minds  & Money Train  & Higher Learning  & With Honors   \\
  5& Judge Dredd  & Under Siege 2: Dark Territory  & Shadow, The  & Mortal Kombat   \\
  6& Terminal Velocity & Under Siege 2: Dark Territory  & Money Train  & Drop Zone    \\
  7& Judge Dredd  &Under Siege 2: Dark Territory  &  Mortal Kombat  & Coneheads   \\
  8& To Catch a Thief  & An American in Paris  & Gigi  & Laura   \\
  9&  To Catch a Thief  & Meet John Doe  & Gigi  & An American in Paris   \\
 10&   Nick of Time  & It Could Happen to You  & Only You  & Milk Money   \\
\bottomrule
\end{tabular}
  \caption{The ten best item proportions with a support between $10$ and $50$.}
  \label{TAB:best_prop_num_basic_10_50}
\end{table}
Now, we find up to 20 unique movies building up these 10 proportions, which is
quite an improvement with respect to the previous settings. But let's be
honest, we have come to a point where the author is unable to further hide his
lack of cinematographic culture. He has never heard of any of these movies, and
will not pretend to.

Still, we shall try to make sense out of this. It seems that our proportion
exhibit some sort of clustering: movies of proportions 1, 3, 8 and 9 are all
movies from the 40'-50's. Clearly the users having rated the corresponding
movies must be some fans of the old movie-making era. Similarly, proportions 2
and 10 are all movies from 1994 or 1995, suggesting a niche. Here again it is
not really suprising that we observe the two patterns $a:b::c:d$ and
$a:b::d:c$. This comes from the fact that these movies are often given similar
(and high) ratings. The other proportions imply movies that are still from the
90's and that have revieved pretty bad reviews from critics: they seem to be
\textit{so bad it's good} kind.

To complete this investigation, have led the same experiments with binary
ratings. The ratings of the MovieLens dataset have been binarized by
associating the value $1$ if the a rating $\rui$ is above the user's average
rating $\mu_u$ (i.e. the user liked the movie) and $0$ otherwise (the user disliked the
movie):
$$\text{binary}(\rui) =
\begin{cases}
  1 \text{ if } \rui \geq \mu_u\\
  0 \text{ else},
\end{cases}
$$
The quality of a movie-based proportion is here the fraction of perfect
component-proportions. When setting a minimal support of 200 without any other
constraint, it should not come as a surprise that the movie-proportions that we
find are extremely similar to that of Table \ref{TAB:best_prop_num_basic_200}.
They involve the same 7 movies and are so alike that we choose not to show them
here to avoid redundancy. Indeed, the set of candidate 4-itemsets are
necessarily the same in the two settings: recall that on the first stage of the
algorithm, we do not care about the rating values so the fact that ratings are
binary or numerical do not change anything. The fact that the best proportion
still are the same is an argument in support of the coherence of the two
quality functions.

There are in total 993 candidate 4-itemsets. Figure
\ref{FIG:quality_of_proportions} shows the quality of these 993 proportions in
descending order.
\begin{figure}[!h]
\centering
  \includegraphics[width=4in]{figures/quality_of_proportions.pdf}
  \caption{Quality of the proportions}
\label{FIG:quality_proportions}
\end{figure}
For the best proportions, a perfect analogy stands on more than 70\% of the
components. This is quite significant as we are measuring this fraction over
more than 200 components. However, we have seen that much of these proportions
are not really interesting as all the common users tend to agree on the popular
movies.

We used another quality function: an analogy stands if the calssical analogy
stands and if we have $a \neq b$ and $c \neq d$. Figure
\ref{quality_proportions_different_ab_cd} gives us the fraction of correct
proportions for the 993 potential analogies.
\begin{figure}[!h]
\centering
  \includegraphics[width=4in]{figures/quality_of_proportions_different.pdf}
  \caption{Quality of the proportions}
\label{FIG:quality_proportions_different_ab_cd}
\end{figure}
Obviously, the quality drops significantly because we have forbidden patterns
like $r:r::r:r$. The only two available patterns are $r:r::r':r'$ and
$r:r'::r:r'$, where $r$ and $r'$ can both take values $0$ and $1$.

This tells us that there are not a lot of interesting analogies. And that all
we're doing is mostly nearest neighbors stuff.


Dire à la fin que bon, finalement les seules analogies convaincantes qu'on a
trouvées c'était des analogies aaaa et donc uqi pouvaient se trouver par knn
directmeent plutôt que de faire de l'analogie.


\section{Conclusion}
